{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 This wiki includes all information about the GIRAF project! The information is split into 6 categories: Getting Started Gives an introduction to GIRAF for students on a new semester. Development Documents how most of the GIRAF project is implemented and how to use it. Legal Legal, privacy, and security information about the GIRAF project. About General info about the GIRAF project. Legacy Archived files from pre 2024 GIRAF","title":"Home"},{"location":"#home","text":"This wiki includes all information about the GIRAF project! The information is split into 6 categories: Getting Started Gives an introduction to GIRAF for students on a new semester. Development Documents how most of the GIRAF project is implemented and how to use it. Legal Legal, privacy, and security information about the GIRAF project. About General info about the GIRAF project. Legacy Archived files from pre 2024 GIRAF","title":"Home"},{"location":"About/","text":"About \u00b6 Giraf is a umbrella term for digital tools developed for autistic children with little or no verbal communication. The Giraf applications have been developed by students at Aalborg University as a semester project on the software engineering education from the spring of 2011. The applications are created in close cooperation with B\u00f8rnehaven Birken (Kindergarten) , Egebakken (School) and Center for Autism and ADHD at Aalborg Municipality. Source : https://giraf.cs.aau.dk/en/about-giraf/","title":"About"},{"location":"About/#about","text":"Giraf is a umbrella term for digital tools developed for autistic children with little or no verbal communication. The Giraf applications have been developed by students at Aalborg University as a semester project on the software engineering education from the spring of 2011. The applications are created in close cooperation with B\u00f8rnehaven Birken (Kindergarten) , Egebakken (School) and Center for Autism and ADHD at Aalborg Municipality. Source : https://giraf.cs.aau.dk/en/about-giraf/","title":"About"},{"location":"Development/","text":"Overview \u00b6 This section historically contained necessary information necessary to know before developing on the GIRAF projects. That information can now be found in the Legacy section for the curious. The exception is generally useful knowledge such as information regarding the Wiki and its upkeep. We advise you check out the GIRAF GitHub for the most recent development information.","title":"Overview"},{"location":"Development/#overview","text":"This section historically contained necessary information necessary to know before developing on the GIRAF projects. That information can now be found in the Legacy section for the curious. The exception is generally useful knowledge such as information regarding the Wiki and its upkeep. We advise you check out the GIRAF GitHub for the most recent development information.","title":"Overview"},{"location":"Development/github/","text":"Use of GitHub in GIRAF \u00b6 This article explains how GitHub is used as a version control tool and more in the development of GIRAF applications. Issues \u00b6 Issues are created by the development teams as well as the PO group. An issue can be a bug report or a task creation request. The PO group prioritises, assigns and adds milestones to issues. The list of issues can be seen at each repository, by navigating to the \"Issues\" tab in the header of the repository. Alternatively a complete list for the whole organization can be viewed. If the application has GitHub Projects set up, it is also possible to view the Issues by navigating to the Projects tab. Here, issues can be assigned to sub teams and set up in Trello boards of readability. Getting an Issue to Work on \u00b6 If you have time to work on a new issue, you can get a new one by following these steps: Find an issue you want to work on Ask the PO group if you can work on that issue The PO group might say no for various reasons and they have the final say, as they have a better overview. There is usually a greater chance of getting a yes if the issue you've picked is either highest or high priority. If you don't have a preferred issue you can ask the PO group to be assigned the most pressing issue, as they have a good overview of the project and they will most likely have some issues that they would like you to work with. Creating an Issue \u00b6 If you find a bug, or have a task creation request you can create an issue: Go the the \"Issues\" tab of the relevant repository (E.g. https://github.com/aau-giraf/foodplanner/issues ). Press the green \"New issue\" button. Choose whether to submit a bug report or task creation request, and press \"Get started\". Create a title and description for the issue. Please follow the template, and don't delete the headers! The title for the Task Creation Request should tell what functionality you would like added using the shown form \"As a developer I would like the docker config file to automatically update so that I don\u00b4t have to manually update the config file\". Instead of the task being for the developer, guardian or user is also frequently used. Label the issue with appropriate labels. It can be a good idea to inform the PO group when you are done, so they can assign and refine the issue. The same can be achieved by accessing the GitHub Projects (E.g. https://github.com/orgs/aau-giraf/projects/42) and adding the issue there. Branches and Pull Requests \u00b6 We follow the GitFlow Workflow as explained in the process manual of 2019 . During the sprints, all development is done in feature branches, branching out from the develop branch. The naming convention for feature branches is feature/xx where xx is replaced by the issue number. When the Release Preparation phase begins, a release branch is created from the develop branch. This branch is now used instead of develop until the sprint is over. The naming convention for release branches is release/YYYYsXrZ where YYYY is replaced by year, X with the sprint number and Z with the release number. E.g. release/2020s1r1 for 2020, sprint 1, release 1. Creating a Branch \u00b6 During Sprints \u00b6 When you start working on an issue, you create a branch from develop called feature/xx where xx is the issue number. From the terminal: 1 2 git checkout develop git checkout -b feature/xx Or from GitHub: Make sure develop is selected. Input the name of the branch (e.g. for feature 400). Press \"Create branch: feature/xx from 'develop'\" During Release Preparation \u00b6 When you start working on a release fix, you create a branch from release/* called releasefix/xx where xx is the issue number. 1 2 git checkout release/* git checkout -b releasefix/xx Or from GitHub using the same procedure as above, but with the release branch as base instead, and with the release fix naming convention. Creating a Pull Request \u00b6 When you have finished your issue, it is time to create a pull request. A pull request is a request to merge your branch into another branch. Before making the pull request, make sure that the code: Only relates to a single issue. (One PR per user story) Is fully tested. Is reachable when opening the application. Fully tested means that if any piece of the functionality is removed, a test should fail. Creating a pull request on GitHub: Open the \"Pull requests\" tab in the repository (e.g. https://github.com/aau-giraf/foodplanner/pulls ) Press \"New pull request\" Select the appropriate branch as base. develop if during sprint release/* if during release preparation Select your branch for as compare Press \"Create pull request\" Name the pull request Feature xx or Feature xx: A title describing changes Write a description If you write closes #xx or fixes #xx , issue xx will be linked to the PR and will close when the PR is merged. ( All keywords ) Code Review \u00b6 After being assigned a pull request, the group should review the code under the Files changed tab. Look for code that may be deprecated, unnecessary, non-optimized or has weird formatting. Start at https://github.com/aau-giraf/ Choose repository eg. weekplanner. Click on the Pull Request tab. Choose an open pull request from the list. Click on the Files Changed Tab . All the changes can be seen in these files. Make a comment or suggestion on a single line or multiple lines by pressing the blue + icon (move the cursor to a line). The red square marks the selection icon which can be used to suggest code that replaces the line(s). You can view what the author will see by clicking Preview . Having looked over all the files, click Review changes . If you made comments, make sure the author looks them through by choosing Request changes before clicking Submit review . If changes are made, you have to re-review the pull request! If the changes makes sense, click Approve .","title":"Use of GitHub in GIRAF"},{"location":"Development/github/#use-of-github-in-giraf","text":"This article explains how GitHub is used as a version control tool and more in the development of GIRAF applications.","title":"Use of GitHub in GIRAF"},{"location":"Development/github/#issues","text":"Issues are created by the development teams as well as the PO group. An issue can be a bug report or a task creation request. The PO group prioritises, assigns and adds milestones to issues. The list of issues can be seen at each repository, by navigating to the \"Issues\" tab in the header of the repository. Alternatively a complete list for the whole organization can be viewed. If the application has GitHub Projects set up, it is also possible to view the Issues by navigating to the Projects tab. Here, issues can be assigned to sub teams and set up in Trello boards of readability.","title":"Issues"},{"location":"Development/github/#getting-an-issue-to-work-on","text":"If you have time to work on a new issue, you can get a new one by following these steps: Find an issue you want to work on Ask the PO group if you can work on that issue The PO group might say no for various reasons and they have the final say, as they have a better overview. There is usually a greater chance of getting a yes if the issue you've picked is either highest or high priority. If you don't have a preferred issue you can ask the PO group to be assigned the most pressing issue, as they have a good overview of the project and they will most likely have some issues that they would like you to work with.","title":"Getting an Issue to Work on"},{"location":"Development/github/#creating-an-issue","text":"If you find a bug, or have a task creation request you can create an issue: Go the the \"Issues\" tab of the relevant repository (E.g. https://github.com/aau-giraf/foodplanner/issues ). Press the green \"New issue\" button. Choose whether to submit a bug report or task creation request, and press \"Get started\". Create a title and description for the issue. Please follow the template, and don't delete the headers! The title for the Task Creation Request should tell what functionality you would like added using the shown form \"As a developer I would like the docker config file to automatically update so that I don\u00b4t have to manually update the config file\". Instead of the task being for the developer, guardian or user is also frequently used. Label the issue with appropriate labels. It can be a good idea to inform the PO group when you are done, so they can assign and refine the issue. The same can be achieved by accessing the GitHub Projects (E.g. https://github.com/orgs/aau-giraf/projects/42) and adding the issue there.","title":"Creating an Issue"},{"location":"Development/github/#branches-and-pull-requests","text":"We follow the GitFlow Workflow as explained in the process manual of 2019 . During the sprints, all development is done in feature branches, branching out from the develop branch. The naming convention for feature branches is feature/xx where xx is replaced by the issue number. When the Release Preparation phase begins, a release branch is created from the develop branch. This branch is now used instead of develop until the sprint is over. The naming convention for release branches is release/YYYYsXrZ where YYYY is replaced by year, X with the sprint number and Z with the release number. E.g. release/2020s1r1 for 2020, sprint 1, release 1.","title":"Branches and Pull Requests"},{"location":"Development/github/#creating-a-branch","text":"","title":"Creating a Branch"},{"location":"Development/github/#during-sprints","text":"When you start working on an issue, you create a branch from develop called feature/xx where xx is the issue number. From the terminal: 1 2 git checkout develop git checkout -b feature/xx Or from GitHub: Make sure develop is selected. Input the name of the branch (e.g. for feature 400). Press \"Create branch: feature/xx from 'develop'\"","title":"During Sprints"},{"location":"Development/github/#during-release-preparation","text":"When you start working on a release fix, you create a branch from release/* called releasefix/xx where xx is the issue number. 1 2 git checkout release/* git checkout -b releasefix/xx Or from GitHub using the same procedure as above, but with the release branch as base instead, and with the release fix naming convention.","title":"During Release Preparation"},{"location":"Development/github/#creating-a-pull-request","text":"When you have finished your issue, it is time to create a pull request. A pull request is a request to merge your branch into another branch. Before making the pull request, make sure that the code: Only relates to a single issue. (One PR per user story) Is fully tested. Is reachable when opening the application. Fully tested means that if any piece of the functionality is removed, a test should fail. Creating a pull request on GitHub: Open the \"Pull requests\" tab in the repository (e.g. https://github.com/aau-giraf/foodplanner/pulls ) Press \"New pull request\" Select the appropriate branch as base. develop if during sprint release/* if during release preparation Select your branch for as compare Press \"Create pull request\" Name the pull request Feature xx or Feature xx: A title describing changes Write a description If you write closes #xx or fixes #xx , issue xx will be linked to the PR and will close when the PR is merged. ( All keywords )","title":"Creating a Pull Request"},{"location":"Development/github/#code-review","text":"After being assigned a pull request, the group should review the code under the Files changed tab. Look for code that may be deprecated, unnecessary, non-optimized or has weird formatting. Start at https://github.com/aau-giraf/ Choose repository eg. weekplanner. Click on the Pull Request tab. Choose an open pull request from the list. Click on the Files Changed Tab . All the changes can be seen in these files. Make a comment or suggestion on a single line or multiple lines by pressing the blue + icon (move the cursor to a line). The red square marks the selection icon which can be used to suggest code that replaces the line(s). You can view what the author will see by clicking Preview . Having looked over all the files, click Review changes . If you made comments, make sure the author looks them through by choosing Request changes before clicking Submit review . If changes are made, you have to re-review the pull request! If the changes makes sense, click Approve .","title":"Code Review"},{"location":"Development/Wiki/","text":"Overview \u00b6 This section gives an overview of the Wiki repository. The wiki is written in Markdown , and rendered using MkDocs . Running the Wiki locally Writing Pages for the Wiki Important Files and Directories \u00b6 1 2 3 4 5 6 7 8 9 10 . \u251c\u2500\u2500 .github \u2502 \u2514\u2500\u2500 workflows \u2502 \u251c\u2500\u2500 ci.yml # GH Actions configuration for Continuous Integration \u2502 \u2514\u2500\u2500 page-build.yml # GH Actions configuration for building \u251c\u2500\u2500 docs # Contains all wiki files \u2502 \u2514\u2500\u2500 extra # Contains extra css as well as the logo. NO Markdown files here! \u251c\u2500\u2500 mkdocs.yml # MkDocs configuration file \u251c\u2500\u2500 requirements.txt # Contains python dependencies \u2514\u2500\u2500 run_mdl.bat # Batch file for running the Markdown linter locally (Windows only)","title":"Overview"},{"location":"Development/Wiki/#overview","text":"This section gives an overview of the Wiki repository. The wiki is written in Markdown , and rendered using MkDocs . Running the Wiki locally Writing Pages for the Wiki","title":"Overview"},{"location":"Development/Wiki/#important-files-and-directories","text":"1 2 3 4 5 6 7 8 9 10 . \u251c\u2500\u2500 .github \u2502 \u2514\u2500\u2500 workflows \u2502 \u251c\u2500\u2500 ci.yml # GH Actions configuration for Continuous Integration \u2502 \u2514\u2500\u2500 page-build.yml # GH Actions configuration for building \u251c\u2500\u2500 docs # Contains all wiki files \u2502 \u2514\u2500\u2500 extra # Contains extra css as well as the logo. NO Markdown files here! \u251c\u2500\u2500 mkdocs.yml # MkDocs configuration file \u251c\u2500\u2500 requirements.txt # Contains python dependencies \u2514\u2500\u2500 run_mdl.bat # Batch file for running the Markdown linter locally (Windows only)","title":"Important Files and Directories"},{"location":"Development/Wiki/running_wiki/","text":"Running the Wiki Locally \u00b6 This page describes how to run the wiki in order to properly edit it. It also decribes how to use the markdown linter locally, in order to catch any potential issues before the GitHub Actions does. MkDocs \u00b6 The wiki is built with MkDocs , using the Material theme . MkDocs is configured in the mkdocs.yml file. The following plugins are used: search awesome-pages git-revision-date-localized If more plugins are downloaded with pip, remember to add them to the requirements.txt file. If any plugins need to be updated, change the version number for the plugin in the requirements.txt file. Running the Wiki with MkDocs \u00b6 First, install the newest version of Python . Then, follow these steps (these steps are only necessary when running MkDocs for the first time ): Open a terminal in the root of the wiki project, or navigate to the root in the terminal. Set up a virtual environment Install virtual environment with pip install virtualenv . Run python -m venv venv . Source the virtual environment. Linux: source venv/bin/activate Windows: .\\venv\\Scripts\\activate.bat Install MkDocs plugins by running pip install -r requirements.txt . Finally, the wiki can be hosted locally by running mkdocs serve . The local server can then be accessed at http://127.0.0.1:8000/ . After following these steps, the wiki can always be hosted by sourcing venv again and then running mkdocs serve in the root of the project. However, if any changes have been made to the requirements.txt file (added or updated plugins) step 3 must be done again after sourcing venv . Markdownlint \u00b6 During the semester of 2020E, a GitHub Action workflow using Markdownlint was set up for the wiki repository. This workflow serves as continuous integration for the repository, and is set up in the ci.yml file. The rules used by the linter can be found here . Any exceptions to these rules are found or set up in the .mdl_style.rb file. Running Markdownlint Locally \u00b6 First, install the newest version of Ruby . Second, install Markdownlint by opening a terminal and running gem install mdl . IF , gem install mdl will not work, clone the Markdownlint repository and then follow these steps: Open a terminal in the root of the cloned Markdownlint repository, or navigate to the root in the terminal, and run the following: gem install rake gem install bundler rake install If either gem or mdl are not recognized, make sure that the .\\bin of either is in the PATH environment variable. Finally, Markdownlint can be run locally by following these steps: Open a terminal in the root of the wiki project, or navigate to the root in the terminal. Depending on OS, run either: Windows: run_mdl.bat Mac/Linux: mdl docs/ After running it, Markdownlint will report any Markdown issues, or it will report nothing indicating that all of the .md files follow the rules.","title":"Running the Wiki Locally"},{"location":"Development/Wiki/running_wiki/#running-the-wiki-locally","text":"This page describes how to run the wiki in order to properly edit it. It also decribes how to use the markdown linter locally, in order to catch any potential issues before the GitHub Actions does.","title":"Running the Wiki Locally"},{"location":"Development/Wiki/running_wiki/#mkdocs","text":"The wiki is built with MkDocs , using the Material theme . MkDocs is configured in the mkdocs.yml file. The following plugins are used: search awesome-pages git-revision-date-localized If more plugins are downloaded with pip, remember to add them to the requirements.txt file. If any plugins need to be updated, change the version number for the plugin in the requirements.txt file.","title":"MkDocs"},{"location":"Development/Wiki/running_wiki/#running-the-wiki-with-mkdocs","text":"First, install the newest version of Python . Then, follow these steps (these steps are only necessary when running MkDocs for the first time ): Open a terminal in the root of the wiki project, or navigate to the root in the terminal. Set up a virtual environment Install virtual environment with pip install virtualenv . Run python -m venv venv . Source the virtual environment. Linux: source venv/bin/activate Windows: .\\venv\\Scripts\\activate.bat Install MkDocs plugins by running pip install -r requirements.txt . Finally, the wiki can be hosted locally by running mkdocs serve . The local server can then be accessed at http://127.0.0.1:8000/ . After following these steps, the wiki can always be hosted by sourcing venv again and then running mkdocs serve in the root of the project. However, if any changes have been made to the requirements.txt file (added or updated plugins) step 3 must be done again after sourcing venv .","title":"Running the Wiki with MkDocs"},{"location":"Development/Wiki/running_wiki/#markdownlint","text":"During the semester of 2020E, a GitHub Action workflow using Markdownlint was set up for the wiki repository. This workflow serves as continuous integration for the repository, and is set up in the ci.yml file. The rules used by the linter can be found here . Any exceptions to these rules are found or set up in the .mdl_style.rb file.","title":"Markdownlint"},{"location":"Development/Wiki/running_wiki/#running-markdownlint-locally","text":"First, install the newest version of Ruby . Second, install Markdownlint by opening a terminal and running gem install mdl . IF , gem install mdl will not work, clone the Markdownlint repository and then follow these steps: Open a terminal in the root of the cloned Markdownlint repository, or navigate to the root in the terminal, and run the following: gem install rake gem install bundler rake install If either gem or mdl are not recognized, make sure that the .\\bin of either is in the PATH environment variable. Finally, Markdownlint can be run locally by following these steps: Open a terminal in the root of the wiki project, or navigate to the root in the terminal. Depending on OS, run either: Windows: run_mdl.bat Mac/Linux: mdl docs/ After running it, Markdownlint will report any Markdown issues, or it will report nothing indicating that all of the .md files follow the rules.","title":"Running Markdownlint Locally"},{"location":"Development/Wiki/writing_wiki/","text":"Writing Pages for the Wiki \u00b6 This page gives guidelines for how to write pages for the wiki. Writing Pages \u00b6 Pages are written in Markdown. A guide for writing Markdown can be seen here . All Markdown files have to be in the docs folder. Every folder in the docs folder creates a new section. If a folder contains a file named index.md, that file will be the main page of the section. Since changes to the wiki are tracked by GitHub, using one line per sentence, will make it easier to keep track of. This means that every time you finish a sentence with a dot, you should change line. This cannot be seen in the render. Additionally, lines should be kept limited to 120 characters each. If a sentence exceeds this limit a linebreak should be added, so the sentence occupies two lines instead of one. Common Mistakes \u00b6 MkDocs can be a bit more strict in regard to its Markdown syntax, compared to GitHub. This means that some mistakes happens often, since they work in some Markdown renders but not in MkDocs. ALL of these mistakes should be caught by the markdown linter described here , unless any rules have been manually exempt. Here are some examples of common mistakes: Only One Top Level Header \u00b6 MkDocs supports only one top level header ( # header ) per page. If another level one header is present, the rest of the page won't be shown in the overview. Missing Blank Lines \u00b6 MkDocs requires elements to be preceded by blank lines. This also means that headers should be proceded by blank lines before content. Wrong: 1 2 3 Some text - List item ... Correct: 1 2 3 4 Some text - List item ... List Indentation \u00b6 MkDocs requires lists to use 4 spaces for indentation. Wrong: 1 2 3 - List item - List subitem ... Correct: 1 2 - List item - List subitem Custom Title \u00b6 File metadata is written using yaml-frontmatter. As an example, a page's title can be specified. 1 2 3 4 5 --- title: \"Custom Title\" --- # Hello World If title is not specified, the page is given the header's content. In the above example that would be Hello World if the title was not specified in the metadata. Arranging Pages \u00b6 Using awesome-pages , a .pages file can be created in every folder. This can be used to arrange pages manually. As an example, the tabs are arranged with the .pages file in the docs folder: 1 2 3 4 5 arrange : - index.md - getting_started - ... # puts all pages/sections not specified here - releases Building the Pages \u00b6 Everytime something is pushed to the master branch of the wiki, the GitHub Actions workflow is run. The workflow deploys the built pages to the gh-pages branch, which is set to be the source of the GitHub Pages. This is configured in the repository settings of the wiki repository.","title":"Writing Pages for the Wiki"},{"location":"Development/Wiki/writing_wiki/#writing-pages-for-the-wiki","text":"This page gives guidelines for how to write pages for the wiki.","title":"Writing Pages for the Wiki"},{"location":"Development/Wiki/writing_wiki/#writing-pages","text":"Pages are written in Markdown. A guide for writing Markdown can be seen here . All Markdown files have to be in the docs folder. Every folder in the docs folder creates a new section. If a folder contains a file named index.md, that file will be the main page of the section. Since changes to the wiki are tracked by GitHub, using one line per sentence, will make it easier to keep track of. This means that every time you finish a sentence with a dot, you should change line. This cannot be seen in the render. Additionally, lines should be kept limited to 120 characters each. If a sentence exceeds this limit a linebreak should be added, so the sentence occupies two lines instead of one.","title":"Writing Pages"},{"location":"Development/Wiki/writing_wiki/#common-mistakes","text":"MkDocs can be a bit more strict in regard to its Markdown syntax, compared to GitHub. This means that some mistakes happens often, since they work in some Markdown renders but not in MkDocs. ALL of these mistakes should be caught by the markdown linter described here , unless any rules have been manually exempt. Here are some examples of common mistakes:","title":"Common Mistakes"},{"location":"Development/Wiki/writing_wiki/#only-one-top-level-header","text":"MkDocs supports only one top level header ( # header ) per page. If another level one header is present, the rest of the page won't be shown in the overview.","title":"Only One Top Level Header"},{"location":"Development/Wiki/writing_wiki/#missing-blank-lines","text":"MkDocs requires elements to be preceded by blank lines. This also means that headers should be proceded by blank lines before content. Wrong: 1 2 3 Some text - List item ... Correct: 1 2 3 4 Some text - List item ...","title":"Missing Blank Lines"},{"location":"Development/Wiki/writing_wiki/#list-indentation","text":"MkDocs requires lists to use 4 spaces for indentation. Wrong: 1 2 3 - List item - List subitem ... Correct: 1 2 - List item - List subitem","title":"List Indentation"},{"location":"Development/Wiki/writing_wiki/#custom-title","text":"File metadata is written using yaml-frontmatter. As an example, a page's title can be specified. 1 2 3 4 5 --- title: \"Custom Title\" --- # Hello World If title is not specified, the page is given the header's content. In the above example that would be Hello World if the title was not specified in the metadata.","title":"Custom Title"},{"location":"Development/Wiki/writing_wiki/#arranging-pages","text":"Using awesome-pages , a .pages file can be created in every folder. This can be used to arrange pages manually. As an example, the tabs are arranged with the .pages file in the docs folder: 1 2 3 4 5 arrange : - index.md - getting_started - ... # puts all pages/sections not specified here - releases","title":"Arranging Pages"},{"location":"Development/Wiki/writing_wiki/#building-the-pages","text":"Everytime something is pushed to the master branch of the wiki, the GitHub Actions workflow is run. The workflow deploys the built pages to the gh-pages branch, which is set to be the source of the GitHub Pages. This is configured in the repository settings of the wiki repository.","title":"Building the Pages"},{"location":"Getting_Started/","text":"Overview \u00b6 This is an overview of things to know when starting with GIRAF. Legacy A compilation of materials used by prior GIRAF teams, and a guide for what should be handed over between GIRAF teams. Ownership Transfer Information regarding what a new GIRAF teams needs in order to assume ownership of the GIRAF project. Process Guide A short guide with useful tips from previous GIRAF teams as to how the mega project should be done. Tools Short descriptions of tools used during development.","title":"Overview"},{"location":"Getting_Started/#overview","text":"This is an overview of things to know when starting with GIRAF. Legacy A compilation of materials used by prior GIRAF teams, and a guide for what should be handed over between GIRAF teams. Ownership Transfer Information regarding what a new GIRAF teams needs in order to assume ownership of the GIRAF project. Process Guide A short guide with useful tips from previous GIRAF teams as to how the mega project should be done. Tools Short descriptions of tools used during development.","title":"Overview"},{"location":"Getting_Started/handover/","text":"Handover \u00b6 Experience and information from previous semesters can found in the section Legacy . It includes things such as: Description of the role structure used in a semester. Information about the sprint events used in a semester. Description of tools that has been used e.g. scripts, Slack etc. Important issues and features that are unfinished. Future GIRAF developers are also advised to check out reports of the groups before them in the AAU Project Library","title":"Handover"},{"location":"Getting_Started/handover/#handover","text":"Experience and information from previous semesters can found in the section Legacy . It includes things such as: Description of the role structure used in a semester. Information about the sprint events used in a semester. Description of tools that has been used e.g. scripts, Slack etc. Important issues and features that are unfinished. Future GIRAF developers are also advised to check out reports of the groups before them in the AAU Project Library","title":"Handover"},{"location":"Getting_Started/ownership_transfer/","text":"Ownership Transfer \u00b6 This document describes how the next generation of GIRAF developers can get ownership of the different services. GIRAF Passwords \u00b6 All relevant passwords can be found with the password manager KeePass . Contact Ulrik Nyman ( ulrik@cs.aau.dk ) to get the .kbdx file containing the passwords. Remember to have the file available some place public (eg. Google Drive) so all members can get access if they need to. Any changes made means that the file will have to be re-uploaded. We recommend that the Process group manages the passwords and the file. GitHub \u00b6 To get access to the GitHub repositories, an owner needs to add the new members. Use the following credentials to login and add all the relevant members. Username: giraf@lists.aau.dk Password: see KeePass The account uses 2FA, so Ulrik Nyman ( ulrik@cs.aau.dk ) has to be contacted in order to get the secondary code. Google Play Store \u00b6 To access the Google Play Store use the following credentials. Email: aaugiraf@gmail.com Password: see KeePass App Store \u00b6 To access the App Store use the following credentials. Email: giraf@lists.aau.dk Password: see KeePass Google Drive (OUTDATED - left for inspiration) \u00b6 To access the Google Drive use the same credentials as for the Google Play Store . Email: aaugiraf@gmail.com Password: see KeePass The Drive contains assets from past GIRAF teams. Among these assets are the tools and code review checklists used during the semester of 2020E. Server Access \u00b6 As of today (02/09/2025) there is no GIRAF Server running. Previous semesters have run their own servers using AAU Strato Portainer Access (OUTDATED - left for inspiration) \u00b6 Portainer allows the visualization of the structure for the docker images. It can be helpful to have an UI instead of working in a terminal. Note that you have to be on the AAU network or use the AAU VPN to access it. URL: 192.38.56.151:9000 Username: admin Password: see KeePass Short Video on How to Access Portainer \u00b6 https://youtu.be/KZYsAJDHlxo","title":"Ownership Transfer"},{"location":"Getting_Started/ownership_transfer/#ownership-transfer","text":"This document describes how the next generation of GIRAF developers can get ownership of the different services.","title":"Ownership Transfer"},{"location":"Getting_Started/ownership_transfer/#giraf-passwords","text":"All relevant passwords can be found with the password manager KeePass . Contact Ulrik Nyman ( ulrik@cs.aau.dk ) to get the .kbdx file containing the passwords. Remember to have the file available some place public (eg. Google Drive) so all members can get access if they need to. Any changes made means that the file will have to be re-uploaded. We recommend that the Process group manages the passwords and the file.","title":"GIRAF Passwords"},{"location":"Getting_Started/ownership_transfer/#github","text":"To get access to the GitHub repositories, an owner needs to add the new members. Use the following credentials to login and add all the relevant members. Username: giraf@lists.aau.dk Password: see KeePass The account uses 2FA, so Ulrik Nyman ( ulrik@cs.aau.dk ) has to be contacted in order to get the secondary code.","title":"GitHub"},{"location":"Getting_Started/ownership_transfer/#google-play-store","text":"To access the Google Play Store use the following credentials. Email: aaugiraf@gmail.com Password: see KeePass","title":"Google Play Store"},{"location":"Getting_Started/ownership_transfer/#app-store","text":"To access the App Store use the following credentials. Email: giraf@lists.aau.dk Password: see KeePass","title":"App Store"},{"location":"Getting_Started/ownership_transfer/#google-drive-outdated-left-for-inspiration","text":"To access the Google Drive use the same credentials as for the Google Play Store . Email: aaugiraf@gmail.com Password: see KeePass The Drive contains assets from past GIRAF teams. Among these assets are the tools and code review checklists used during the semester of 2020E.","title":"Google Drive (OUTDATED - left for inspiration)"},{"location":"Getting_Started/ownership_transfer/#server-access","text":"As of today (02/09/2025) there is no GIRAF Server running. Previous semesters have run their own servers using AAU Strato","title":"Server Access"},{"location":"Getting_Started/ownership_transfer/#portainer-access-outdated-left-for-inspiration","text":"Portainer allows the visualization of the structure for the docker images. It can be helpful to have an UI instead of working in a terminal. Note that you have to be on the AAU network or use the AAU VPN to access it. URL: 192.38.56.151:9000 Username: admin Password: see KeePass","title":"Portainer Access (OUTDATED - left for inspiration)"},{"location":"Getting_Started/ownership_transfer/#short-video-on-how-to-access-portainer","text":"https://youtu.be/KZYsAJDHlxo","title":"Short Video on How to Access Portainer"},{"location":"Getting_Started/process/","text":"Process \u00b6 It is recommended that the groups work agile in this project. This section will provide an insight in the process of the former groups of GIRAF. If this guide is insufficient or you have further questions it is suggested that you contact the Agile Software Engineering (ASE) lecturer. The Legacy tab provides former students experiences, primarily regarding the weekplanner project, which although not relevant can provide useful insight. This includes meeting notes with Carsten, the previous ASE lecturer, and many other resources. Role Overview \u00b6 Roles can either be set for the semester, or alternate such that every student tries every role. (Swapping roles proved difficult and required good communication and notes) The imagine above shows the general structure in the GIRAF Projects. Each project group has a PO and SM, and these are sent to scrum of scrum meetings forming a PO and SM group which governs the project. The matters discussed in the PO and SM group are then brought back to the project groups and summarized, such that every member of the GIRAF project is up to date. (Groups are encouraged to speak together and knowledge share, however the responsibility lies with the PO and SM to ensure the developers are up to date) PO \u00b6 The PO group consists of a single group, which has contact to the stakeholders. The following things are the responsibility of the members within the PO group: Contact with stakeholders Maintain Product Backlog Create new issues Create prototypes if an issue needs one Create and conduct usability tests Maintain design guidelines Decide together with the Process group, which issues that should be in the Sprint Backlog Add release descriptions to the Wiki as described in Release Guide Can work on issues if necessary SM \u00b6 The SM group consists of a single group, which administers the process of the GIRAF team(s). They enforce the SCRUM rules. The following things are the responsibility of the members in the Process group. Maintain the process in the GIRAF team Structure of sprint of events Plan meetings within the GIRAF team (E.g. location and time for the sprint events) Moderator in the sprint events Distribute issues from GitHub Add reviewers to pull requests Decide together with PO, which issues that should be in the Sprint Backlog Can work on issues if necessary DEV \u00b6 Remainder of the students are developers. The following things are the responsibility of the members within a Development team. Work on issues Create new issues Review pull requests (This should always be the main priority) Sprint Overview \u00b6 Before the Sprint Planning \u00b6 Prior to Sprint Planning the Process group and the PO group have decided which issues to include in the Sprint Backlog. Only those issues will be time estimated and prioritized during Sprint Planning. Minor adjustments may be made afterwards based on the outcome of the Sprint Planning. Next, everyone is divided into Sprint Planning Cross-groups. Each Sprint Planning Cross-group has at least one Process group member and one PO group member. This means that the number of Sprint Planning - Cross-groups cannot exceed the number of members in the Process or PO group. The Sprint Planning Cross-groups are created randomly for each sprint. The Sprint Planning Cross-groups are send out to the whole GIRAF team prior to the Sprint Planning. After the Sprint Planning Cross-groups are made, the issues in the Sprint Backlog should be divided evenly between the groups. This process has been automated through the Sprint Planning Tool . Sprint Planning \u00b6 Everyone on the GIRAF Scrum team (PO, SM and Development teams) attend this event that marks the start of a sprint. Expected Duration \u00b6 Max 4 hours (Adjusted for 2 week sprint). Result \u00b6 A prioritized list of issues for each Development team to work on during the sprint. Agenda \u00b6 Process group member presents the following: Changes made to the process, since the previous sprint, based on the information from Sprint Retrospective. PO group member presents the following: Semester goal The overall goal of the project e.g. has the goal changed since last Sprint Planning. Sprint goal The goal of this sprint. New and/or important issues in the sprint Time estimation begins. Time Estimation \u00b6 Time estimation is done through Planning Poker Remember to include testing, review, documentation and usability test design in your estimations. Everyone has ten cards with these numbers: 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144 (the fibonacci numbers). 5 is equal to an 8-hour work day. The purpose of this is to not think of the numbers as hours but as relative to one day's workload. The Process group member has a list of issues the group should time estimate. Each issue is only estimated by one group. For every issue follow these steps: Make sure everyone understands a given issue Everyone reads the issue Discuss the issue If anyone is in doubt about the nature of an issue ask the PO member. Guess the time required to complete an issue. Everyone picks a card and holds the value secret. Everyone shows their cards simultaneously. If the cards are more than two steps apart, then the ones with highest and lowest numbers present their point of view. By more than two steps means that e.g 3 and 8 are two steps apart. 3 and 5 are not. Repeat steps 2.a-2.c up to 3 times or until an agreement is reached. These scenarios are seen as agreements: If everyone has estimated the issue to the same number. This number is noted. If there is one between the highest and lowest estimate. The highest number is noted. The following is done, if there is no agreement after 3 times: The median or the number closest to the median is noted e.g. with the numbers 3, 3, 5, 8 the median is 4 but 5 is noted since that is the closest fibonacci number. After Sprint Planning \u00b6 When all Sprint Planning Cross-groups have finished time estimation each Development team gets back together to prioritize the issues. The prioritization are made according to what the Development team would like to work with. All issues in the sprint should prioritized as either: High (Want the most) Medium Low (Want the least) This list should be sent to the Process group by the end of Sprint Planning. The Process group will then assign the Development teams to certain issues. The assignment will use the following constraints: As many high priority issues as possible. As few low priority issues as possible. Equal workload between Development Teams. The issue assignments will be sent out at most 24 hours later along with a list of which types of issues to solve first e.g. bug fixes before features. Development Phase \u00b6 The Development Phase starts after the Sprint Planning and ends on the day of the Sprint Review. Work with Issues \u00b6 In the Development Phase, the Development teams work with the issues they have been given by the Process group after the Sprint Planning. Need More Issues to Work On \u00b6 If you have time to work on a additional issues, than the ones that were distributed after Sprint Planning, you can get a new one by following these steps: Find an issue you want to work on in the Sprint Backlog Ask the Process group if you can work on that issue The Process group might say no for various reasons and they have the final say, as they have a better overview. There is usually a greater chance of getting a yes, if the issue you have picked is either highest or high priority. If you don't have a preferred issue you can ask the PO group to be assigned the most pressing issue, as they have a good overview of the project and they will most likely have some issues that they would like you to work with. If a Development team need more issues to work on, they have to contact the Process group.","title":"Process"},{"location":"Getting_Started/process/#process","text":"It is recommended that the groups work agile in this project. This section will provide an insight in the process of the former groups of GIRAF. If this guide is insufficient or you have further questions it is suggested that you contact the Agile Software Engineering (ASE) lecturer. The Legacy tab provides former students experiences, primarily regarding the weekplanner project, which although not relevant can provide useful insight. This includes meeting notes with Carsten, the previous ASE lecturer, and many other resources.","title":"Process"},{"location":"Getting_Started/process/#role-overview","text":"Roles can either be set for the semester, or alternate such that every student tries every role. (Swapping roles proved difficult and required good communication and notes) The imagine above shows the general structure in the GIRAF Projects. Each project group has a PO and SM, and these are sent to scrum of scrum meetings forming a PO and SM group which governs the project. The matters discussed in the PO and SM group are then brought back to the project groups and summarized, such that every member of the GIRAF project is up to date. (Groups are encouraged to speak together and knowledge share, however the responsibility lies with the PO and SM to ensure the developers are up to date)","title":"Role Overview"},{"location":"Getting_Started/process/#po","text":"The PO group consists of a single group, which has contact to the stakeholders. The following things are the responsibility of the members within the PO group: Contact with stakeholders Maintain Product Backlog Create new issues Create prototypes if an issue needs one Create and conduct usability tests Maintain design guidelines Decide together with the Process group, which issues that should be in the Sprint Backlog Add release descriptions to the Wiki as described in Release Guide Can work on issues if necessary","title":"PO"},{"location":"Getting_Started/process/#sm","text":"The SM group consists of a single group, which administers the process of the GIRAF team(s). They enforce the SCRUM rules. The following things are the responsibility of the members in the Process group. Maintain the process in the GIRAF team Structure of sprint of events Plan meetings within the GIRAF team (E.g. location and time for the sprint events) Moderator in the sprint events Distribute issues from GitHub Add reviewers to pull requests Decide together with PO, which issues that should be in the Sprint Backlog Can work on issues if necessary","title":"SM"},{"location":"Getting_Started/process/#dev","text":"Remainder of the students are developers. The following things are the responsibility of the members within a Development team. Work on issues Create new issues Review pull requests (This should always be the main priority)","title":"DEV"},{"location":"Getting_Started/process/#sprint-overview","text":"","title":"Sprint Overview"},{"location":"Getting_Started/process/#before-the-sprint-planning","text":"Prior to Sprint Planning the Process group and the PO group have decided which issues to include in the Sprint Backlog. Only those issues will be time estimated and prioritized during Sprint Planning. Minor adjustments may be made afterwards based on the outcome of the Sprint Planning. Next, everyone is divided into Sprint Planning Cross-groups. Each Sprint Planning Cross-group has at least one Process group member and one PO group member. This means that the number of Sprint Planning - Cross-groups cannot exceed the number of members in the Process or PO group. The Sprint Planning Cross-groups are created randomly for each sprint. The Sprint Planning Cross-groups are send out to the whole GIRAF team prior to the Sprint Planning. After the Sprint Planning Cross-groups are made, the issues in the Sprint Backlog should be divided evenly between the groups. This process has been automated through the Sprint Planning Tool .","title":"Before the Sprint Planning"},{"location":"Getting_Started/process/#sprint-planning","text":"Everyone on the GIRAF Scrum team (PO, SM and Development teams) attend this event that marks the start of a sprint.","title":"Sprint Planning"},{"location":"Getting_Started/process/#expected-duration","text":"Max 4 hours (Adjusted for 2 week sprint).","title":"Expected Duration"},{"location":"Getting_Started/process/#result","text":"A prioritized list of issues for each Development team to work on during the sprint.","title":"Result"},{"location":"Getting_Started/process/#agenda","text":"Process group member presents the following: Changes made to the process, since the previous sprint, based on the information from Sprint Retrospective. PO group member presents the following: Semester goal The overall goal of the project e.g. has the goal changed since last Sprint Planning. Sprint goal The goal of this sprint. New and/or important issues in the sprint Time estimation begins.","title":"Agenda"},{"location":"Getting_Started/process/#time-estimation","text":"Time estimation is done through Planning Poker Remember to include testing, review, documentation and usability test design in your estimations. Everyone has ten cards with these numbers: 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144 (the fibonacci numbers). 5 is equal to an 8-hour work day. The purpose of this is to not think of the numbers as hours but as relative to one day's workload. The Process group member has a list of issues the group should time estimate. Each issue is only estimated by one group. For every issue follow these steps: Make sure everyone understands a given issue Everyone reads the issue Discuss the issue If anyone is in doubt about the nature of an issue ask the PO member. Guess the time required to complete an issue. Everyone picks a card and holds the value secret. Everyone shows their cards simultaneously. If the cards are more than two steps apart, then the ones with highest and lowest numbers present their point of view. By more than two steps means that e.g 3 and 8 are two steps apart. 3 and 5 are not. Repeat steps 2.a-2.c up to 3 times or until an agreement is reached. These scenarios are seen as agreements: If everyone has estimated the issue to the same number. This number is noted. If there is one between the highest and lowest estimate. The highest number is noted. The following is done, if there is no agreement after 3 times: The median or the number closest to the median is noted e.g. with the numbers 3, 3, 5, 8 the median is 4 but 5 is noted since that is the closest fibonacci number.","title":"Time Estimation"},{"location":"Getting_Started/process/#after-sprint-planning","text":"When all Sprint Planning Cross-groups have finished time estimation each Development team gets back together to prioritize the issues. The prioritization are made according to what the Development team would like to work with. All issues in the sprint should prioritized as either: High (Want the most) Medium Low (Want the least) This list should be sent to the Process group by the end of Sprint Planning. The Process group will then assign the Development teams to certain issues. The assignment will use the following constraints: As many high priority issues as possible. As few low priority issues as possible. Equal workload between Development Teams. The issue assignments will be sent out at most 24 hours later along with a list of which types of issues to solve first e.g. bug fixes before features.","title":"After Sprint Planning"},{"location":"Getting_Started/process/#development-phase","text":"The Development Phase starts after the Sprint Planning and ends on the day of the Sprint Review.","title":"Development Phase"},{"location":"Getting_Started/process/#work-with-issues","text":"In the Development Phase, the Development teams work with the issues they have been given by the Process group after the Sprint Planning.","title":"Work with Issues"},{"location":"Getting_Started/process/#need-more-issues-to-work-on","text":"If you have time to work on a additional issues, than the ones that were distributed after Sprint Planning, you can get a new one by following these steps: Find an issue you want to work on in the Sprint Backlog Ask the Process group if you can work on that issue The Process group might say no for various reasons and they have the final say, as they have a better overview. There is usually a greater chance of getting a yes, if the issue you have picked is either highest or high priority. If you don't have a preferred issue you can ask the PO group to be assigned the most pressing issue, as they have a good overview of the project and they will most likely have some issues that they would like you to work with. If a Development team need more issues to work on, they have to contact the Process group.","title":"Need More Issues to Work On"},{"location":"Getting_Started/tools/","text":"Tools \u00b6 Discord \u00b6 A communication platform. Great for day-to-day communication, quick updates, team alignment, and holding virtual meetings. Invite Link Github \u00b6 A cloud-based platform for version control and collaborative coding using Git. Essential for storing code, tracking changes, and ensuring multiple team members can contribute without conflicts. It allows branching, pull requests, and code reviews for smooth collaboration across projects. GitHub Link Github projects \u00b6 A project management tool built into GitHub that integrates directly with repositories. Useful for planning, tracking tasks, and organizing workflows. Teams can create boards or timelines, assign tasks, and link them to issues and pull requests, making progress transparent and connected to actual code development. Figma \u00b6 A collaborative design tool for UI/UX, prototyping, and visual brainstorming. Ideal for creating and sharing design systems, wireframes, prototypes, and mockups. Multiple team members can work on the same design file in real time, making it easy to iterate and align design with development. Foodplanner Figma Google drev \u00b6 A cloud-based storage and file-sharing service. Acts as a central hub for storing and sharing non-code files. Useful for documentation, reports, meeting notes, and collaborative writing, ensuring everyone has access to the latest versions. Foodplanner Drive","title":"Tools"},{"location":"Getting_Started/tools/#tools","text":"","title":"Tools"},{"location":"Getting_Started/tools/#discord","text":"A communication platform. Great for day-to-day communication, quick updates, team alignment, and holding virtual meetings. Invite Link","title":"Discord"},{"location":"Getting_Started/tools/#github","text":"A cloud-based platform for version control and collaborative coding using Git. Essential for storing code, tracking changes, and ensuring multiple team members can contribute without conflicts. It allows branching, pull requests, and code reviews for smooth collaboration across projects. GitHub Link","title":"Github"},{"location":"Getting_Started/tools/#github-projects","text":"A project management tool built into GitHub that integrates directly with repositories. Useful for planning, tracking tasks, and organizing workflows. Teams can create boards or timelines, assign tasks, and link them to issues and pull requests, making progress transparent and connected to actual code development.","title":"Github projects"},{"location":"Getting_Started/tools/#figma","text":"A collaborative design tool for UI/UX, prototyping, and visual brainstorming. Ideal for creating and sharing design systems, wireframes, prototypes, and mockups. Multiple team members can work on the same design file in real time, making it easy to iterate and align design with development. Foodplanner Figma","title":"Figma"},{"location":"Getting_Started/tools/#google-drev","text":"A cloud-based storage and file-sharing service. Acts as a central hub for storing and sharing non-code files. Useful for documentation, reports, meeting notes, and collaborative writing, ensuring everyone has access to the latest versions. Foodplanner Drive","title":"Google drev"},{"location":"Legacy/","text":"Overview \u00b6 This section contains legacy content related to GIRAF development between 2018 and 2020. The files were meant to inspire and guide future GIRAF developers, and although the weekplanner project has been retired, they still provide usefull insights.","title":"Overview"},{"location":"Legacy/#overview","text":"This section contains legacy content related to GIRAF development between 2018 and 2020. The files were meant to inspire and guide future GIRAF developers, and although the weekplanner project has been retired, they still provide usefull insights.","title":"Overview"},{"location":"Legacy/Handover/","text":"Overview \u00b6 This handover is a compilation of materials used by prior GIRAF teams. The handover from the semesters are organized in sections named after the semesters year. Handover Guide The handover guide explains the overall structure of the main handover section, and gives guidelines for what should be handed over between semesters. Review Checklists The review checklists section contains all the checklists used for code review.","title":"Overview"},{"location":"Legacy/Handover/#overview","text":"This handover is a compilation of materials used by prior GIRAF teams. The handover from the semesters are organized in sections named after the semesters year. Handover Guide The handover guide explains the overall structure of the main handover section, and gives guidelines for what should be handed over between semesters. Review Checklists The review checklists section contains all the checklists used for code review.","title":"Overview"},{"location":"Legacy/Handover/handover_guide/","text":"Handover Guide \u00b6 This guide explains which sections that should be added or updated in the Wiki whenever a GIRAF team is finished with a semester. New Sections \u00b6 Here is a description of the new sections that has to be added to the Wiki. Main Handover \u00b6 The placement of the main handover from a GIRAF team is illustrated below. The five main things to include are: Description of role structure Description of sprint events Description of the tools that has been used (Scripts, Google Sheets etc.) Description of the overarching features that have yet to be implemented and list the issues related to these Description of incomplete issues that should continue being worked on 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Handover \u251c\u2500\u2500 Handover Guide \u251c\u2500\u2500 Semester name # New section \u2502 \u251c\u2500\u2500 Role Structure # \u2502 \u251c\u2500\u2500 Sprint Events # \u2502 \u251c\u2500\u2500 Tools # \u2502 \u2502 \u251c\u2500\u2500 Process Group # \u2502 \u2502 \u2514\u2500\u2500 Other Tools # | \u251c\u2500\u2500 Feature Overview \u2502 \u2514\u2500\u2500 Issues \u251c\u2500\u2500 ... \u251c\u2500\u2500 2020E \u251c\u2500\u2500 2020F \u251c\u2500\u2500 2019 \u2514\u2500\u2500 2018 Semester name is the year a semester is held e.g. 2020. 2020 is followed by E or F because GIRAF was worked on in both the Spring (F) and Autumn (E) semester. Feature Overview \u00b6 This document should describe overarching features that have multiple related issues. The purpose of this document is to make it easy for the next GIRAF teams to get an overview of these larger features without having to go through all issues in all the repositories. For each feature there should be a headline in the feature_overview.md file. Under this headline there should be a description of the feature, as well as a table containing all the issues related to the feature. The table should have a column containing links to the issues, and a column containing their descriptions. This description can simply be the title of the issue. Issues Handover \u00b6 Any fundamental issues that have had work done on them can also be handed over to the next GIRAF team. These issues can or cannot have an active pull request. In order to make it possible for future GIRAF teams to work on such issues, an adequate amount of documentation should be written about the issues in the Issues folder in the main handover. For these kinds of issues there are two recommendations: If an issue warrants continued work by a future GIRAF team, then the issue should have handover documentation on the wiki. If there is handover material for the issue, then its branch should NOT be deleted. If the issue has a pull request, the pull request SHOULD be rejected. If the work done on an issue is not worthy for handover, then its branch should be deleted. Then, the assignee should document what they tried to do to solve the issue as a comment to the issue itself on GitHub. Updated Sections \u00b6 Here is a description of the sections that needs to be updated. Prototypes \u00b6 The PO group should add links for the new prototype files. 1 2 3 UI_Design \u2514\u2500\u2500 Prototypes \u2514\u2500\u2500 Overview # File to update The newest entry should be the top most in the table shown in the overview.","title":"Handover Guide"},{"location":"Legacy/Handover/handover_guide/#handover-guide","text":"This guide explains which sections that should be added or updated in the Wiki whenever a GIRAF team is finished with a semester.","title":"Handover Guide"},{"location":"Legacy/Handover/handover_guide/#new-sections","text":"Here is a description of the new sections that has to be added to the Wiki.","title":"New Sections"},{"location":"Legacy/Handover/handover_guide/#main-handover","text":"The placement of the main handover from a GIRAF team is illustrated below. The five main things to include are: Description of role structure Description of sprint events Description of the tools that has been used (Scripts, Google Sheets etc.) Description of the overarching features that have yet to be implemented and list the issues related to these Description of incomplete issues that should continue being worked on 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Handover \u251c\u2500\u2500 Handover Guide \u251c\u2500\u2500 Semester name # New section \u2502 \u251c\u2500\u2500 Role Structure # \u2502 \u251c\u2500\u2500 Sprint Events # \u2502 \u251c\u2500\u2500 Tools # \u2502 \u2502 \u251c\u2500\u2500 Process Group # \u2502 \u2502 \u2514\u2500\u2500 Other Tools # | \u251c\u2500\u2500 Feature Overview \u2502 \u2514\u2500\u2500 Issues \u251c\u2500\u2500 ... \u251c\u2500\u2500 2020E \u251c\u2500\u2500 2020F \u251c\u2500\u2500 2019 \u2514\u2500\u2500 2018 Semester name is the year a semester is held e.g. 2020. 2020 is followed by E or F because GIRAF was worked on in both the Spring (F) and Autumn (E) semester.","title":"Main Handover"},{"location":"Legacy/Handover/handover_guide/#feature-overview","text":"This document should describe overarching features that have multiple related issues. The purpose of this document is to make it easy for the next GIRAF teams to get an overview of these larger features without having to go through all issues in all the repositories. For each feature there should be a headline in the feature_overview.md file. Under this headline there should be a description of the feature, as well as a table containing all the issues related to the feature. The table should have a column containing links to the issues, and a column containing their descriptions. This description can simply be the title of the issue.","title":"Feature Overview"},{"location":"Legacy/Handover/handover_guide/#issues-handover","text":"Any fundamental issues that have had work done on them can also be handed over to the next GIRAF team. These issues can or cannot have an active pull request. In order to make it possible for future GIRAF teams to work on such issues, an adequate amount of documentation should be written about the issues in the Issues folder in the main handover. For these kinds of issues there are two recommendations: If an issue warrants continued work by a future GIRAF team, then the issue should have handover documentation on the wiki. If there is handover material for the issue, then its branch should NOT be deleted. If the issue has a pull request, the pull request SHOULD be rejected. If the work done on an issue is not worthy for handover, then its branch should be deleted. Then, the assignee should document what they tried to do to solve the issue as a comment to the issue itself on GitHub.","title":"Issues Handover"},{"location":"Legacy/Handover/handover_guide/#updated-sections","text":"Here is a description of the sections that needs to be updated.","title":"Updated Sections"},{"location":"Legacy/Handover/handover_guide/#prototypes","text":"The PO group should add links for the new prototype files. 1 2 3 UI_Design \u2514\u2500\u2500 Prototypes \u2514\u2500\u2500 Overview # File to update The newest entry should be the top most in the table shown in the overview.","title":"Prototypes"},{"location":"Legacy/Handover/2018/FutureWork/","text":"Future Work \u00b6 Most of these tasks consists of technical tasks with the purpose of improving the API hence its maintainability, readability and extensibility. The list has been prioritized according to what we believe is most important. Each task contains a relative estimate for how long time we believe the tasks should take relative to each other. Many of these tasks are small and most of these tasks should be able to be finished in at most 1 sprint except rewriting unit tests. Recommended Backend Changes \u00b6 (medium/long) Support parents in the system (currently the database has support for creating a guardian-citizen relationship between a guardian and a citizen. Parent can be easily implemented by adding a new role and a parent controller without extending the structure of the database (arguably girafuserDTO can be split up to one DTO for each role but that is your choice). The admin panel should be extended to support adding a parent. (medium) Pictogram search is currently a bit slow on the production database (where there is 15k pictograms) the algorithm should be improved or changed. This issue was discovered late and really not a focus to implement. (short) The admin panel should support setting screenname when creating a new user and not only username. Furthermore UserNameDTO should be replayed by ScreenNameDTO as the users be able to have the same screen name i.e if they have the same names. (short) It was discovered very late in sprint 4 after we have made the acceptance test that the settings table (which we got from last year) was a 1-n relation instead of a 0..1-1 relation. As we already made the acceptance test and that the API only support 1-1 we did not fix this problem as we did not want to change code after the acceptance test. However you should fix the database structure such that a User only have one setting. This is simply implemented as you just let the Key on the setting table also be a FK to the GirafUser table and then delete the current FK from the GirafUser table. After that you modify the up and down methods in the migration script to make the proper table transformations (the easiest thing would be to nuke the migration script, but we DO NOT recommend this). (short) Find out from the stakeholders who should be able to see which pictograms. Currently: Everyone can see public pictograms, only the user itself can see private all users in an department can see protected. Depends on what you find out you might have to change this (for now we assume it to be true as this was the structure left for us by last years students and has not been the focus this year (2018). This is related to the Obsolete methods in the API. Extend admin panel to support editing icon of users. (short) Add more extensive logging i.e. log IP address. (short) Update EF core and use lazy loading (this feature should be available when you get the API it is currently not supported by our MySQL provider as it is only in beta mode). This means that eager loading should be entirely removed from the API. (long) Make unit tests easier maintainable by i.e. using AutoFixture. (medium) Move all model checking to DTOs. (short) For consistency rename Key to Id a and ScreenName to DisplayName all places (this was not done this year as we did not want to break anything late in the last sprint). Recommended changes to the Administration Panel \u00b6 Setting the screen name when creating a citizen, guardian, or department. When changing the name of a department, the user name of the associated department user should also change. Rename \\textit{p\u00e6dagog} to \\textit{medarbejder}. The log out button should be more visible. WARNING: Do not do as old groups and redo the API from scratch. This causes several years of work to be lost (this goes for all components of Giraf) NOTE: We recommend that any larger modifications to the API is done as early as possible and that stable releases are kept on separate release branches","title":"Future Work"},{"location":"Legacy/Handover/2018/FutureWork/#future-work","text":"Most of these tasks consists of technical tasks with the purpose of improving the API hence its maintainability, readability and extensibility. The list has been prioritized according to what we believe is most important. Each task contains a relative estimate for how long time we believe the tasks should take relative to each other. Many of these tasks are small and most of these tasks should be able to be finished in at most 1 sprint except rewriting unit tests.","title":"Future Work"},{"location":"Legacy/Handover/2018/FutureWork/#recommended-backend-changes","text":"(medium/long) Support parents in the system (currently the database has support for creating a guardian-citizen relationship between a guardian and a citizen. Parent can be easily implemented by adding a new role and a parent controller without extending the structure of the database (arguably girafuserDTO can be split up to one DTO for each role but that is your choice). The admin panel should be extended to support adding a parent. (medium) Pictogram search is currently a bit slow on the production database (where there is 15k pictograms) the algorithm should be improved or changed. This issue was discovered late and really not a focus to implement. (short) The admin panel should support setting screenname when creating a new user and not only username. Furthermore UserNameDTO should be replayed by ScreenNameDTO as the users be able to have the same screen name i.e if they have the same names. (short) It was discovered very late in sprint 4 after we have made the acceptance test that the settings table (which we got from last year) was a 1-n relation instead of a 0..1-1 relation. As we already made the acceptance test and that the API only support 1-1 we did not fix this problem as we did not want to change code after the acceptance test. However you should fix the database structure such that a User only have one setting. This is simply implemented as you just let the Key on the setting table also be a FK to the GirafUser table and then delete the current FK from the GirafUser table. After that you modify the up and down methods in the migration script to make the proper table transformations (the easiest thing would be to nuke the migration script, but we DO NOT recommend this). (short) Find out from the stakeholders who should be able to see which pictograms. Currently: Everyone can see public pictograms, only the user itself can see private all users in an department can see protected. Depends on what you find out you might have to change this (for now we assume it to be true as this was the structure left for us by last years students and has not been the focus this year (2018). This is related to the Obsolete methods in the API. Extend admin panel to support editing icon of users. (short) Add more extensive logging i.e. log IP address. (short) Update EF core and use lazy loading (this feature should be available when you get the API it is currently not supported by our MySQL provider as it is only in beta mode). This means that eager loading should be entirely removed from the API. (long) Make unit tests easier maintainable by i.e. using AutoFixture. (medium) Move all model checking to DTOs. (short) For consistency rename Key to Id a and ScreenName to DisplayName all places (this was not done this year as we did not want to break anything late in the last sprint).","title":"Recommended Backend Changes"},{"location":"Legacy/Handover/2018/FutureWork/#recommended-changes-to-the-administration-panel","text":"Setting the screen name when creating a citizen, guardian, or department. When changing the name of a department, the user name of the associated department user should also change. Rename \\textit{p\u00e6dagog} to \\textit{medarbejder}. The log out button should be more visible. WARNING: Do not do as old groups and redo the API from scratch. This causes several years of work to be lost (this goes for all components of Giraf) NOTE: We recommend that any larger modifications to the API is done as early as possible and that stable releases are kept on separate release branches","title":"Recommended changes to the Administration Panel"},{"location":"Legacy/Handover/2019/","text":"Overview \u00b6 The following will describe the process used in the 2019 project of the GIRAF project. It was updated at the very end of the project, to include all adaptations of processes, to help the year of 2020 get a headstart on the project. Most decisions will be described to explain the circumstances and arguments that made it the preferable decision at the time. We recommend that the year of 2020 try to replicate this process, and change it to match inevitable changes to your circumstances. Best of luck. How to read this guide \u00b6 This guide is intended to be read partly as a cover-to-cover guide of the process that we developed during the semester. The primary focus, however, is for it to be a guide that you can go and look at, every time you are in doubt about the meaning of something. If you are a process group member, you will probably benefit from reading it almost cover-to-cover or keeping it very close to you in the first few weeks. If you are part of a development group, you will benefit greatly from the last sections that describe how the code lifecycle works. Other than that, you might be able to use the rest of the sections for reference in your report. Introduction for the next process group \u00b6 You have been chosen as the process group of this semester. It might be against your will, but nonetheless it's an important job that needs to be done. If this is at the very beginning of the semester, we recommend that you do of course read the entirety of the process manual, but for a quick-start you might read the following sections: General development in Process group information Project meetings Especially sprint intro and cross-group standups How to handle roles in Process group information Changing the process Your responsibility is to define how project groups do everything: from writing proper code to meetings and releasing the code. Just remember: especially in the beginning it's a good idea to just take quick decisions and correct errors later. 30-40 people wait just for you to get started, so any plan is better than no plan. We do of course recommend our plan. That's still better than no plan. To avoid micromanaging we don't care for how groups do things internally. We only care about things that affect other groups as well. So if one groups wants to work hardcore Scrum, and another don't want any formal process, let them. Just make sure that they participate in the common activities that you define, and make sure that you can reach them in normal work hours (preferably in person.) Last but not least: Good luck, and remember that agile approaches (especially scrum) is not the answer! Both us and numerous groups before us experienced this!","title":"Overview"},{"location":"Legacy/Handover/2019/#overview","text":"The following will describe the process used in the 2019 project of the GIRAF project. It was updated at the very end of the project, to include all adaptations of processes, to help the year of 2020 get a headstart on the project. Most decisions will be described to explain the circumstances and arguments that made it the preferable decision at the time. We recommend that the year of 2020 try to replicate this process, and change it to match inevitable changes to your circumstances. Best of luck.","title":"Overview"},{"location":"Legacy/Handover/2019/#how-to-read-this-guide","text":"This guide is intended to be read partly as a cover-to-cover guide of the process that we developed during the semester. The primary focus, however, is for it to be a guide that you can go and look at, every time you are in doubt about the meaning of something. If you are a process group member, you will probably benefit from reading it almost cover-to-cover or keeping it very close to you in the first few weeks. If you are part of a development group, you will benefit greatly from the last sections that describe how the code lifecycle works. Other than that, you might be able to use the rest of the sections for reference in your report.","title":"How to read this guide"},{"location":"Legacy/Handover/2019/#introduction-for-the-next-process-group","text":"You have been chosen as the process group of this semester. It might be against your will, but nonetheless it's an important job that needs to be done. If this is at the very beginning of the semester, we recommend that you do of course read the entirety of the process manual, but for a quick-start you might read the following sections: General development in Process group information Project meetings Especially sprint intro and cross-group standups How to handle roles in Process group information Changing the process Your responsibility is to define how project groups do everything: from writing proper code to meetings and releasing the code. Just remember: especially in the beginning it's a good idea to just take quick decisions and correct errors later. 30-40 people wait just for you to get started, so any plan is better than no plan. We do of course recommend our plan. That's still better than no plan. To avoid micromanaging we don't care for how groups do things internally. We only care about things that affect other groups as well. So if one groups wants to work hardcore Scrum, and another don't want any formal process, let them. Just make sure that they participate in the common activities that you define, and make sure that you can reach them in normal work hours (preferably in person.) Last but not least: Good luck, and remember that agile approaches (especially scrum) is not the answer! Both us and numerous groups before us experienced this!","title":"Introduction for the next process group"},{"location":"Legacy/Handover/2019/changing_the_process/","text":"Changing the process \u00b6 Please read this entire section. Please revisit this section, when you want to change something described in this section. And please, please, please, make informed decisions that have a very high possibility of being the best decision for many years to come. Explanation on our Working Method \u00b6 We have had multiple influences for the process that we follow. This section will try to describe some of them, so your foundation for making informed decisions will be stronger and close to the insights that we had. Carsten Ruseng Jakobsen \u00b6 We had a meeting with Carsten Ruseng Jakobsen, the Software Engineering lecturer. We can only suggest that you do the same. Carsten can bring a lot of insight into the changes that you probably want to do, and can help you understand if different approaches are a good idea or not. A more plan-driven approach \u00b6 We like agile, you probably like agile, and Carsten likes agile. Even then, we strongly recommend that you do not practice a clear agile approach. If you ask Carsten, he will tell you that, while agile is good, it isn't suited for this project. As the next section describes, agile takes time to refine. Scrum takes time to refine \u00b6 One of the major points that Carsten presented, and that we experienced in the first sprint before we talked to Carsten, was that Scrum takes time to refine. Assembling a new team of developers, or even introducing new developers to an existing team, will throw the method off course for while. This also means that all estimation and poker planning will be off for most of the teams in the beginning. If one team assigns themselves to a lot of high-priority tasks, other teams ahead of schedule can't help clear these, because they are already taken. Carsten estimates that for a team to be up and running smoothly, they should expect about a year's time. You only have around 4 months. In Scrum, estimation takes a lot of time \u00b6 When poker planning is held, it takes a lot of time. Last year they started by having the poker planning collectively, with all 40 developers participating in the same poker planning session. This year we tried to have poker planning in the individual groups. When a group finished splitting up an issue and estimating it, they would take a new issue and do the same. This posed a few problems, the first being that it was still time-consuming. Other than that, we experienced that some groups quickly split up and estimated issues, while others were slower. This meant that some groups would take 4-5 high-priority issues, while another group only managed to take one or two, and lower priority tasks as the rest. As such, the first group would spend the entire sprint clearing the high-priority tasks, and the other group would spend half their sprint working on tasks with lower priority. Since the Scrum process isn't refined, this happened a lot, meaning that a lot of high-priority tasks were locked and delayed for several weeks. Instead we suggest that groups are given as many (or as few) tasks at a time, as they can manage. If they can work on two or three tasks in parallel, please do give the tasks to them. However, if teams want to be assigned an issue because they will soon have created pull requests for their other issues, they shouldn't be allowed to do this. Hogging issues is strongly discouraged. Note that when a group creates a pull request, they will often have time to take a new issue - so even though the pull request isn't merged, they should be allowed to take a new one. Social gatherings \u00b6 We haven't had great success with social gatherings. This is true for events that are purely social. F-klubben provides plenty of these. Events that combine both social and professional events had greater success, but we did not necessarily have great success with them. The most popular events were hackathons, where students with particular insight into different subjects shared their knowledge. Examples of this could be a hackathon for Flutter, testing, or the BLoC pattern. It may be beneficial to try to arrange something extraordinary, such as trying to get someone from the Google Developer Group Aalborg to come around and give an introduction to Flutter, as it may increase the likelihood that people will participate. Development process \u00b6 This section will describe different thoughts about the process relating to the development. Generally we tried to choose as popular technologies as possible, or technologies that seem to have emerging popularity. This increases the likelihood of current and future students being familiar with the technologies. Language \u00b6 We have chosen to work with the Flutter framework, which uses the Dart language rather than Xamarin, which was previously used. There is a multitude of reasons for choosing Flutter. Cross-platform \u00b6 When you develop something in Flutter, it can run on both Android and iOS. Some of the users have iPads, and others have Android tablets. Instead of developing two apps that look exactly the same, you write the code in one language (Dart), and run it on both platforms. Cross-compilation \u00b6 Xamarin is owned by Microsoft, and does not support Linux products - meaning that Xamarin cannot officially be compiled on Linux. However, compiling Xamarin on Linux was possible, but it took a lot of effort to set up, and a lot of effort to compile the code. Even when finding an efficient way of doing the compilation, it will depend on unofficial ways of doing it. We had 11 developers running Linux. We don't know how many you have, and you don't know how many they have next year. Please keep this in mind, if you consider dismissing this point because you don't have a lot of Linux users. Compiles to native code \u00b6 Flutter will compile directly to native iOS and Android code. Xamarin runs in a container, meaning it is less efficient, as it has to be translated afterwards. As Flutter directly compiles to native code it will run much more smoothly and much more efficiently. Hot reload \u00b6 You will love hot reload. If you run the app on your phone while developing, every time you hit save, the app will be reloaded in a quarter of a second. And it feels that fast. Bias of experience \u00b6 Previous years mentioned changing to another language because \"that's what the developers felt most comfortable with.\" No one from this year had any previous experience with neither Dart nor Flutter. We have no guarantee that you know anything about Dart and Flutter, and you don't know if next year will. As such, this argument should be dismissed entirely. Considering experience is only relevant in a very short term. Instead, accessibility and documentation has long term relevancy, as this is what will help all the developers that don't know the language. A few developers with a lot of experience can't lift the entire project anyway. And they definitely can't help next year. GitHub \u00b6 We changed from GitLab to GitHub. GitLab was hosted on one of AAU's servers, but we choose to let GitHub host the project. Below are the reasons. Self-hosting gives you the responsibility \u00b6 If the GitLab solution which we had to self-host was kept, we would be responsible for server maintenance on this part. This does not make any sense. There exists plenty of alternative solutions (read: GitHub) that relieves all of the pain of hosting. And frankly; we trust GitHub more in terms of up-time than we trust ourselves and the AAU servers. Also, in the very beginning of the project the servers crashed. Since all information about the servers were hosted on the servers, it took us two weeks to get them back online. We essentially wasted the first two weeks of the semester, because the servers depended on us for maintenance. The last point for this section is that since you have the responsibility, the servers will not be monitored for half a year. Since the project is open-source, and external collaborators have shown interest, you will be leaving the server unguarded for half a year. If the servers break down a week after you hand in the project, it will likely take half a year for someone to get it back online. That won't happen on GitHub. Familiarity \u00b6 GitHub is much more popular, meaning that it is much more likely that people will have worked with GitHub instead of GitLab. Since there exists such a familiar option, it is a no-brainer to choose that. There is a much higher probability that your current semester and the following semesters will have more people that are familiar with GitHub. So even if that is not the case on your semester, or you have a few people who claim to be GitLab experts, please refrain from choosing it! It will mess up future semester who will be bound to go your route. Azure pipelines \u00b6 We run everything related to continuous integration (CI) on an Azure server. We did this for a few reasons. 10 free and parallel build threads \u00b6 Since the GIRAF project is an open source project, Azure allows for 10 free and parallel build threads. This means that 10 developers can have the CI checks run at once, instead of waiting for them to finish one after another. This is really nice for release preparation where a lot of small changes are often submitted at the same time. Not self-hosted \u00b6 Once again, not being self-hosted is a major advantage. We trust that Microsoft are much more competent in keeping the servers up and running than we are. It also allows for the parallel build threads, and we don't have to worry about scaling and maintenance. Builds on Mac \u00b6 The major reason for choosing Azure is the ability to build on a Mac environment. Checking if the app actually works on iOS requires a Mac environment. This is decided by Apple, as they haven't released any official ways to compile iOS code on other environments than Mac. As such, if you want the CI to run checks on both Android and iOS (and you really do!), then you have to use Azure. Server \u00b6 We have begun work to remove as much responsibility about the server as possible. Everything that we can let others host, we do. Everything that we can create stable automation from, we have. We have done this due to a very simply philosophy: Maintaining our own servers is like writing an app in C, without libraries. It is possible to write an app in C without libraries. But it is a really stupid idea to insist on handling every responsibility yourself, especially when free services exist that will make it not-harder, better, faster, stronger. Earlier both CI and the repositories were hosted on the AAU servers. We moved these out to their own. Now the only thing left on the AAU servers are the database and the API. It still takes a bit of maintenance once in a while, and especially if you want to change it. But the server group should figure this out, and the process group should simply suggest to them that they do everything they can to eliminate the amount of time spent working on servers.","title":"Changing the process"},{"location":"Legacy/Handover/2019/changing_the_process/#changing-the-process","text":"Please read this entire section. Please revisit this section, when you want to change something described in this section. And please, please, please, make informed decisions that have a very high possibility of being the best decision for many years to come.","title":"Changing the process"},{"location":"Legacy/Handover/2019/changing_the_process/#explanation-on-our-working-method","text":"We have had multiple influences for the process that we follow. This section will try to describe some of them, so your foundation for making informed decisions will be stronger and close to the insights that we had.","title":"Explanation on our Working Method"},{"location":"Legacy/Handover/2019/changing_the_process/#carsten-ruseng-jakobsen","text":"We had a meeting with Carsten Ruseng Jakobsen, the Software Engineering lecturer. We can only suggest that you do the same. Carsten can bring a lot of insight into the changes that you probably want to do, and can help you understand if different approaches are a good idea or not.","title":"Carsten Ruseng Jakobsen"},{"location":"Legacy/Handover/2019/changing_the_process/#a-more-plan-driven-approach","text":"We like agile, you probably like agile, and Carsten likes agile. Even then, we strongly recommend that you do not practice a clear agile approach. If you ask Carsten, he will tell you that, while agile is good, it isn't suited for this project. As the next section describes, agile takes time to refine.","title":"A more plan-driven approach"},{"location":"Legacy/Handover/2019/changing_the_process/#scrum-takes-time-to-refine","text":"One of the major points that Carsten presented, and that we experienced in the first sprint before we talked to Carsten, was that Scrum takes time to refine. Assembling a new team of developers, or even introducing new developers to an existing team, will throw the method off course for while. This also means that all estimation and poker planning will be off for most of the teams in the beginning. If one team assigns themselves to a lot of high-priority tasks, other teams ahead of schedule can't help clear these, because they are already taken. Carsten estimates that for a team to be up and running smoothly, they should expect about a year's time. You only have around 4 months.","title":"Scrum takes time to refine"},{"location":"Legacy/Handover/2019/changing_the_process/#in-scrum-estimation-takes-a-lot-of-time","text":"When poker planning is held, it takes a lot of time. Last year they started by having the poker planning collectively, with all 40 developers participating in the same poker planning session. This year we tried to have poker planning in the individual groups. When a group finished splitting up an issue and estimating it, they would take a new issue and do the same. This posed a few problems, the first being that it was still time-consuming. Other than that, we experienced that some groups quickly split up and estimated issues, while others were slower. This meant that some groups would take 4-5 high-priority issues, while another group only managed to take one or two, and lower priority tasks as the rest. As such, the first group would spend the entire sprint clearing the high-priority tasks, and the other group would spend half their sprint working on tasks with lower priority. Since the Scrum process isn't refined, this happened a lot, meaning that a lot of high-priority tasks were locked and delayed for several weeks. Instead we suggest that groups are given as many (or as few) tasks at a time, as they can manage. If they can work on two or three tasks in parallel, please do give the tasks to them. However, if teams want to be assigned an issue because they will soon have created pull requests for their other issues, they shouldn't be allowed to do this. Hogging issues is strongly discouraged. Note that when a group creates a pull request, they will often have time to take a new issue - so even though the pull request isn't merged, they should be allowed to take a new one.","title":"In Scrum, estimation takes a lot of time"},{"location":"Legacy/Handover/2019/changing_the_process/#social-gatherings","text":"We haven't had great success with social gatherings. This is true for events that are purely social. F-klubben provides plenty of these. Events that combine both social and professional events had greater success, but we did not necessarily have great success with them. The most popular events were hackathons, where students with particular insight into different subjects shared their knowledge. Examples of this could be a hackathon for Flutter, testing, or the BLoC pattern. It may be beneficial to try to arrange something extraordinary, such as trying to get someone from the Google Developer Group Aalborg to come around and give an introduction to Flutter, as it may increase the likelihood that people will participate.","title":"Social gatherings"},{"location":"Legacy/Handover/2019/changing_the_process/#development-process","text":"This section will describe different thoughts about the process relating to the development. Generally we tried to choose as popular technologies as possible, or technologies that seem to have emerging popularity. This increases the likelihood of current and future students being familiar with the technologies.","title":"Development process"},{"location":"Legacy/Handover/2019/changing_the_process/#language","text":"We have chosen to work with the Flutter framework, which uses the Dart language rather than Xamarin, which was previously used. There is a multitude of reasons for choosing Flutter.","title":"Language"},{"location":"Legacy/Handover/2019/changing_the_process/#cross-platform","text":"When you develop something in Flutter, it can run on both Android and iOS. Some of the users have iPads, and others have Android tablets. Instead of developing two apps that look exactly the same, you write the code in one language (Dart), and run it on both platforms.","title":"Cross-platform"},{"location":"Legacy/Handover/2019/changing_the_process/#cross-compilation","text":"Xamarin is owned by Microsoft, and does not support Linux products - meaning that Xamarin cannot officially be compiled on Linux. However, compiling Xamarin on Linux was possible, but it took a lot of effort to set up, and a lot of effort to compile the code. Even when finding an efficient way of doing the compilation, it will depend on unofficial ways of doing it. We had 11 developers running Linux. We don't know how many you have, and you don't know how many they have next year. Please keep this in mind, if you consider dismissing this point because you don't have a lot of Linux users.","title":"Cross-compilation"},{"location":"Legacy/Handover/2019/changing_the_process/#compiles-to-native-code","text":"Flutter will compile directly to native iOS and Android code. Xamarin runs in a container, meaning it is less efficient, as it has to be translated afterwards. As Flutter directly compiles to native code it will run much more smoothly and much more efficiently.","title":"Compiles to native code"},{"location":"Legacy/Handover/2019/changing_the_process/#hot-reload","text":"You will love hot reload. If you run the app on your phone while developing, every time you hit save, the app will be reloaded in a quarter of a second. And it feels that fast.","title":"Hot reload"},{"location":"Legacy/Handover/2019/changing_the_process/#bias-of-experience","text":"Previous years mentioned changing to another language because \"that's what the developers felt most comfortable with.\" No one from this year had any previous experience with neither Dart nor Flutter. We have no guarantee that you know anything about Dart and Flutter, and you don't know if next year will. As such, this argument should be dismissed entirely. Considering experience is only relevant in a very short term. Instead, accessibility and documentation has long term relevancy, as this is what will help all the developers that don't know the language. A few developers with a lot of experience can't lift the entire project anyway. And they definitely can't help next year.","title":"Bias of experience"},{"location":"Legacy/Handover/2019/changing_the_process/#github","text":"We changed from GitLab to GitHub. GitLab was hosted on one of AAU's servers, but we choose to let GitHub host the project. Below are the reasons.","title":"GitHub"},{"location":"Legacy/Handover/2019/changing_the_process/#self-hosting-gives-you-the-responsibility","text":"If the GitLab solution which we had to self-host was kept, we would be responsible for server maintenance on this part. This does not make any sense. There exists plenty of alternative solutions (read: GitHub) that relieves all of the pain of hosting. And frankly; we trust GitHub more in terms of up-time than we trust ourselves and the AAU servers. Also, in the very beginning of the project the servers crashed. Since all information about the servers were hosted on the servers, it took us two weeks to get them back online. We essentially wasted the first two weeks of the semester, because the servers depended on us for maintenance. The last point for this section is that since you have the responsibility, the servers will not be monitored for half a year. Since the project is open-source, and external collaborators have shown interest, you will be leaving the server unguarded for half a year. If the servers break down a week after you hand in the project, it will likely take half a year for someone to get it back online. That won't happen on GitHub.","title":"Self-hosting gives you the responsibility"},{"location":"Legacy/Handover/2019/changing_the_process/#familiarity","text":"GitHub is much more popular, meaning that it is much more likely that people will have worked with GitHub instead of GitLab. Since there exists such a familiar option, it is a no-brainer to choose that. There is a much higher probability that your current semester and the following semesters will have more people that are familiar with GitHub. So even if that is not the case on your semester, or you have a few people who claim to be GitLab experts, please refrain from choosing it! It will mess up future semester who will be bound to go your route.","title":"Familiarity"},{"location":"Legacy/Handover/2019/changing_the_process/#azure-pipelines","text":"We run everything related to continuous integration (CI) on an Azure server. We did this for a few reasons.","title":"Azure pipelines"},{"location":"Legacy/Handover/2019/changing_the_process/#10-free-and-parallel-build-threads","text":"Since the GIRAF project is an open source project, Azure allows for 10 free and parallel build threads. This means that 10 developers can have the CI checks run at once, instead of waiting for them to finish one after another. This is really nice for release preparation where a lot of small changes are often submitted at the same time.","title":"10 free and parallel build threads"},{"location":"Legacy/Handover/2019/changing_the_process/#not-self-hosted","text":"Once again, not being self-hosted is a major advantage. We trust that Microsoft are much more competent in keeping the servers up and running than we are. It also allows for the parallel build threads, and we don't have to worry about scaling and maintenance.","title":"Not self-hosted"},{"location":"Legacy/Handover/2019/changing_the_process/#builds-on-mac","text":"The major reason for choosing Azure is the ability to build on a Mac environment. Checking if the app actually works on iOS requires a Mac environment. This is decided by Apple, as they haven't released any official ways to compile iOS code on other environments than Mac. As such, if you want the CI to run checks on both Android and iOS (and you really do!), then you have to use Azure.","title":"Builds on Mac"},{"location":"Legacy/Handover/2019/changing_the_process/#server","text":"We have begun work to remove as much responsibility about the server as possible. Everything that we can let others host, we do. Everything that we can create stable automation from, we have. We have done this due to a very simply philosophy: Maintaining our own servers is like writing an app in C, without libraries. It is possible to write an app in C without libraries. But it is a really stupid idea to insist on handling every responsibility yourself, especially when free services exist that will make it not-harder, better, faster, stronger. Earlier both CI and the repositories were hosted on the AAU servers. We moved these out to their own. Now the only thing left on the AAU servers are the database and the API. It still takes a bit of maintenance once in a while, and especially if you want to change it. But the server group should figure this out, and the process group should simply suggest to them that they do everything they can to eliminate the amount of time spent working on servers.","title":"Server"},{"location":"Legacy/Handover/2019/how_to/","text":"How to's for developers \u00b6 This file is intended for helping developers understand their role in the process. How to: Get an issue to work on \u00b6 If you have enough time to work on another issue after finishing your previous ones you should get another. Try to aim for no more than two or three issues, as you shouldn't take on another issue before you can actually work on it. There's no need for you to be assigned to an issue that you can't work on right now, as it might block others progress. When you have decided that you have enough time for a new issue, you should go and get one. This process is very simple. You have two options: Option number 1 \u00b6 You go to the issues tab of one of the repositories, e.g. github.com/aau-giraf/weekplanner/issues and locate an interesting issue. Then, when you have located the issue, you note the number. That is denoted by the # and a number. Remember the number and do a time estimation of the issue. Thereafter go to the product owner (PO) group, and ask if you can work on this issue. The PO group might say no for various reasons. Just remember: they have the overview of the project and probably have good reasons for saying as they do. There is usually a greater chance of getting a yes if the issue you've picked is either highest or high priority. Option number 2 \u00b6 Go ask the PO group to be assigned the most pressing issue, as they have a good overview of the project, they will most likely have some issues that they would love to have you work on. How to: Write code \u00b6 To get started writing code, you should first setup the project. This is described in the Setup from scratch tutorial . Then, after setting up the code, you should make a branch for the feature you are going to implement. Name the branch fixes_ISSUENUMBER, where the ISSUENUMBER is the number of the issue you are implementing. Follow the default coding standards that you can find. For Flutter the linter should catch most things that go out of style. If in doubt, try to find other places in the code that does similar things to what you want, and follow that standard. Remember that all functionality related to an issue should be implemented with unit tests before an issue is considered done. In the How to review section you can read more about how that will happen. Make sure that you plan for the pull request to go through as fast as possible. And if you are in doubt and stuck, just go ask another group for help, as there are a lot of clever people on the GIRAF project, and they usually love to help. How to: Create a pull request \u00b6 When development of a piece of software is done, and it is deemed that the code solves the assigned issue, a pull request should be made. Before making the pull request, make sure that the code: Only relates to a single issue (One PR per user story). Is fully tested. Is reachable when opening the application. Fully tested means that if any piece of the functionality is removed, a test should fail. To create the pull request, you should go to the relevant repository, click on pull requests and click the green \"New Pull Request\" button. The base branch should be \"develop\" and the compare branch should be your feature branch, with the code that you want to add to the develop branch. Then click the green \"Create pull request\" button. Name the pull request the same as your branch. To the right there is a tab called projects. Add the request to the current GIRAF project. For the first sprint you would add it to the project called \"2020 1. sprint\", for the second sprint \"2020 2. sprint\" and so on. Next, include pictures (or a gif/video) of all the screens that you added, in the text field, so someone from the product owner group can verify that the screens follow the design manual. Remember to include the text \"fixes #YourIssueID\". This will automatically close the issue when the pull request is merged. When the PR is closed, check your issue to make sure that it has been closed too. Now all that is left is for you to inform the process group, and ask them to assign reviewers. They will assign reviewers who will review your pull request. How To: Get Code in Review \u00b6 In order to get code into review, you firstly have to make sure that the tests you wrote all pass, and that they cover the relevant parts of the written code. The way you make a pull request (PR) starts by visiting github.com. You navigate to the repository in which you've done your work. Go to the pull requests tab, click the \"new pull request\" button. Then you select the correct base and compare branch, and then your create a pull request. Otherwise, if you have recently pushed commits github will actually suggest if you want to create a pull requests, this eases the process. When a pull request have been created, you have to notify the process group, write them or talk to them. The process group is responsible for delegating the two reviews which are needed to merge the pull request. The review will be performed by others than the group who wrote the code, thus we use external review. Once the pull request is in the review phase, you should be able to in the meantime work on other user stories, or simply write about the user story in your report. When other groups review you PR, they most likely have some form of feedback, comments, or some things they do not understand. Thus it is to prefer if both the reviewers and the ones responsible for the pull request are quickly to respond to comments on github. Once the reviewers have approved the pull request is able to merged. How To: Review a Pull Request \u00b6 When reviewing a pull request it is absolutely important that you consider the goal of the review: Value should be added to the project, and the developers involved should gain knowledge and improve. As such, it is important to keep a light and educational tone when involved with the pull requests. When reviewing a pull request, you should check for points in the Code Review Checklist . In case a point is not fulfilled you should leave a comment that points out the problem, why it is a problem, and, if possible, how to develop a solution. When reviewing the code, you can click on the \"Files Changed\" tab, in which you will be able to see all changes to the code. If you click on the left side of the code, right besides the line numbers, there will be a small, blue plus button available, which will let you leave a comment to this specific line of the code. If possible, you should always leave a comment on the line of the code that is problematic. As a reviewer you should never commit to the branch with the pull request, merge the pull request, or delete the pull request. Since the first two actions might introduce problems to the codebase, the author of the pull request should always do these things, as that person will be best suited to handle potential problems. If you are reviewing a complex piece of code, or a particularly problematic piece of code, it is highly advisable that you locate the author of the pull request, and walk through the code with them, side by side, in order to clarify any misunderstandings efficiently and effectively. How to: Create an issue \u00b6 While the PO group are the ones responsible for creating user stories, there might be times where other groups feel the need to create issues on GitHub. With the Weekplanner repository as the example, people can either create a Task Creation Request or a Bug Report . To do this you go to the Weekplanner repository on GitHub and select the Issues tab. On the right side of the page, right above the list of issues you should see a green button labeled New issue . Clicking on this button takes you to a page where you can select to create either a Bug Report or a Task Creation Request . If you are in doubt as to which one to choose the the following can be of help: If you want to report unexpected behavior in already existing functionality of the application, you should choose Bug Report . If you feel that the application needs functionality that is out of your current scope and that you do not feel is being represented in other user stories, then you should choose Task Creation Request . Once you choose one of the two options and click the respective button you should see a page with a form that asks you for a title. The title for the Task Creation Request should tell what functionality you would like added using the shown form \"As a developer I would like the docker config file to automatically update so that I don\u00b4t have to manually update the config file\". Instead of the task being for the developer, guardian or user is also frequently used. For the bug the title should be the problem at hand, not the solution. ex. \"A error message pops up whenever the app is open\". There is a field underneath the title in which you write a description of what you want to report on. Note that the field where you can write a description is already filled out with a template on how to structure your own description. Once you have created a good title and written a description of the issue you click on the green Submit new issue button at the bottom of the form. With this you have created your issue. It can be a good idea to inform the PO group, so they can assign and refine the issue. You have now created your issue, congratulations!","title":"How to's for developers"},{"location":"Legacy/Handover/2019/how_to/#how-tos-for-developers","text":"This file is intended for helping developers understand their role in the process.","title":"How to's for developers"},{"location":"Legacy/Handover/2019/how_to/#how-to-get-an-issue-to-work-on","text":"If you have enough time to work on another issue after finishing your previous ones you should get another. Try to aim for no more than two or three issues, as you shouldn't take on another issue before you can actually work on it. There's no need for you to be assigned to an issue that you can't work on right now, as it might block others progress. When you have decided that you have enough time for a new issue, you should go and get one. This process is very simple. You have two options:","title":"How to: Get an issue to work on"},{"location":"Legacy/Handover/2019/how_to/#option-number-1","text":"You go to the issues tab of one of the repositories, e.g. github.com/aau-giraf/weekplanner/issues and locate an interesting issue. Then, when you have located the issue, you note the number. That is denoted by the # and a number. Remember the number and do a time estimation of the issue. Thereafter go to the product owner (PO) group, and ask if you can work on this issue. The PO group might say no for various reasons. Just remember: they have the overview of the project and probably have good reasons for saying as they do. There is usually a greater chance of getting a yes if the issue you've picked is either highest or high priority.","title":"Option number 1"},{"location":"Legacy/Handover/2019/how_to/#option-number-2","text":"Go ask the PO group to be assigned the most pressing issue, as they have a good overview of the project, they will most likely have some issues that they would love to have you work on.","title":"Option number 2"},{"location":"Legacy/Handover/2019/how_to/#how-to-write-code","text":"To get started writing code, you should first setup the project. This is described in the Setup from scratch tutorial . Then, after setting up the code, you should make a branch for the feature you are going to implement. Name the branch fixes_ISSUENUMBER, where the ISSUENUMBER is the number of the issue you are implementing. Follow the default coding standards that you can find. For Flutter the linter should catch most things that go out of style. If in doubt, try to find other places in the code that does similar things to what you want, and follow that standard. Remember that all functionality related to an issue should be implemented with unit tests before an issue is considered done. In the How to review section you can read more about how that will happen. Make sure that you plan for the pull request to go through as fast as possible. And if you are in doubt and stuck, just go ask another group for help, as there are a lot of clever people on the GIRAF project, and they usually love to help.","title":"How to: Write code"},{"location":"Legacy/Handover/2019/how_to/#how-to-create-a-pull-request","text":"When development of a piece of software is done, and it is deemed that the code solves the assigned issue, a pull request should be made. Before making the pull request, make sure that the code: Only relates to a single issue (One PR per user story). Is fully tested. Is reachable when opening the application. Fully tested means that if any piece of the functionality is removed, a test should fail. To create the pull request, you should go to the relevant repository, click on pull requests and click the green \"New Pull Request\" button. The base branch should be \"develop\" and the compare branch should be your feature branch, with the code that you want to add to the develop branch. Then click the green \"Create pull request\" button. Name the pull request the same as your branch. To the right there is a tab called projects. Add the request to the current GIRAF project. For the first sprint you would add it to the project called \"2020 1. sprint\", for the second sprint \"2020 2. sprint\" and so on. Next, include pictures (or a gif/video) of all the screens that you added, in the text field, so someone from the product owner group can verify that the screens follow the design manual. Remember to include the text \"fixes #YourIssueID\". This will automatically close the issue when the pull request is merged. When the PR is closed, check your issue to make sure that it has been closed too. Now all that is left is for you to inform the process group, and ask them to assign reviewers. They will assign reviewers who will review your pull request.","title":"How to: Create a pull request"},{"location":"Legacy/Handover/2019/how_to/#how-to-get-code-in-review","text":"In order to get code into review, you firstly have to make sure that the tests you wrote all pass, and that they cover the relevant parts of the written code. The way you make a pull request (PR) starts by visiting github.com. You navigate to the repository in which you've done your work. Go to the pull requests tab, click the \"new pull request\" button. Then you select the correct base and compare branch, and then your create a pull request. Otherwise, if you have recently pushed commits github will actually suggest if you want to create a pull requests, this eases the process. When a pull request have been created, you have to notify the process group, write them or talk to them. The process group is responsible for delegating the two reviews which are needed to merge the pull request. The review will be performed by others than the group who wrote the code, thus we use external review. Once the pull request is in the review phase, you should be able to in the meantime work on other user stories, or simply write about the user story in your report. When other groups review you PR, they most likely have some form of feedback, comments, or some things they do not understand. Thus it is to prefer if both the reviewers and the ones responsible for the pull request are quickly to respond to comments on github. Once the reviewers have approved the pull request is able to merged.","title":"How To: Get Code in Review"},{"location":"Legacy/Handover/2019/how_to/#how-to-review-a-pull-request","text":"When reviewing a pull request it is absolutely important that you consider the goal of the review: Value should be added to the project, and the developers involved should gain knowledge and improve. As such, it is important to keep a light and educational tone when involved with the pull requests. When reviewing a pull request, you should check for points in the Code Review Checklist . In case a point is not fulfilled you should leave a comment that points out the problem, why it is a problem, and, if possible, how to develop a solution. When reviewing the code, you can click on the \"Files Changed\" tab, in which you will be able to see all changes to the code. If you click on the left side of the code, right besides the line numbers, there will be a small, blue plus button available, which will let you leave a comment to this specific line of the code. If possible, you should always leave a comment on the line of the code that is problematic. As a reviewer you should never commit to the branch with the pull request, merge the pull request, or delete the pull request. Since the first two actions might introduce problems to the codebase, the author of the pull request should always do these things, as that person will be best suited to handle potential problems. If you are reviewing a complex piece of code, or a particularly problematic piece of code, it is highly advisable that you locate the author of the pull request, and walk through the code with them, side by side, in order to clarify any misunderstandings efficiently and effectively.","title":"How To: Review a Pull Request"},{"location":"Legacy/Handover/2019/how_to/#how-to-create-an-issue","text":"While the PO group are the ones responsible for creating user stories, there might be times where other groups feel the need to create issues on GitHub. With the Weekplanner repository as the example, people can either create a Task Creation Request or a Bug Report . To do this you go to the Weekplanner repository on GitHub and select the Issues tab. On the right side of the page, right above the list of issues you should see a green button labeled New issue . Clicking on this button takes you to a page where you can select to create either a Bug Report or a Task Creation Request . If you are in doubt as to which one to choose the the following can be of help: If you want to report unexpected behavior in already existing functionality of the application, you should choose Bug Report . If you feel that the application needs functionality that is out of your current scope and that you do not feel is being represented in other user stories, then you should choose Task Creation Request . Once you choose one of the two options and click the respective button you should see a page with a form that asks you for a title. The title for the Task Creation Request should tell what functionality you would like added using the shown form \"As a developer I would like the docker config file to automatically update so that I don\u00b4t have to manually update the config file\". Instead of the task being for the developer, guardian or user is also frequently used. For the bug the title should be the problem at hand, not the solution. ex. \"A error message pops up whenever the app is open\". There is a field underneath the title in which you write a description of what you want to report on. Note that the field where you can write a description is already filled out with a template on how to structure your own description. Once you have created a good title and written a description of the issue you click on the green Submit new issue button at the bottom of the form. With this you have created your issue. It can be a good idea to inform the PO group, so they can assign and refine the issue. You have now created your issue, congratulations!","title":"How to: Create an issue"},{"location":"Legacy/Handover/2019/process_group_information/","text":"Process group information \u00b6 This section mostly contains information that is relevant for the process group, and as such it is written to the process group. It might be used for development groups for the sake of documenting the process in the reports. General development \u00b6 This is a short section that outlines how a single sprint went. If you want more in-depth information, you should check the Project Meetings section . We started by using the Scrum of Scrums framework, where each group would send a representative to attend one or two standup meetings per week. We would also do both poker planning where each group did poker planning until they had filled out their points, and we would be doing review and retrospective. After talking to Carsten Ruseng, our lecturer in software engineering, as well as realizing how inefficient poker planning is when you start out, we would skip poker planning. We did this because we experienced that most estimates were off by quite a margin, and this meant that if groups took two high-priority tasks and were delayed on their first, the second would be blocked while other groups worked on different things. Instead, a sprint went something like this: At the very first day of the sprint a sprint intro is held. At this event the PO group will present the product vision and the sprint goal for all developers. The process group will present any changes that they have come up with during the previous retrospective. At the end of the meeting each group will be assigned their first issue to work with. This is done by the PO group who have ranked the issues as lowest, low, medium, high, or highest priority. Then ordinary development ensues. When a development group feels that they are done with a user story, a PR will be created, and the process group will be responsible for finding reviewers . During the week you should have some cross-group standups . In the beginning when you still follow lectures we recommend a single standup a week. It's also a good idea to organize the standup between the two days with the most time for your project. For example, if you have no lectures Monday and Tuesday, but full schemes the rest of the week, then place the standup somewhere at the end of Monday or start of Tuesday. When you no longer have any lectures we found 2 standups to be beneficial. At the end of the sprint the process group should host a release preparation , where you take everything that was developed during the sprint and test it thoroughly to polish the little details. The release preparation ends with a release party where each group is given the opportunity to showcase the work they did during the sprint. This aims to help other developers gain an overview of the project, as well as instill a feeling of teamwork. Fullstack teams \u00b6 In the previous years, groups worked with a horizontal slice of the architecture, i.e. some groups only worked on the database, some groups only worked on frontend, etc. This meant that features were developed like on an assembly line. Instead, we were suggested to create fullstack groups, where groups independently implement an issue from server, through backend, to frontend. We were recommended to drop the horizontal slice approach for several reasons: Groups will only know what the state of the app is for the particular area that they work with. Frontend won't know how the backend works, and the server group will most likely never compile the frontend. Groups will be more isolated and not have as many developers to ask for help. You have been working as a fullstack team in previous years anyway. It is harder to assign reviewers, as there are fewer people who understand the area in which you work. If you choose fullstack teams, all development of a single feature happens in a single group, instead of having to hand over a feature. How to handle roles \u00b6 You will be interacting with most of the other GIRAF students during the semester, and will be interacting with the students as they take on different roles. This section will try to describe and prepare you for handling the different roles that exist in the project. Development groups \u00b6 The development groups are simply the groups in which you write the report. Every single group of students are a development group - even the process group, and the PO group. The process group's role will mostly be to listen to these groups' concerns during the semester. They usually bring questions about the process, how to create pull requests , who to ask for issues and such. We recommend that for issue related questions you refer to the PO group, and let them handle that, to ensure that they can keep an overview of what is happening in the project. You should of course help by remembering and recommending good practices, e.g. creating one issue per bug/feature, creating one pull request per issue. If the questions are related to pull requests, collaboration, testing, or anything else, we recommend that you, if you haven't already, discuss the issue in your group and try to reach the agreement that benefits the project the most. Remember that even though it is always recommended to follow all best code-practices, this is sometimes impossible as you have a limited amount of time available. Try to get an overview of the kind of best practices we followed as we recommend that you follow them. For anything else, see the developers' time as a scarce resource. What will the project and next year's students benefit from the most? Following the practice, or simply developing new features / document the existing ones? Skill groups \u00b6 These groups have a representative from each development group, and have a single responsibility area, i.e. the frontend, backend, or server. They will do work that is not directly related to an issue, but is more related to general maintenance and management of the three areas. We recommend that major changes or decisions related to one of the areas are discussed in these groups, as they will be the most qualified at making these decisions. The development groups should also be able to consult their representative when they have specific questions to one of the areas. We did not have a lot to do with these groups, other than setting the time and place for the first meeting, and introducing the idea to them. From this point they figured out how they could gain an overview of the area, and continue work. They also organized their own meetings after the first meeting. Product owner group \u00b6 The product owner group, also called the PO group, is the group responsible for the issues in the project. According to regular Scrum, that we started with an intention to follow, product owner is a bit of a misleading name. But the responsible that they were assigned by us, was to handle the issues. When a feature suggestion was provided, the PO group would make it an issue, prioritize it, create and add prototypes to it, and assign it when development groups came and asked for new issues. They would also prune overlapping issues, and approve the design of all pull requests, to ensure that they are consistent with the prototypes and the design guide. We were assigned the same room as the PO group in 2019. This was very beneficial for the cooperation, as we could quickly take a short discussion with them about the different aspects of the project. In certain areas your work will be overlapping, and other students coming into your group rooms to discuss something, will not always know which one of you to address. If you are seated in the same room, you will be able to eavesdrop on the requests to the PO group, and vice versa. This will both give you an overview of the project, as well as provide the PO group with an overview of the process changes that you discuss. Embrace their feedback and input, as they will have the overview of the issues in the project and the progress of these. You should remind the PO group of the different meetings, and what they should do to prepare. For the sprint intro, the PO group should bring the product vision and the sprint vision. For the release preparation, please refer to the release preparation section as this requires some work from both of you. Your role and relevant information for you \u00b6 This section includes information directly related to your role as the process group. It is meant as a work of reference that you can consult when in doubt about the different tasks that you should perform during the semester. Communication \u00b6 We created a Slack channel that we used for communication in the project. All groups had their own channel that could be used for getting a hold of the group. Each skill group also had their own channel. Other than that, he had the following channels: Announcements Only used for very important information that all should receive. General Mostly used for relevant discussions and talks about the project, or for asking everyone for help. github_watch GitHub bot so you can follow all the sweet progress. Flutter General resources and questions about Flutter. Random To keep everything meme-related away from the other channels We very strongly recommend that you add the setting that displays people's full name instead of their obscure username . Likewise, it is beneficial if people put their group in the What I do field, so that it is easy for everyone to identify where the person they're talking to belongs. We also recommended that you ask people to add their real name to GitHub, so you can search for that when assigning them, instead of having to deal with their GitHub usernames. They change that by going to https://github.com/settings/profile and changing their name to their real name. General about meetings \u00b6 You will be responsible for planning most meetings. We created a GIRAF Google Calendar that we shared with people, where we asked them to add all meetings all groups are welcome - that included skill group meetings and the general process meetings. When people add this calendar to theirs, it wasn't necessary for us to promote the meeting on Slack, saving a lot of messages. If you choose to do the same, remember to add location to the meetings. We generally planned a retrospective on Monday morning and sprint intro on Tuesday morning, so you have time to review any changes to the process. This also gives the PO group time to organize their issues, and the development groups a bit of short time to catch up on their reports. Release preparation would usually be two full days. If the sprint ends on a Friday, we would hold a release preparation from Thursday morning until Friday at 13 o'clock. From 13 to 14 we would hold the release party. We had the release party early in the day, so as many as possible would attend it. Our adaptation of GitFlow \u00b6 We used the GitFlow Workflow as the branching strategy in the GIRAF project. We also enforce squash merges from feature branches into the develop branch, so each feature will be in a single commit. This is our adaptation, but please read the entire section, as there might be better ways to do it. For the release preparation , the process group creates a release branch, which is merged out from the master branch. Then commits are chosen from the develop branch by the PO-group. Only commits that are actually relevant for the upcoming release are added. This is called cherry picking commits. In the release preparation, developers will add bug-fixes to the release branch. When the release preparation ends, the release branch is merged into the master branch, which is then merged into the develop branch. This ensures that bug-fixes are also added to the develop branch. One problem with this approach is the manual part with the creation of the feature branch. If you forget a commit, or miss it, you risk having critical functionality that was never added to the master branch, but will be present on the develop branch. You will never get a complete and automatic addition of the changes from the develop branch, and as such you cannot guarantee that everything was added to the master branch. To accommodate this problem, one solution would be to make even stricter rules for the develop branch. As it is currently, the develop branch allows sub-features to be pushed, which are parts of bigger features. As such you might have a \"login\" button, but not the \"create user\" button. This renders the login button useless, but it can still be included in the develop branch right now. An example of how to properly handle this, could be to create a feature branch where all login related functionality could be added. Different issues could be merged into this feature branch. When everything login related has been fixed and merged to the feature branch, this feature branch can be merged into the develop branch. Assigning reviewers \u00b6 Assigning reviewers is done in an excel document. This sheet will keep track of the number of PRs a group reviewed and the size of these. We reset it once in a while, when the numbers become too big. It helps to gain an overview of which group to assign for review. Every PR needs two groups, who are not the author, as reviewers, as well as an approving review from a PO group member if it contains anything GUI related. When groups have been chosen, you should assign a representative from the group as a reviewer on the PR. We provide a checklist to the reviewers, depending on if the PR is made to a code repository or the wiki repository . This is provided as a comment on the PR on GitHub. Gatekeepers of good code practice \u00b6 You, the process group, will be mostly responsible for maintaining a good code practice. Some developers will not be informed about the best practices, others will not care, and others yet again will fail to see the reasons for good code practice. As such you will be responsible for sustaining a good code practice. You should actively seek out more information, and ensure that you thoroughly research pros and cons of both existing practices and new practices. Definition of done \u00b6 For an issue, we have defined the following definition of done: An issue and its associated tasks are considered done when the feature branch has been successfully merged into the development branch by a single pull request and the feature documentation has been updated in the GIRAF wiki. This means that all functionality should have been designed and implemented, so you have fully working functionality merged. Essentially things should be included in the develop branch when you would publish it to the user (after a bug test). You should check that people practice this mentality. You will be able to spot this if you keep an eye on the incoming pull requests. Some groups will add several pieces of functionality in a single PR, and others will add incomplete functionality, stating that \"it wasn't specified in the issue that we should do it.\" For the second case, tell them that it is a shortcoming of the issue, and when they discovered this they should have taken immediate contact to the PO group and asked for it to be included in the issue. Functionality isn't worth much, if it isn't finished, and as such shouldn't be included in the develop branch. Issue reporting \u00b6 Once in a while, someone will approach you and tell you about something that the program should have, or something that doesn't work. You should be responsible for one of the two things: Creating an issue describing it. Making sure the developer creates an issue describing it. If you don't do it, you probably won't remember it. Reviews \u00b6 It will be so easy to cut corners with this. A lot of your time will be spent doing reviews, and you will probably be tempted to cut down on the time spent on reviews. Know for sure that other students will, at least. We really hope that you encourage people to spend the time on the reviews. It will save you so much time in the long run, as you will catch a lot of errors and bad practices before they even emerge. The students will also get a lot of insight into other people's way of writing code, as well as the insight of learning how other parts of the codebase were developed. The time spent is well invested and will benefit you and the coming years in the long run, as the code will be much more aligned, and developers will increase their knowledge a lot.","title":"Process group information"},{"location":"Legacy/Handover/2019/process_group_information/#process-group-information","text":"This section mostly contains information that is relevant for the process group, and as such it is written to the process group. It might be used for development groups for the sake of documenting the process in the reports.","title":"Process group information"},{"location":"Legacy/Handover/2019/process_group_information/#general-development","text":"This is a short section that outlines how a single sprint went. If you want more in-depth information, you should check the Project Meetings section . We started by using the Scrum of Scrums framework, where each group would send a representative to attend one or two standup meetings per week. We would also do both poker planning where each group did poker planning until they had filled out their points, and we would be doing review and retrospective. After talking to Carsten Ruseng, our lecturer in software engineering, as well as realizing how inefficient poker planning is when you start out, we would skip poker planning. We did this because we experienced that most estimates were off by quite a margin, and this meant that if groups took two high-priority tasks and were delayed on their first, the second would be blocked while other groups worked on different things. Instead, a sprint went something like this: At the very first day of the sprint a sprint intro is held. At this event the PO group will present the product vision and the sprint goal for all developers. The process group will present any changes that they have come up with during the previous retrospective. At the end of the meeting each group will be assigned their first issue to work with. This is done by the PO group who have ranked the issues as lowest, low, medium, high, or highest priority. Then ordinary development ensues. When a development group feels that they are done with a user story, a PR will be created, and the process group will be responsible for finding reviewers . During the week you should have some cross-group standups . In the beginning when you still follow lectures we recommend a single standup a week. It's also a good idea to organize the standup between the two days with the most time for your project. For example, if you have no lectures Monday and Tuesday, but full schemes the rest of the week, then place the standup somewhere at the end of Monday or start of Tuesday. When you no longer have any lectures we found 2 standups to be beneficial. At the end of the sprint the process group should host a release preparation , where you take everything that was developed during the sprint and test it thoroughly to polish the little details. The release preparation ends with a release party where each group is given the opportunity to showcase the work they did during the sprint. This aims to help other developers gain an overview of the project, as well as instill a feeling of teamwork.","title":"General development"},{"location":"Legacy/Handover/2019/process_group_information/#fullstack-teams","text":"In the previous years, groups worked with a horizontal slice of the architecture, i.e. some groups only worked on the database, some groups only worked on frontend, etc. This meant that features were developed like on an assembly line. Instead, we were suggested to create fullstack groups, where groups independently implement an issue from server, through backend, to frontend. We were recommended to drop the horizontal slice approach for several reasons: Groups will only know what the state of the app is for the particular area that they work with. Frontend won't know how the backend works, and the server group will most likely never compile the frontend. Groups will be more isolated and not have as many developers to ask for help. You have been working as a fullstack team in previous years anyway. It is harder to assign reviewers, as there are fewer people who understand the area in which you work. If you choose fullstack teams, all development of a single feature happens in a single group, instead of having to hand over a feature.","title":"Fullstack teams"},{"location":"Legacy/Handover/2019/process_group_information/#how-to-handle-roles","text":"You will be interacting with most of the other GIRAF students during the semester, and will be interacting with the students as they take on different roles. This section will try to describe and prepare you for handling the different roles that exist in the project.","title":"How to handle roles"},{"location":"Legacy/Handover/2019/process_group_information/#development-groups","text":"The development groups are simply the groups in which you write the report. Every single group of students are a development group - even the process group, and the PO group. The process group's role will mostly be to listen to these groups' concerns during the semester. They usually bring questions about the process, how to create pull requests , who to ask for issues and such. We recommend that for issue related questions you refer to the PO group, and let them handle that, to ensure that they can keep an overview of what is happening in the project. You should of course help by remembering and recommending good practices, e.g. creating one issue per bug/feature, creating one pull request per issue. If the questions are related to pull requests, collaboration, testing, or anything else, we recommend that you, if you haven't already, discuss the issue in your group and try to reach the agreement that benefits the project the most. Remember that even though it is always recommended to follow all best code-practices, this is sometimes impossible as you have a limited amount of time available. Try to get an overview of the kind of best practices we followed as we recommend that you follow them. For anything else, see the developers' time as a scarce resource. What will the project and next year's students benefit from the most? Following the practice, or simply developing new features / document the existing ones?","title":"Development groups"},{"location":"Legacy/Handover/2019/process_group_information/#skill-groups","text":"These groups have a representative from each development group, and have a single responsibility area, i.e. the frontend, backend, or server. They will do work that is not directly related to an issue, but is more related to general maintenance and management of the three areas. We recommend that major changes or decisions related to one of the areas are discussed in these groups, as they will be the most qualified at making these decisions. The development groups should also be able to consult their representative when they have specific questions to one of the areas. We did not have a lot to do with these groups, other than setting the time and place for the first meeting, and introducing the idea to them. From this point they figured out how they could gain an overview of the area, and continue work. They also organized their own meetings after the first meeting.","title":"Skill groups"},{"location":"Legacy/Handover/2019/process_group_information/#product-owner-group","text":"The product owner group, also called the PO group, is the group responsible for the issues in the project. According to regular Scrum, that we started with an intention to follow, product owner is a bit of a misleading name. But the responsible that they were assigned by us, was to handle the issues. When a feature suggestion was provided, the PO group would make it an issue, prioritize it, create and add prototypes to it, and assign it when development groups came and asked for new issues. They would also prune overlapping issues, and approve the design of all pull requests, to ensure that they are consistent with the prototypes and the design guide. We were assigned the same room as the PO group in 2019. This was very beneficial for the cooperation, as we could quickly take a short discussion with them about the different aspects of the project. In certain areas your work will be overlapping, and other students coming into your group rooms to discuss something, will not always know which one of you to address. If you are seated in the same room, you will be able to eavesdrop on the requests to the PO group, and vice versa. This will both give you an overview of the project, as well as provide the PO group with an overview of the process changes that you discuss. Embrace their feedback and input, as they will have the overview of the issues in the project and the progress of these. You should remind the PO group of the different meetings, and what they should do to prepare. For the sprint intro, the PO group should bring the product vision and the sprint vision. For the release preparation, please refer to the release preparation section as this requires some work from both of you.","title":"Product owner group"},{"location":"Legacy/Handover/2019/process_group_information/#your-role-and-relevant-information-for-you","text":"This section includes information directly related to your role as the process group. It is meant as a work of reference that you can consult when in doubt about the different tasks that you should perform during the semester.","title":"Your role and relevant information for you"},{"location":"Legacy/Handover/2019/process_group_information/#communication","text":"We created a Slack channel that we used for communication in the project. All groups had their own channel that could be used for getting a hold of the group. Each skill group also had their own channel. Other than that, he had the following channels: Announcements Only used for very important information that all should receive. General Mostly used for relevant discussions and talks about the project, or for asking everyone for help. github_watch GitHub bot so you can follow all the sweet progress. Flutter General resources and questions about Flutter. Random To keep everything meme-related away from the other channels We very strongly recommend that you add the setting that displays people's full name instead of their obscure username . Likewise, it is beneficial if people put their group in the What I do field, so that it is easy for everyone to identify where the person they're talking to belongs. We also recommended that you ask people to add their real name to GitHub, so you can search for that when assigning them, instead of having to deal with their GitHub usernames. They change that by going to https://github.com/settings/profile and changing their name to their real name.","title":"Communication"},{"location":"Legacy/Handover/2019/process_group_information/#general-about-meetings","text":"You will be responsible for planning most meetings. We created a GIRAF Google Calendar that we shared with people, where we asked them to add all meetings all groups are welcome - that included skill group meetings and the general process meetings. When people add this calendar to theirs, it wasn't necessary for us to promote the meeting on Slack, saving a lot of messages. If you choose to do the same, remember to add location to the meetings. We generally planned a retrospective on Monday morning and sprint intro on Tuesday morning, so you have time to review any changes to the process. This also gives the PO group time to organize their issues, and the development groups a bit of short time to catch up on their reports. Release preparation would usually be two full days. If the sprint ends on a Friday, we would hold a release preparation from Thursday morning until Friday at 13 o'clock. From 13 to 14 we would hold the release party. We had the release party early in the day, so as many as possible would attend it.","title":"General about meetings"},{"location":"Legacy/Handover/2019/process_group_information/#our-adaptation-of-gitflow","text":"We used the GitFlow Workflow as the branching strategy in the GIRAF project. We also enforce squash merges from feature branches into the develop branch, so each feature will be in a single commit. This is our adaptation, but please read the entire section, as there might be better ways to do it. For the release preparation , the process group creates a release branch, which is merged out from the master branch. Then commits are chosen from the develop branch by the PO-group. Only commits that are actually relevant for the upcoming release are added. This is called cherry picking commits. In the release preparation, developers will add bug-fixes to the release branch. When the release preparation ends, the release branch is merged into the master branch, which is then merged into the develop branch. This ensures that bug-fixes are also added to the develop branch. One problem with this approach is the manual part with the creation of the feature branch. If you forget a commit, or miss it, you risk having critical functionality that was never added to the master branch, but will be present on the develop branch. You will never get a complete and automatic addition of the changes from the develop branch, and as such you cannot guarantee that everything was added to the master branch. To accommodate this problem, one solution would be to make even stricter rules for the develop branch. As it is currently, the develop branch allows sub-features to be pushed, which are parts of bigger features. As such you might have a \"login\" button, but not the \"create user\" button. This renders the login button useless, but it can still be included in the develop branch right now. An example of how to properly handle this, could be to create a feature branch where all login related functionality could be added. Different issues could be merged into this feature branch. When everything login related has been fixed and merged to the feature branch, this feature branch can be merged into the develop branch.","title":"Our adaptation of GitFlow"},{"location":"Legacy/Handover/2019/process_group_information/#assigning-reviewers","text":"Assigning reviewers is done in an excel document. This sheet will keep track of the number of PRs a group reviewed and the size of these. We reset it once in a while, when the numbers become too big. It helps to gain an overview of which group to assign for review. Every PR needs two groups, who are not the author, as reviewers, as well as an approving review from a PO group member if it contains anything GUI related. When groups have been chosen, you should assign a representative from the group as a reviewer on the PR. We provide a checklist to the reviewers, depending on if the PR is made to a code repository or the wiki repository . This is provided as a comment on the PR on GitHub.","title":"Assigning reviewers"},{"location":"Legacy/Handover/2019/process_group_information/#gatekeepers-of-good-code-practice","text":"You, the process group, will be mostly responsible for maintaining a good code practice. Some developers will not be informed about the best practices, others will not care, and others yet again will fail to see the reasons for good code practice. As such you will be responsible for sustaining a good code practice. You should actively seek out more information, and ensure that you thoroughly research pros and cons of both existing practices and new practices.","title":"Gatekeepers of good code practice"},{"location":"Legacy/Handover/2019/process_group_information/#definition-of-done","text":"For an issue, we have defined the following definition of done: An issue and its associated tasks are considered done when the feature branch has been successfully merged into the development branch by a single pull request and the feature documentation has been updated in the GIRAF wiki. This means that all functionality should have been designed and implemented, so you have fully working functionality merged. Essentially things should be included in the develop branch when you would publish it to the user (after a bug test). You should check that people practice this mentality. You will be able to spot this if you keep an eye on the incoming pull requests. Some groups will add several pieces of functionality in a single PR, and others will add incomplete functionality, stating that \"it wasn't specified in the issue that we should do it.\" For the second case, tell them that it is a shortcoming of the issue, and when they discovered this they should have taken immediate contact to the PO group and asked for it to be included in the issue. Functionality isn't worth much, if it isn't finished, and as such shouldn't be included in the develop branch.","title":"Definition of done"},{"location":"Legacy/Handover/2019/process_group_information/#issue-reporting","text":"Once in a while, someone will approach you and tell you about something that the program should have, or something that doesn't work. You should be responsible for one of the two things: Creating an issue describing it. Making sure the developer creates an issue describing it. If you don't do it, you probably won't remember it.","title":"Issue reporting"},{"location":"Legacy/Handover/2019/process_group_information/#reviews","text":"It will be so easy to cut corners with this. A lot of your time will be spent doing reviews, and you will probably be tempted to cut down on the time spent on reviews. Know for sure that other students will, at least. We really hope that you encourage people to spend the time on the reviews. It will save you so much time in the long run, as you will catch a lot of errors and bad practices before they even emerge. The students will also get a lot of insight into other people's way of writing code, as well as the insight of learning how other parts of the codebase were developed. The time spent is well invested and will benefit you and the coming years in the long run, as the code will be much more aligned, and developers will increase their knowledge a lot.","title":"Reviews"},{"location":"Legacy/Handover/2019/project_meetings/","text":"Project meetings \u00b6 This section will go through all the different meetings, and describe the agenda and purpose of each meeting. Generally you should try to waste as little time as possible with the meetings. Having everyone attend all meetings is a bad strategy, so only invite the necessary students. Sprint intro \u00b6 This meeting is used to introduce and mark the beginning of the sprint. All developers should be present in the same room, and the PO group will start out by presenting the following things: The project vision What do we want to accomplish in the entire project? Did this change since last meeting? (if not, just give a recap, so people remember) The sprint vision What do we want to accomplish until the next release? New and important issues in this sprint Typically based on usability tests with the customer. This is followed by a presentation from the process group, where changes to the process, based on the sprint retrospective , are presented. The very first sprint intro will probably be the longest, as the process group should present the major parts of the process. All following sprint intros will probably be very short meetings, lasting no more than 15 - 20 minutes. Cross-group standups \u00b6 This meeting should be held once or twice a week. When you still have lectures, once a week should be sufficient, and when you don't twice a week will be better. The purpose of the cross-group standups is to coordinate the events of the project, and to minimize the amount of merge conflicts caused by having multiple groups work in the same files. The agenda for a cross-group standup is as follows: You book a room with the secretaries, and announce the meeting in the calendar and on Slack. At the meeting, which should only take a maximum of 15 minutes, everyone should be standing around a table. Make sure that you have found the issues for each group. You can typically search for assignees and simply use the representatives username from each group. They should answer the following questions: For each user story What issue number is it? What is it about? How do they solve it? When do they create a PR for it? Then you should remind everyone that: They should contact the author of a PR when they have reviewed it. They should contact the reviewers when the author responded to all comments. They should ask the PO group if they have any questions about issues. Release preparation \u00b6 The last two work-days before a sprint ends should be spent actively hunting for, and fixing bugs. We strongly recommend that all groups are present in the same rooms. We recommend that each issue is handed out to two groups, none of them being the author group. Each group will then be responsible for testing the issue according to the following checklist: A new issue should be created on GitHub if 'no' is the answer to any of the following questions: Can the screen be reached through navigation in the application? Can you perform all the functionality defined in the issue? Can it be used without crashing? Does it run without bugs? Does all of the above still look acceptable if you change to a new device or change orientation? If a bug is discovered, an issue should be created in the relevant repository, describing the bug. The label called ReleaseFix should be added to the issue. The PO group will assign the bug-fixing issues. When groups solve an issue, they should write a test to verify that that the bug was solved, i.e. the test should fail before solving the bug, and pass after. Most of the time, there will only be a few bugs left, and most developers will have some extra time with nothing to do. A way to keep them engaged could be to organize writing tests. Just asking people to \"test the code\" will yield approximately 0 new tests. As such, it is necessary to ask people to write specific tests. This should be defined by the process group, and could be along the lines of: \"test file X according to issues Y.\" Also encourage people to help each other out, as someone will be more experienced with testing than others. That's why you should be in the same room, so helping each other is easier. We suggest that some hours before the release party, beer is brought in from F-klubben, with someone organizing the purchase of all beers that have been taken. Ask your local F-klub representative if it is okay. It should be, and it helps the morale. Release party \u00b6 At the very end of the release preparation you should host a release party. The party should be planned to start at 1 or 2 o'clock, so people actually want to participate. This release party will start by merging everything in the release branch into the master branch. This should be followed by ecstatic cheering and clapping, because you are awesome. Then, while enjoying a sip of your beer, every group should, in turns, come to the front and present the work that have been done in the sprint. Encourage people to show what they have done. Do this by connecting a tablet to the projector. This will give an overview of the state of the product, and will help people's feeling of involvement in the project. After this, we had no more activities planned. We encouraged people to stay and drink a beer, but most would go home, possibly from a lack of things to do. It could be a good idea to encourage people to play CurveFever Pro, Scribble, or anything like it. Sprint Retrospective \u00b6 The sprint retrospective is the process group's opportunity to hear what problems exist with the process. This meeting should be held the next workday after the release party. Initially we used Dotstorming to find issues and suggestions, and then vote on them. But the problem was that Dotstorming only allows for users to have 3 votes, so if there were 5 ideas that everyone agreed 100% on, it would only show on 3 of the ideas, while 2 of them wouldn't get any votes. Equally, Dotstorming does not allow for negative votes. So, controversial topics will only have the positive votes shown. Instead we came up with the following solution: First, we split into groups. The number of groups was determined by the number of members from the process group that were present. Each group would have their own tab in an Excel sheet. ( See example ) Members of the process group would be responsible for reporting the issues that developers report. It does not matter if you write down problems or solutions. Having a solution is nice, but bringing up a problem is just fine as well. Remember that the issues isn't an expression of the process group member's feelings or opinions about feedback, so try to report the feedback as accurate as possible. Oftentimes the discussions in the groups will also be hard to get going. The process group member should ask about the different parts of the sprint, like \"what did you think about the sprint intro?\" and \"what about the release preparation?\" Also feel free to, as a process group member, bring something up that you felt didn't work out. The rest might not have experienced it as a problem, they might have a solution, and if not, it might help as an icebreaker for the conversation. When everyone have had 15-20 minutes in the groups to discuss different problems and/or solutions, the process group members should get together. Here, they should remove duplicates. In the example sheet everything have been copied into the \"samlet\" tab. Here we marked the duplicate entries that we choose to remove with yellow marking. Then we created a Google Forms where every suggestion or idea were directly pasted in. The following, generic, options were given: \"It's a good idea / I agree\" \"I don't care\" \"It's not a good idea / I disagree\" This meant that every developer had to give their opinion on every problem or solution, and it revealed some controversial topics. We, the process group, spent the rest of the day discussing changes to the process, and presented it the next day to the sprint intro.","title":"Project meetings"},{"location":"Legacy/Handover/2019/project_meetings/#project-meetings","text":"This section will go through all the different meetings, and describe the agenda and purpose of each meeting. Generally you should try to waste as little time as possible with the meetings. Having everyone attend all meetings is a bad strategy, so only invite the necessary students.","title":"Project meetings"},{"location":"Legacy/Handover/2019/project_meetings/#sprint-intro","text":"This meeting is used to introduce and mark the beginning of the sprint. All developers should be present in the same room, and the PO group will start out by presenting the following things: The project vision What do we want to accomplish in the entire project? Did this change since last meeting? (if not, just give a recap, so people remember) The sprint vision What do we want to accomplish until the next release? New and important issues in this sprint Typically based on usability tests with the customer. This is followed by a presentation from the process group, where changes to the process, based on the sprint retrospective , are presented. The very first sprint intro will probably be the longest, as the process group should present the major parts of the process. All following sprint intros will probably be very short meetings, lasting no more than 15 - 20 minutes.","title":"Sprint intro"},{"location":"Legacy/Handover/2019/project_meetings/#cross-group-standups","text":"This meeting should be held once or twice a week. When you still have lectures, once a week should be sufficient, and when you don't twice a week will be better. The purpose of the cross-group standups is to coordinate the events of the project, and to minimize the amount of merge conflicts caused by having multiple groups work in the same files. The agenda for a cross-group standup is as follows: You book a room with the secretaries, and announce the meeting in the calendar and on Slack. At the meeting, which should only take a maximum of 15 minutes, everyone should be standing around a table. Make sure that you have found the issues for each group. You can typically search for assignees and simply use the representatives username from each group. They should answer the following questions: For each user story What issue number is it? What is it about? How do they solve it? When do they create a PR for it? Then you should remind everyone that: They should contact the author of a PR when they have reviewed it. They should contact the reviewers when the author responded to all comments. They should ask the PO group if they have any questions about issues.","title":"Cross-group standups"},{"location":"Legacy/Handover/2019/project_meetings/#release-preparation","text":"The last two work-days before a sprint ends should be spent actively hunting for, and fixing bugs. We strongly recommend that all groups are present in the same rooms. We recommend that each issue is handed out to two groups, none of them being the author group. Each group will then be responsible for testing the issue according to the following checklist: A new issue should be created on GitHub if 'no' is the answer to any of the following questions: Can the screen be reached through navigation in the application? Can you perform all the functionality defined in the issue? Can it be used without crashing? Does it run without bugs? Does all of the above still look acceptable if you change to a new device or change orientation? If a bug is discovered, an issue should be created in the relevant repository, describing the bug. The label called ReleaseFix should be added to the issue. The PO group will assign the bug-fixing issues. When groups solve an issue, they should write a test to verify that that the bug was solved, i.e. the test should fail before solving the bug, and pass after. Most of the time, there will only be a few bugs left, and most developers will have some extra time with nothing to do. A way to keep them engaged could be to organize writing tests. Just asking people to \"test the code\" will yield approximately 0 new tests. As such, it is necessary to ask people to write specific tests. This should be defined by the process group, and could be along the lines of: \"test file X according to issues Y.\" Also encourage people to help each other out, as someone will be more experienced with testing than others. That's why you should be in the same room, so helping each other is easier. We suggest that some hours before the release party, beer is brought in from F-klubben, with someone organizing the purchase of all beers that have been taken. Ask your local F-klub representative if it is okay. It should be, and it helps the morale.","title":"Release preparation"},{"location":"Legacy/Handover/2019/project_meetings/#release-party","text":"At the very end of the release preparation you should host a release party. The party should be planned to start at 1 or 2 o'clock, so people actually want to participate. This release party will start by merging everything in the release branch into the master branch. This should be followed by ecstatic cheering and clapping, because you are awesome. Then, while enjoying a sip of your beer, every group should, in turns, come to the front and present the work that have been done in the sprint. Encourage people to show what they have done. Do this by connecting a tablet to the projector. This will give an overview of the state of the product, and will help people's feeling of involvement in the project. After this, we had no more activities planned. We encouraged people to stay and drink a beer, but most would go home, possibly from a lack of things to do. It could be a good idea to encourage people to play CurveFever Pro, Scribble, or anything like it.","title":"Release party"},{"location":"Legacy/Handover/2019/project_meetings/#sprint-retrospective","text":"The sprint retrospective is the process group's opportunity to hear what problems exist with the process. This meeting should be held the next workday after the release party. Initially we used Dotstorming to find issues and suggestions, and then vote on them. But the problem was that Dotstorming only allows for users to have 3 votes, so if there were 5 ideas that everyone agreed 100% on, it would only show on 3 of the ideas, while 2 of them wouldn't get any votes. Equally, Dotstorming does not allow for negative votes. So, controversial topics will only have the positive votes shown. Instead we came up with the following solution: First, we split into groups. The number of groups was determined by the number of members from the process group that were present. Each group would have their own tab in an Excel sheet. ( See example ) Members of the process group would be responsible for reporting the issues that developers report. It does not matter if you write down problems or solutions. Having a solution is nice, but bringing up a problem is just fine as well. Remember that the issues isn't an expression of the process group member's feelings or opinions about feedback, so try to report the feedback as accurate as possible. Oftentimes the discussions in the groups will also be hard to get going. The process group member should ask about the different parts of the sprint, like \"what did you think about the sprint intro?\" and \"what about the release preparation?\" Also feel free to, as a process group member, bring something up that you felt didn't work out. The rest might not have experienced it as a problem, they might have a solution, and if not, it might help as an icebreaker for the conversation. When everyone have had 15-20 minutes in the groups to discuss different problems and/or solutions, the process group members should get together. Here, they should remove duplicates. In the example sheet everything have been copied into the \"samlet\" tab. Here we marked the duplicate entries that we choose to remove with yellow marking. Then we created a Google Forms where every suggestion or idea were directly pasted in. The following, generic, options were given: \"It's a good idea / I agree\" \"I don't care\" \"It's not a good idea / I disagree\" This meant that every developer had to give their opinion on every problem or solution, and it revealed some controversial topics. We, the process group, spent the rest of the day discussing changes to the process, and presented it the next day to the sprint intro.","title":"Sprint Retrospective"},{"location":"Legacy/Handover/2019/from_2019/PO_advice/","text":"PO advice for next year's PO group \u00b6 This document serves as a list of advice for the next PO group based on the experience we gathered throughout the semester. The PO group's main responsibility is to communicate with the customers of the GIRAF project and document this interaction. It is important that the customers' reactions, opinions and wishes for the program are written down, as these are the foundation for all the decisions that you, as the PO group, will take. In the following sections we provide an in-depth description of how we advice customer contact to be handled, and how the information gathered should be used. Customer contact \u00b6 In the start of the semester Ulrik will most likely schedule some meetings with the customers which everyone on the GIRAF project should attend. These serve to get the developers up to speed with what exactly the GIRAF project is and how the program is supposed to help people diagnosed with autism. Before these meetings it is a good idea to contact the customers to try and arrange a time for a meeting just for you, the PO group, where you can ask more specific questions regarding functionality and design of the program. The other groups should not be present for these meetings as it is your responsibility to relay this information to the other groups. We arranged a meeting with Emil from Egebakken right after his initial presentation. A transcription of this interview can be seen in the PO report of 2019. It is a good idea to get their phone numbers if possible, as it has been evident from our experience that communication through mail is often lacking. After this meeting you should have a good idea of what the customers want. You can then start setting goals for what should be completed in the upcoming sprint, as well as for the whole semester. As soon as you have the start and end dates for you sprints solidified, you should contact as many customers as you can with dates for usability testing. We did this by sending a mail to every customer involved in the project. Remember to schedule usability testing after your releases so that you have a working program for the customers to test and evaluate. It is important that you ask the customers to confirm that they will participate in these tests. If they do not respond you should try and contact them to see if they simply forgot to reply. We did this by calling them on their phones or by calling their workplaces. It is very important to take notes and document all meetings with customers. You will receive a lot of questions from the different developer groups about how functionality should be made and how it should look. Therefore, it is always better to have written down exactly what the customers want so that you do not have to guess and then refactor later if you guessed wrong. For example, an issue we faced was that we thought it would be sufficient for a guardian to copy activities one day at a time, but when we showed prototypes to the guardians they were very adamant on having the functionality be able to copy to multiple days at once. Another reason to document as much as possible, is that the users' feedback is the basis of all the user stories you are going to create. Creating user stories \u00b6 As the PO group it is your responsibility to create user stories. User stories are created based on requests from the users. We structured user stories in the following way: As a ... I would like ..., so that ... When creating a user story you should consider the amount of work that is needed for it to be completed, and whether or not it should be split up into multiple smaller user stories. We created user stories in the appropriate repositories on our GitHub page under the issue tab. This made it easy to organize as user stories are uniquely numbered and can be put into milestones. When creating a pull request it is easy to tag the user story so that it will be automatically closed when the pull request has been merged by adding \"Fixes #xx\" to the comment, where xx is the ID of the issue in the repository. It also allowed us to assign them with tags such as \"feature\", and give them different priorities ranging from lowest to highest. A user story should, if needed, contain a prototype that ideally has been approved by a customer and a further description of the problem. We also had great success having one of the more experienced Flutter developers write a short technical comment explaining how they would structure the solution for the user story. This gave the developers who were not as experienced with Flutter a better starting point. Below is an image of a user story. As you can see on the figure, the user story has the number 88 and was created by the user Eduardsen. It has the tags \"priority: highest\" and \"type: feature\" and is under the milestone called sprint 2. In the right hand side you can see the profile picture of the developer currently assigned to implementing this user story. Another approach to creating user stories is when developer groups create feature requests. In most cases these need to be rewritten into user stories if the problem they are describing has not been formulated correctly or adequately. When creating user stories, take special care to ensure that your user stories are not written ambiguously and, if so, that you have the precise functionality that the users wanted explained within them. Prioritizing user stories and issues \u00b6 The figure below shows a guide for how to prioritize user stories: The figure is available in .svg format here and can be edited in tools like draw.io if updates are needed. Prototypes \u00b6 One of the important tasks of the PO group is to create and maintain prototypes. Prototypes should conform to the design guide which can be found on the Github Wiki-page. PO groups from previous years had been using PowerPoint to create prototypes, but we found this process to be slow and require a lot of repetitive work. Instead, we opted to use Adobe XD which reduced repetitive work and let us import the exact icons we used throughout the design guide, making the prototypes much more consistent. Doing meetings with the customers you should show them the prototypes you have made since the last time you spoke. It is a very good idea to get customer feedback so that you know you are headed in the right direction in relation to their expectations. It is also helpful if developers have a creative way to interpret a user story, as it allows you to show them how it should be with certainty. The prototypes can be found in both pdf and Adobe XD format here . Remember to update the files whenever you change the prototypes, so that the development team are up to date. Distribution of user stories \u00b6 In the beginning of a new sprint you should present the goals of the sprint to all the other groups. Here you should also present the user stories you believe correspond to these goals. Afterwards, groups are free to take one or two user stories that they can start working on from the ones you presented. These are the user stories will help you reach the functionality defined in the sprint goal, and should be prioritized highly. If a development group finishes their user stories before the sprint is over, they should be able to contact the PO group by writing or visiting the PO group to ask for permission to start a new user story. This should be done so the PO group can constantly keep track of the user stories currently being implemented. We decided to continually update a board in our group room with the status of all development groups and their user stories. For the final sprint we chose to decide how to distribute user stories. We did this because we had a lot of documentation tasks that needed to be done and decided to distribute these as well to increase the likelihood of them being finished for next years groups. Consider doing the same for final/short sprints to maximize value for the customer. As you know which groups implemented which stories, and how productive the different groups are, you can use this to your advantage in distributing tasks. Communication with other groups \u00b6 During a sprint it is crucial to keep track of the status of other groups. Certain user stories can be blocked by other user stories and therefore you should regularly walk to the other group rooms and ask how their work is proceeding. Being the PO group you should have a good overview of the developer groups' skills. If a certain group is stuck with a user story you should ask other groups to help them complete it. This is a great way to share knowledge and speed up development in some cases. Knowing what people are working on also gives you the advantage of knowing what files they most likely are making changes in. This information can help you decide which user stories developer groups should take so that you reduce the amount of merge conflicts. Cooperation with process group \u00b6 A lot of the choices in regards to the process affects the work of the PO group. Because of this it is essential that you communicate with them and ensure that you are in agreement about what is being planned. Approval of designs \u00b6 We recommend enforcing that the PO group should be assigned to all pull requests that deal with the user interface of the application, as they are the group with the best understanding of how the customers interact with the system. This means that the design should be consistent with the prototypes that have been approved by the customer, and be very intuitive for them to use. It is a commonly known fact that software engineers are not designers and do not always think of user friendliness, so remember to keep them in check so the customers can actually figure out how to use the system. Some things we have noticed that you should pay special attention towards in design related PRs are: Good error messages should be shown to the user Icons are only used for one thing The design should be consistent with the design guide Release preparation \u00b6 Be prepared that at the end of every sprint you will not have much time to finish your assigned user stories, because the last days of every sprint will be spent on preparing for the release. At the first release preparation we tried to get as many user stories included in the release as possible. This meant that we were waiting for some groups to finish their user stories. We realized that this was not feasible and that we had to make a concrete deadline. This deadline had to be kept. Even though one of the user stories just needed 10 minutes extra before it would be completed it would have to wait until next release to be included. It was not feasible for us to keep delaying the release. When the deadline was over, the release branch was created. Then we would cooperate with the process group to assign groups to review the different user stories that had been implemented throughout the sprint and try to find bugs. When a development group found a bug, they would create a release fix issue and the group that originally implemented the user story that was being investigated would be assigned to fixing the issue. At the first release we assigned every group to some issues and let them review them by themselves. This worked decently, but was a bit hard to coordinate because all the communication was done through Slack. At the second release we chose, in collaboration with the process group, to host a hackathon for release fixing where all the groups sat together. This made it much easier to communicate between all the groups and we felt that the process was much smoother because of this. It is a good idea to document the releases as they are happening. This makes it much easier for you as the PO group to write about them in the report as well as in the wiki. Keep track of which issues were included in the release and which groups worked on these issues. Usability testing \u00b6 At the end of each sprint, a usability test should be conducted to get feedback on the features that have been developed in the sprint, and on potential new prototypes. We recommend that you arrange the usability tests shortly after the sprint has ended, so that you have time to translate the results into user stories. The customers usually prefer that the usability tests are held on weekdays at around 8am, so that they can do it before going to work. Remember to book a room for the usability test, and have someone from the group bring coffee and mugs to make it cozier for the customers - that makes them want to come back. For the usability tests, you should have a series of assignments prepared for the users, based on the latest release. Remember to use their time well, and pose follow-up questions to the assignments and things you are uncertain of to clarify potential problems. If it goes smoothly, you can show them additional features on the develop branch, even though they may be buggy. Finally, after having conducted the usability test, you can show them new prototypes to get feedback on them. Keeping GitHub issues updated \u00b6 GitHub is the primary place for keeping track of the system, all features are listed as issues that are spread across the repositories of GIRAF. Thus, it is very important that you keep the information on GitHub up to date. This includes regularly going through the issues and checking if the issues are still relevant, and updating the priorities of the issues. At the end of every sprint, it is important that you go through issues to re-order the priorities of user stories, as each sprint will most likely deplete the repositories of stories that are marked as highest and high priority. Likewise, it is important to keep an eye on the bug reports and feature requests that are reported by other groups and translating these into user stories. Sprint planning \u00b6 The PO group is responsible for planning what should be done in each sprint. At the beginning of each sprint, the PO group will do a short presentation with the goals of the upcoming sprint and present which user stories they have picked out as a focus for the sprint. We came to the conclusion that it is better to include fewer, but more essential user stories to the sprint and let groups come talk to us when they are done with that user story, so that we are sure that they will be able to finish implementation within the time period of the sprint. In the first sprints, we let the groups choose an assignment from the presented user stories themselves, whereas in the last sprint we decided the user stories for each group, based on what they had previously worked on and what essential features we needed to include to have a minimum viable product available. Try to spread the user stories across the system as much as possible, so that multiple groups will not be working on the same screens if this can be avoided. This decreases the amount of merge conflicts and frustration for the developers. Internal sharing of knowledge \u00b6 An often overlooked problem for us was to ensure that all members of the PO group shared what information was given to groups when they came in for advice on a user story. If knowledge is not shared properly within the group, it is very easy to cause confusion for not only the PO group, but also the group asking the question, as it may not always be the same person in the PO group they talk to. It may be a good idea to keep a shared document where you can write down your decisions and which groups it was discussed with.","title":"PO advice for next year's PO group"},{"location":"Legacy/Handover/2019/from_2019/PO_advice/#po-advice-for-next-years-po-group","text":"This document serves as a list of advice for the next PO group based on the experience we gathered throughout the semester. The PO group's main responsibility is to communicate with the customers of the GIRAF project and document this interaction. It is important that the customers' reactions, opinions and wishes for the program are written down, as these are the foundation for all the decisions that you, as the PO group, will take. In the following sections we provide an in-depth description of how we advice customer contact to be handled, and how the information gathered should be used.","title":"PO advice for next year's PO group"},{"location":"Legacy/Handover/2019/from_2019/PO_advice/#customer-contact","text":"In the start of the semester Ulrik will most likely schedule some meetings with the customers which everyone on the GIRAF project should attend. These serve to get the developers up to speed with what exactly the GIRAF project is and how the program is supposed to help people diagnosed with autism. Before these meetings it is a good idea to contact the customers to try and arrange a time for a meeting just for you, the PO group, where you can ask more specific questions regarding functionality and design of the program. The other groups should not be present for these meetings as it is your responsibility to relay this information to the other groups. We arranged a meeting with Emil from Egebakken right after his initial presentation. A transcription of this interview can be seen in the PO report of 2019. It is a good idea to get their phone numbers if possible, as it has been evident from our experience that communication through mail is often lacking. After this meeting you should have a good idea of what the customers want. You can then start setting goals for what should be completed in the upcoming sprint, as well as for the whole semester. As soon as you have the start and end dates for you sprints solidified, you should contact as many customers as you can with dates for usability testing. We did this by sending a mail to every customer involved in the project. Remember to schedule usability testing after your releases so that you have a working program for the customers to test and evaluate. It is important that you ask the customers to confirm that they will participate in these tests. If they do not respond you should try and contact them to see if they simply forgot to reply. We did this by calling them on their phones or by calling their workplaces. It is very important to take notes and document all meetings with customers. You will receive a lot of questions from the different developer groups about how functionality should be made and how it should look. Therefore, it is always better to have written down exactly what the customers want so that you do not have to guess and then refactor later if you guessed wrong. For example, an issue we faced was that we thought it would be sufficient for a guardian to copy activities one day at a time, but when we showed prototypes to the guardians they were very adamant on having the functionality be able to copy to multiple days at once. Another reason to document as much as possible, is that the users' feedback is the basis of all the user stories you are going to create.","title":"Customer contact"},{"location":"Legacy/Handover/2019/from_2019/PO_advice/#creating-user-stories","text":"As the PO group it is your responsibility to create user stories. User stories are created based on requests from the users. We structured user stories in the following way: As a ... I would like ..., so that ... When creating a user story you should consider the amount of work that is needed for it to be completed, and whether or not it should be split up into multiple smaller user stories. We created user stories in the appropriate repositories on our GitHub page under the issue tab. This made it easy to organize as user stories are uniquely numbered and can be put into milestones. When creating a pull request it is easy to tag the user story so that it will be automatically closed when the pull request has been merged by adding \"Fixes #xx\" to the comment, where xx is the ID of the issue in the repository. It also allowed us to assign them with tags such as \"feature\", and give them different priorities ranging from lowest to highest. A user story should, if needed, contain a prototype that ideally has been approved by a customer and a further description of the problem. We also had great success having one of the more experienced Flutter developers write a short technical comment explaining how they would structure the solution for the user story. This gave the developers who were not as experienced with Flutter a better starting point. Below is an image of a user story. As you can see on the figure, the user story has the number 88 and was created by the user Eduardsen. It has the tags \"priority: highest\" and \"type: feature\" and is under the milestone called sprint 2. In the right hand side you can see the profile picture of the developer currently assigned to implementing this user story. Another approach to creating user stories is when developer groups create feature requests. In most cases these need to be rewritten into user stories if the problem they are describing has not been formulated correctly or adequately. When creating user stories, take special care to ensure that your user stories are not written ambiguously and, if so, that you have the precise functionality that the users wanted explained within them.","title":"Creating user stories"},{"location":"Legacy/Handover/2019/from_2019/PO_advice/#prioritizing-user-stories-and-issues","text":"The figure below shows a guide for how to prioritize user stories: The figure is available in .svg format here and can be edited in tools like draw.io if updates are needed.","title":"Prioritizing user stories and issues"},{"location":"Legacy/Handover/2019/from_2019/PO_advice/#prototypes","text":"One of the important tasks of the PO group is to create and maintain prototypes. Prototypes should conform to the design guide which can be found on the Github Wiki-page. PO groups from previous years had been using PowerPoint to create prototypes, but we found this process to be slow and require a lot of repetitive work. Instead, we opted to use Adobe XD which reduced repetitive work and let us import the exact icons we used throughout the design guide, making the prototypes much more consistent. Doing meetings with the customers you should show them the prototypes you have made since the last time you spoke. It is a very good idea to get customer feedback so that you know you are headed in the right direction in relation to their expectations. It is also helpful if developers have a creative way to interpret a user story, as it allows you to show them how it should be with certainty. The prototypes can be found in both pdf and Adobe XD format here . Remember to update the files whenever you change the prototypes, so that the development team are up to date.","title":"Prototypes"},{"location":"Legacy/Handover/2019/from_2019/PO_advice/#distribution-of-user-stories","text":"In the beginning of a new sprint you should present the goals of the sprint to all the other groups. Here you should also present the user stories you believe correspond to these goals. Afterwards, groups are free to take one or two user stories that they can start working on from the ones you presented. These are the user stories will help you reach the functionality defined in the sprint goal, and should be prioritized highly. If a development group finishes their user stories before the sprint is over, they should be able to contact the PO group by writing or visiting the PO group to ask for permission to start a new user story. This should be done so the PO group can constantly keep track of the user stories currently being implemented. We decided to continually update a board in our group room with the status of all development groups and their user stories. For the final sprint we chose to decide how to distribute user stories. We did this because we had a lot of documentation tasks that needed to be done and decided to distribute these as well to increase the likelihood of them being finished for next years groups. Consider doing the same for final/short sprints to maximize value for the customer. As you know which groups implemented which stories, and how productive the different groups are, you can use this to your advantage in distributing tasks.","title":"Distribution of user stories"},{"location":"Legacy/Handover/2019/from_2019/PO_advice/#communication-with-other-groups","text":"During a sprint it is crucial to keep track of the status of other groups. Certain user stories can be blocked by other user stories and therefore you should regularly walk to the other group rooms and ask how their work is proceeding. Being the PO group you should have a good overview of the developer groups' skills. If a certain group is stuck with a user story you should ask other groups to help them complete it. This is a great way to share knowledge and speed up development in some cases. Knowing what people are working on also gives you the advantage of knowing what files they most likely are making changes in. This information can help you decide which user stories developer groups should take so that you reduce the amount of merge conflicts.","title":"Communication with other groups"},{"location":"Legacy/Handover/2019/from_2019/PO_advice/#cooperation-with-process-group","text":"A lot of the choices in regards to the process affects the work of the PO group. Because of this it is essential that you communicate with them and ensure that you are in agreement about what is being planned.","title":"Cooperation with process group"},{"location":"Legacy/Handover/2019/from_2019/PO_advice/#approval-of-designs","text":"We recommend enforcing that the PO group should be assigned to all pull requests that deal with the user interface of the application, as they are the group with the best understanding of how the customers interact with the system. This means that the design should be consistent with the prototypes that have been approved by the customer, and be very intuitive for them to use. It is a commonly known fact that software engineers are not designers and do not always think of user friendliness, so remember to keep them in check so the customers can actually figure out how to use the system. Some things we have noticed that you should pay special attention towards in design related PRs are: Good error messages should be shown to the user Icons are only used for one thing The design should be consistent with the design guide","title":"Approval of designs"},{"location":"Legacy/Handover/2019/from_2019/PO_advice/#release-preparation","text":"Be prepared that at the end of every sprint you will not have much time to finish your assigned user stories, because the last days of every sprint will be spent on preparing for the release. At the first release preparation we tried to get as many user stories included in the release as possible. This meant that we were waiting for some groups to finish their user stories. We realized that this was not feasible and that we had to make a concrete deadline. This deadline had to be kept. Even though one of the user stories just needed 10 minutes extra before it would be completed it would have to wait until next release to be included. It was not feasible for us to keep delaying the release. When the deadline was over, the release branch was created. Then we would cooperate with the process group to assign groups to review the different user stories that had been implemented throughout the sprint and try to find bugs. When a development group found a bug, they would create a release fix issue and the group that originally implemented the user story that was being investigated would be assigned to fixing the issue. At the first release we assigned every group to some issues and let them review them by themselves. This worked decently, but was a bit hard to coordinate because all the communication was done through Slack. At the second release we chose, in collaboration with the process group, to host a hackathon for release fixing where all the groups sat together. This made it much easier to communicate between all the groups and we felt that the process was much smoother because of this. It is a good idea to document the releases as they are happening. This makes it much easier for you as the PO group to write about them in the report as well as in the wiki. Keep track of which issues were included in the release and which groups worked on these issues.","title":"Release preparation"},{"location":"Legacy/Handover/2019/from_2019/PO_advice/#usability-testing","text":"At the end of each sprint, a usability test should be conducted to get feedback on the features that have been developed in the sprint, and on potential new prototypes. We recommend that you arrange the usability tests shortly after the sprint has ended, so that you have time to translate the results into user stories. The customers usually prefer that the usability tests are held on weekdays at around 8am, so that they can do it before going to work. Remember to book a room for the usability test, and have someone from the group bring coffee and mugs to make it cozier for the customers - that makes them want to come back. For the usability tests, you should have a series of assignments prepared for the users, based on the latest release. Remember to use their time well, and pose follow-up questions to the assignments and things you are uncertain of to clarify potential problems. If it goes smoothly, you can show them additional features on the develop branch, even though they may be buggy. Finally, after having conducted the usability test, you can show them new prototypes to get feedback on them.","title":"Usability testing"},{"location":"Legacy/Handover/2019/from_2019/PO_advice/#keeping-github-issues-updated","text":"GitHub is the primary place for keeping track of the system, all features are listed as issues that are spread across the repositories of GIRAF. Thus, it is very important that you keep the information on GitHub up to date. This includes regularly going through the issues and checking if the issues are still relevant, and updating the priorities of the issues. At the end of every sprint, it is important that you go through issues to re-order the priorities of user stories, as each sprint will most likely deplete the repositories of stories that are marked as highest and high priority. Likewise, it is important to keep an eye on the bug reports and feature requests that are reported by other groups and translating these into user stories.","title":"Keeping GitHub issues updated"},{"location":"Legacy/Handover/2019/from_2019/PO_advice/#sprint-planning","text":"The PO group is responsible for planning what should be done in each sprint. At the beginning of each sprint, the PO group will do a short presentation with the goals of the upcoming sprint and present which user stories they have picked out as a focus for the sprint. We came to the conclusion that it is better to include fewer, but more essential user stories to the sprint and let groups come talk to us when they are done with that user story, so that we are sure that they will be able to finish implementation within the time period of the sprint. In the first sprints, we let the groups choose an assignment from the presented user stories themselves, whereas in the last sprint we decided the user stories for each group, based on what they had previously worked on and what essential features we needed to include to have a minimum viable product available. Try to spread the user stories across the system as much as possible, so that multiple groups will not be working on the same screens if this can be avoided. This decreases the amount of merge conflicts and frustration for the developers.","title":"Sprint planning"},{"location":"Legacy/Handover/2019/from_2019/PO_advice/#internal-sharing-of-knowledge","text":"An often overlooked problem for us was to ensure that all members of the PO group shared what information was given to groups when they came in for advice on a user story. If knowledge is not shared properly within the group, it is very easy to cause confusion for not only the PO group, but also the group asking the question, as it may not always be the same person in the PO group they talk to. It may be a good idea to keep a shared document where you can write down your decisions and which groups it was discussed with.","title":"Internal sharing of knowledge"},{"location":"Legacy/Handover/2019/from_2019/general_advice/","text":"General Advice for the Future Giraf Projects \u00b6 This document contains some general advice for the Giraf project. Do Not Switch From The Flutter Framework in the Weekplaner Application \u00b6 For the Giraf 2019 project we chose to switch from Xamarin to Flutter to combat some issues associated with developing Xamarin applications on Linux. This meant that the whole weekplanner application had to be ported over and somewhat rewritten. We estimated that it would take about one month to have the Flutter application at the same state as the Xamarin application. However, this took us almost three months. There was significantly more work in rewriting functionality and learning the new framework than was thought initially. However difficult it might be to pick up Flutter and learn it, we can almost guarantee that rewriting the application in yet another framework would take even longer. Furthermore changing the framework may not pose many benefits to the customers. In retrospect, the change to Flutter had both benefits and drawbacks. The Flutter framework allowed for all students to develop, which we prioritized the highest due to it being a university project. In our opinion the Flutter framework allows for a better looking and more smooth UI experience, although this may not be the customers biggest concern. The migration to Flutter did require three months of development to reach the same functionality as the Xamarin application. This was a huge drawback for the customer, which outside of a studying context should be of the highest priority when making decisions like this. We made the decision with the university aspect as the highest priority. For future development we do not recommend migrating to a new framework, unless the framework becomes unsupported. We urge the coming Giraf project groups (the 2020 project) to perform the analysis of Flutter that we neglected to do and base your final decision on whether to keep Flutter or not on this analysis. However, we will say that should the analysis conclude that Flutter is, in any way, a viable framework for the Giraf project, then you should keep it. This is due to the fact of how much time would be spent changing framework again if you do not choose to keep it.","title":"General Advice for the Future Giraf Projects"},{"location":"Legacy/Handover/2019/from_2019/general_advice/#general-advice-for-the-future-giraf-projects","text":"This document contains some general advice for the Giraf project.","title":"General Advice for the Future Giraf Projects"},{"location":"Legacy/Handover/2019/from_2019/general_advice/#do-not-switch-from-the-flutter-framework-in-the-weekplaner-application","text":"For the Giraf 2019 project we chose to switch from Xamarin to Flutter to combat some issues associated with developing Xamarin applications on Linux. This meant that the whole weekplanner application had to be ported over and somewhat rewritten. We estimated that it would take about one month to have the Flutter application at the same state as the Xamarin application. However, this took us almost three months. There was significantly more work in rewriting functionality and learning the new framework than was thought initially. However difficult it might be to pick up Flutter and learn it, we can almost guarantee that rewriting the application in yet another framework would take even longer. Furthermore changing the framework may not pose many benefits to the customers. In retrospect, the change to Flutter had both benefits and drawbacks. The Flutter framework allowed for all students to develop, which we prioritized the highest due to it being a university project. In our opinion the Flutter framework allows for a better looking and more smooth UI experience, although this may not be the customers biggest concern. The migration to Flutter did require three months of development to reach the same functionality as the Xamarin application. This was a huge drawback for the customer, which outside of a studying context should be of the highest priority when making decisions like this. We made the decision with the university aspect as the highest priority. For future development we do not recommend migrating to a new framework, unless the framework becomes unsupported. We urge the coming Giraf project groups (the 2020 project) to perform the analysis of Flutter that we neglected to do and base your final decision on whether to keep Flutter or not on this analysis. However, we will say that should the analysis conclude that Flutter is, in any way, a viable framework for the Giraf project, then you should keep it. This is due to the fact of how much time would be spent changing framework again if you do not choose to keep it.","title":"Do Not Switch From The Flutter Framework in the Weekplaner Application"},{"location":"Legacy/Handover/2019/from_2019/process_advice/","text":"Advice for the Process of Future Giraf Projects \u00b6 This document provides some advice for the future process groups of the Giraf project. This advice is compiled by the process group of the 2019 Giraf project. Do Use Full-Stack Teams \u00b6 During the Giraf 2019 project we chose to use a full-stack approach instead of the commonly used specialist approach. With the specialist approach each group is assigned a specific part of the system that they work on as a sole focus. E.g. two groups could be server groups, one could be back end, etc. With this approach each feature (user story) is potentially developed by many groups as they each work on different parts of the system to solve the issue. With the full-stack approach each group develops full features (user stories) from top to bottom. This approach has many benefits such as: Every group gets to see more of the code because they implement full features. Less pipelining is needed as features are not made in parts by different groups. People are motivated to finish their tasks because they can see their implementations immediately in the application. Having used the full stack approach for this year's Giraf project, we believe that it is a good fit for this kind of semester project. We would therefore urge future groups to keep using this approach and not go back to the specialist approach. This approach does have downsides that need to be managed still. One downside is that people collaborate less across groups, because every group works on separate user stories. This should be taken into account when using the full-stack approach. However, most of the downsides are not intrinsic to the approach itself and would also be a factor with the specialist approach. Be careful to consider the following points when you think out the process for your own project: How do you find dependencies between user stories? How do you handle dependencies once discovered? How do you make sure that one group's work is not made obsolete by another's? (Especially when the two groups are working in the same files.) We found that handling dependencies needs constant work with our process. It is quickly forgotten by individual groups that their work might affect others or that others might depend on one user story to implement their own if people are not made aware of this constantly. Do Mention Everything Explicitly \u00b6 Throughout the 2019 Giraf project we found that the only way to make sure that every group followed the process correctly was to tell them every part of it (sometimes repeatedly). While this might sound trivial, there are a lot of small things that can be seen as implicit to some people and if the process group finds something to be self-evident, they might not mention it. However small any part of the process might be, it should always be mentioned explicitly to everyone in the project. If you decide to use some form of frequent meetings (e.g. daily Scrum of Scrums stand up meetings), this is a good place to remind people of small things related to process. Even if it might be redundant at the end, at least you know that people are on the same page. Do Plan All Meetings of a Sprint at the Beginning of it \u00b6 It is important that everyone on the Giraf project knows when certain milestones and meetings occur during a sprint. It is all too easy to schedule these (especially frequent recurring meetings such as Scrum of Scrums stand up meetings) on a week by week basis. However, we found that with this way of scheduling, some meetings were scheduled too late for people to notice. This caused some stress for the different groups as they could not schedule their work around meetings far in advance. Later in the project we began scheduling all meetings of a sprint at the beginning of the sprint itself. This meant that everyone knew when all meetings were happening and it also meant that no meetings had to be delayed or skipped because of announcing them too late. Do Enforce a Certain Standard for the Code \u00b6 For most people this will be the first time that they work on a project like this. While many might have different ways of writing code it is important that everyone adheres to same standards for this project. Since the project is an ongoing one that is being developed by many different people through many semesters, it is important to write understandable code. It is all too easy to just code away to maximize what you achieve during a semester, but if the people after you do not understand the code they might not even use it. Do Use Code Review \u00b6 We used code review throughout the 2019 Giraf project. This was a success from the beginning as it made sure that code standards where upheld, that code was written in the correct files, and that the user stories were actually implemented correctly. It is possible to use either internal or external code review, meaning code review done in-group or code review done by other groups. Between these two there is no wrong choice. However, code review needs to be performed by someone who did not help in writing the code that is in review. For the 2019 Giraf project we used external code review and enforced that two separate reviewers sign off on the code before a pull request on GitHub could be successfully merged into the main develop branch. One very important thing about code review is that it should not be used to try and enforce your own coding style onto others. By this we mean that if you are a reviewer of a certain solution to a problem and you think of another solution you should not try to push your own solution, unless there is a real benefit to the overall application. There is not only one way to solve something when coding and you should respect that people have different ways of handling the same issue. Do NOT Dictate How People Work in Their Own Groups \u00b6 While there should be one cohesive process for how everyone works between groups during the Giraf project, there is no need to force a standardized process onto the individual groups. Respect that people work best in different ways and just make sure that they are present at meetings and meet deadlines.","title":"Advice for the Process of Future Giraf Projects"},{"location":"Legacy/Handover/2019/from_2019/process_advice/#advice-for-the-process-of-future-giraf-projects","text":"This document provides some advice for the future process groups of the Giraf project. This advice is compiled by the process group of the 2019 Giraf project.","title":"Advice for the Process of Future Giraf Projects"},{"location":"Legacy/Handover/2019/from_2019/process_advice/#do-use-full-stack-teams","text":"During the Giraf 2019 project we chose to use a full-stack approach instead of the commonly used specialist approach. With the specialist approach each group is assigned a specific part of the system that they work on as a sole focus. E.g. two groups could be server groups, one could be back end, etc. With this approach each feature (user story) is potentially developed by many groups as they each work on different parts of the system to solve the issue. With the full-stack approach each group develops full features (user stories) from top to bottom. This approach has many benefits such as: Every group gets to see more of the code because they implement full features. Less pipelining is needed as features are not made in parts by different groups. People are motivated to finish their tasks because they can see their implementations immediately in the application. Having used the full stack approach for this year's Giraf project, we believe that it is a good fit for this kind of semester project. We would therefore urge future groups to keep using this approach and not go back to the specialist approach. This approach does have downsides that need to be managed still. One downside is that people collaborate less across groups, because every group works on separate user stories. This should be taken into account when using the full-stack approach. However, most of the downsides are not intrinsic to the approach itself and would also be a factor with the specialist approach. Be careful to consider the following points when you think out the process for your own project: How do you find dependencies between user stories? How do you handle dependencies once discovered? How do you make sure that one group's work is not made obsolete by another's? (Especially when the two groups are working in the same files.) We found that handling dependencies needs constant work with our process. It is quickly forgotten by individual groups that their work might affect others or that others might depend on one user story to implement their own if people are not made aware of this constantly.","title":"Do Use Full-Stack Teams"},{"location":"Legacy/Handover/2019/from_2019/process_advice/#do-mention-everything-explicitly","text":"Throughout the 2019 Giraf project we found that the only way to make sure that every group followed the process correctly was to tell them every part of it (sometimes repeatedly). While this might sound trivial, there are a lot of small things that can be seen as implicit to some people and if the process group finds something to be self-evident, they might not mention it. However small any part of the process might be, it should always be mentioned explicitly to everyone in the project. If you decide to use some form of frequent meetings (e.g. daily Scrum of Scrums stand up meetings), this is a good place to remind people of small things related to process. Even if it might be redundant at the end, at least you know that people are on the same page.","title":"Do Mention Everything Explicitly"},{"location":"Legacy/Handover/2019/from_2019/process_advice/#do-plan-all-meetings-of-a-sprint-at-the-beginning-of-it","text":"It is important that everyone on the Giraf project knows when certain milestones and meetings occur during a sprint. It is all too easy to schedule these (especially frequent recurring meetings such as Scrum of Scrums stand up meetings) on a week by week basis. However, we found that with this way of scheduling, some meetings were scheduled too late for people to notice. This caused some stress for the different groups as they could not schedule their work around meetings far in advance. Later in the project we began scheduling all meetings of a sprint at the beginning of the sprint itself. This meant that everyone knew when all meetings were happening and it also meant that no meetings had to be delayed or skipped because of announcing them too late.","title":"Do Plan All Meetings of a Sprint at the Beginning of it"},{"location":"Legacy/Handover/2019/from_2019/process_advice/#do-enforce-a-certain-standard-for-the-code","text":"For most people this will be the first time that they work on a project like this. While many might have different ways of writing code it is important that everyone adheres to same standards for this project. Since the project is an ongoing one that is being developed by many different people through many semesters, it is important to write understandable code. It is all too easy to just code away to maximize what you achieve during a semester, but if the people after you do not understand the code they might not even use it.","title":"Do Enforce a Certain Standard for the Code"},{"location":"Legacy/Handover/2019/from_2019/process_advice/#do-use-code-review","text":"We used code review throughout the 2019 Giraf project. This was a success from the beginning as it made sure that code standards where upheld, that code was written in the correct files, and that the user stories were actually implemented correctly. It is possible to use either internal or external code review, meaning code review done in-group or code review done by other groups. Between these two there is no wrong choice. However, code review needs to be performed by someone who did not help in writing the code that is in review. For the 2019 Giraf project we used external code review and enforced that two separate reviewers sign off on the code before a pull request on GitHub could be successfully merged into the main develop branch. One very important thing about code review is that it should not be used to try and enforce your own coding style onto others. By this we mean that if you are a reviewer of a certain solution to a problem and you think of another solution you should not try to push your own solution, unless there is a real benefit to the overall application. There is not only one way to solve something when coding and you should respect that people have different ways of handling the same issue.","title":"Do Use Code Review"},{"location":"Legacy/Handover/2019/from_2019/process_advice/#do-not-dictate-how-people-work-in-their-own-groups","text":"While there should be one cohesive process for how everyone works between groups during the Giraf project, there is no need to force a standardized process onto the individual groups. Respect that people work best in different ways and just make sure that they are present at meetings and meet deadlines.","title":"Do NOT Dictate How People Work in Their Own Groups"},{"location":"Legacy/Handover/2020E/","text":"Overview \u00b6 This section contains the handover material from the 2020E semester. An overview of the different materials can be seen below: Role Structure Sprint Events Tools Issues For advice on the process and recommendations we refer to the reports of each individual group.","title":"Overview"},{"location":"Legacy/Handover/2020E/#overview","text":"This section contains the handover material from the 2020E semester. An overview of the different materials can be seen below: Role Structure Sprint Events Tools Issues For advice on the process and recommendations we refer to the reports of each individual group.","title":"Overview"},{"location":"Legacy/Handover/2020E/feature_overview/","text":"Feature Overview \u00b6 This page is an overview of the features that should be implemented in the application. Each feature has a description, and a list of related issues, that should describe the details of the required solution. This is meant to eliminate the need to look through all the repositories for issues related to a specific feature. Folder System \u00b6 The citizens at Egebakken are divided into grades, and the citizens at Birken are divided into groups. The class system is meant to organise the overview of citizens in a way that reflects this. This should be done by creating a folder system, where each folder is a collection of citizens. Issues \u00b6 Issue Description weekplanner#510 Update weekplanner with a new 'Folder' screen and bloc web-api#134 Update web-api with new Folder entity api_client#61 Update the api client with the folder entity weekplanner#512 As a guardian I would like to be able to move a citizen from one folder to another weekplanner#513 As a guardian I would like to be able to make a new folder weekplanner#677 As a guardian I would like to be able to create nested folders Additional User Roles \u00b6 Aside from the guardian and citizen roles, there should be an administrator role and a parent role. Administrators should be able to create and delete guardians, as well as delete citizens. This role is an extension of the guardian role, meaning an administrator should also have all the privileges of a guardian. Parents should only be able to access their own children, and should have limited privileges to make changes in the application. The administrator role is meant to make it easier for guardians to use the app. The administrators at an institution should be a few guardians, with more knowledge of the application. The other guardians will then be able to contact a colleague, and ask them to carry out an administrative task. This could also serve to reduce the number of mistakes, by limiting the access to critical features to a few experienced guardians. Issues \u00b6 Issue Description web-api#164 Add support for new user types api_client#78 Add support for new user types weekplanner#624 As an administrator I would like to be able to reset a user\u2019s password weekplanner#625 As an administrator I would like to be able to create a new user weekplanner#626 As an administrator I would like to be able to delete a user weekplanner#627 As an administrator I would like to be able to elevate a guardian to be an administrator weekplanner#628 As an administrator I would like a settings page for my extended permissions weekplanner#629 As a parent I should only be able to access my children\u2019s week plans weekplanner#662 As an administrator I would like to be able to demote a guardian to no longer be an administrator Offline Mode \u00b6 The app should be functional offline, and whenever possible continue working in the same manner, as if it had been online. The customers from Egebakken wanted this functionality as the citizens at the institution could become dependent on the app. Therefore it should not stop working if there is no internet connection. Futheremore it is to ensure that the behaviour of the app is consistent, as unexpected behaviour could upset or confuse some of the citizens. There is a guide on how to implement the offline functionality here . The issues related to this feature have the label \"Offline mode\". Issues \u00b6 Issue Description weekplanner#9 As a guardian, I would like that the app is fully available offline so that I can still use it if the internet is down api_client#80 As a developer I would like an interface for the offline features weekplanner#388 As a guardian I would like to be able to log in when offline if I have been logged in online before weekplanner#389 As a developer I need code infrastructure for a local database and model adapters in order to implement offline functionality api_client#72 As a developer I need code infrastructure for a local database and model adapters in order to implement offline functionality weekplanner#390 As a guardian I would like my latest changes (offline and online) to be synchronized with the online database when my device is online. weekplanner#397 As a guardian I would like to be able to take a picture to use as a pictogram in offline mode and optionally sync it when online again. weekplanner#400 As a citizen I would like to mark an activity in the current week as completed while offline. weekplanner#402 As a citizen I would like to use an activity\u2019s timer while offline. weekplanner#405 As a guardian I would like to cancel activities in the current weekplan while offline. weekplanner#406 As a guardian I would like to use the timer functionality in the current weekplan while offline. weekplanner#407 As a guardian I would like to edit the weekplan's activities for the current week while offline. weekplanner#408 As a guardian I would like to view the current week in the weekplans overview while offline weekplanner#409 As a guardian I would like to edit the weekplan for the current week while offline. weekplanner#410 As a guardian I would like to create and delete weekplans while offline weekplanner#411 As a guardian I would like to use all functionalities as a guardian not limited to the current week weekplanner#412 As a guardian I would like to access and edit a citizen\u2019s settings while offline weekplanner#414 As a citizen I would like to be able to view my activities and my weekplan for the current week while offline. weekplanner#415 As a guardian I would like to view my activities and the weekplan for the current week while offline. weekplanner#632 As a guardian I would like to have the pictograms that are used in my citizens' week plans to be available offline Pictogram Management \u00b6 Pictograms are central to GIRAF, many of the tasks in the product backlog concern the management of pictograms. This is a collection of issues that all concern the management of pictograms in the application. Issues \u00b6 Issue Description weekplanner#134 As a guardian, I would like a way to add pictograms by taking a picture from my camera so that I can quickly improvise if the system does not have the activity I want weekplanner#219 As a guardian I would like a visual representation of where the pictogram I'm dragging is going to end up on the weekplan so that I can easily place it correctly weekplanner#227 As a guardian, I would like the search for pictograms to be ordered by how popular a pictogram is, so that I can find the most commonly used pictogram quickly weekplanner#266 As a guardian I would like to be able to edit a text to a pictogram in a weekplan so that other guardians know what I mean by it weekplanner#572 As a user i would like to sort my pictograms by categories when searching. weekplanner#631 As a guardian I would like to delete a pictogram, but any activities that use that pictogram should stay, and their picture should be replace by a red warning weekplanner#634 As a guardian I would like to receive a warning if I\u2019m copying a week plan with an activity that is missing a pictogram weekplanner#635 As a guardian I would like to delete a pictogram without changing the name of activities that use it weekplanner#639 As a guardian I would like a page for managing pictograms without selecting a citizen weekplanner#640 As a guardian I would like a page for managing pictograms after selecting a citizen weekplanner#643 As a guardian I would like to be able to view more pages when searching for pictograms weekplanner#644 As a guardian I would like the images that I upload to be cropped to fit the activity box weekplanner#652 As a guardian I would like to be able to have pictograms in upper case and lower case weekplanner#666 As a guardian I would like for \"Offentlig\" to be renamed to \"Institution\" when uploading pictograms weekplanner#667 As a guardian I would like for the bar on the \"Add pictogram\" page to be consistent with the rest of the application weekplanner#671 As a guardian I would like to have names under the pictograms when searching for them weekplanner#672 As a guardian I would like for photos added as pictograms to have the same aspect ratio as standard pictograms weekplanner#676 As a guardian I would like to be able to decide how a photo added as a pictogram should be cut to have the same aspect ratio as standard pictograms","title":"Feature Overview"},{"location":"Legacy/Handover/2020E/feature_overview/#feature-overview","text":"This page is an overview of the features that should be implemented in the application. Each feature has a description, and a list of related issues, that should describe the details of the required solution. This is meant to eliminate the need to look through all the repositories for issues related to a specific feature.","title":"Feature Overview"},{"location":"Legacy/Handover/2020E/feature_overview/#folder-system","text":"The citizens at Egebakken are divided into grades, and the citizens at Birken are divided into groups. The class system is meant to organise the overview of citizens in a way that reflects this. This should be done by creating a folder system, where each folder is a collection of citizens.","title":"Folder System"},{"location":"Legacy/Handover/2020E/feature_overview/#issues","text":"Issue Description weekplanner#510 Update weekplanner with a new 'Folder' screen and bloc web-api#134 Update web-api with new Folder entity api_client#61 Update the api client with the folder entity weekplanner#512 As a guardian I would like to be able to move a citizen from one folder to another weekplanner#513 As a guardian I would like to be able to make a new folder weekplanner#677 As a guardian I would like to be able to create nested folders","title":"Issues"},{"location":"Legacy/Handover/2020E/feature_overview/#additional-user-roles","text":"Aside from the guardian and citizen roles, there should be an administrator role and a parent role. Administrators should be able to create and delete guardians, as well as delete citizens. This role is an extension of the guardian role, meaning an administrator should also have all the privileges of a guardian. Parents should only be able to access their own children, and should have limited privileges to make changes in the application. The administrator role is meant to make it easier for guardians to use the app. The administrators at an institution should be a few guardians, with more knowledge of the application. The other guardians will then be able to contact a colleague, and ask them to carry out an administrative task. This could also serve to reduce the number of mistakes, by limiting the access to critical features to a few experienced guardians.","title":"Additional User Roles"},{"location":"Legacy/Handover/2020E/feature_overview/#issues_1","text":"Issue Description web-api#164 Add support for new user types api_client#78 Add support for new user types weekplanner#624 As an administrator I would like to be able to reset a user\u2019s password weekplanner#625 As an administrator I would like to be able to create a new user weekplanner#626 As an administrator I would like to be able to delete a user weekplanner#627 As an administrator I would like to be able to elevate a guardian to be an administrator weekplanner#628 As an administrator I would like a settings page for my extended permissions weekplanner#629 As a parent I should only be able to access my children\u2019s week plans weekplanner#662 As an administrator I would like to be able to demote a guardian to no longer be an administrator","title":"Issues"},{"location":"Legacy/Handover/2020E/feature_overview/#offline-mode","text":"The app should be functional offline, and whenever possible continue working in the same manner, as if it had been online. The customers from Egebakken wanted this functionality as the citizens at the institution could become dependent on the app. Therefore it should not stop working if there is no internet connection. Futheremore it is to ensure that the behaviour of the app is consistent, as unexpected behaviour could upset or confuse some of the citizens. There is a guide on how to implement the offline functionality here . The issues related to this feature have the label \"Offline mode\".","title":"Offline Mode"},{"location":"Legacy/Handover/2020E/feature_overview/#issues_2","text":"Issue Description weekplanner#9 As a guardian, I would like that the app is fully available offline so that I can still use it if the internet is down api_client#80 As a developer I would like an interface for the offline features weekplanner#388 As a guardian I would like to be able to log in when offline if I have been logged in online before weekplanner#389 As a developer I need code infrastructure for a local database and model adapters in order to implement offline functionality api_client#72 As a developer I need code infrastructure for a local database and model adapters in order to implement offline functionality weekplanner#390 As a guardian I would like my latest changes (offline and online) to be synchronized with the online database when my device is online. weekplanner#397 As a guardian I would like to be able to take a picture to use as a pictogram in offline mode and optionally sync it when online again. weekplanner#400 As a citizen I would like to mark an activity in the current week as completed while offline. weekplanner#402 As a citizen I would like to use an activity\u2019s timer while offline. weekplanner#405 As a guardian I would like to cancel activities in the current weekplan while offline. weekplanner#406 As a guardian I would like to use the timer functionality in the current weekplan while offline. weekplanner#407 As a guardian I would like to edit the weekplan's activities for the current week while offline. weekplanner#408 As a guardian I would like to view the current week in the weekplans overview while offline weekplanner#409 As a guardian I would like to edit the weekplan for the current week while offline. weekplanner#410 As a guardian I would like to create and delete weekplans while offline weekplanner#411 As a guardian I would like to use all functionalities as a guardian not limited to the current week weekplanner#412 As a guardian I would like to access and edit a citizen\u2019s settings while offline weekplanner#414 As a citizen I would like to be able to view my activities and my weekplan for the current week while offline. weekplanner#415 As a guardian I would like to view my activities and the weekplan for the current week while offline. weekplanner#632 As a guardian I would like to have the pictograms that are used in my citizens' week plans to be available offline","title":"Issues"},{"location":"Legacy/Handover/2020E/feature_overview/#pictogram-management","text":"Pictograms are central to GIRAF, many of the tasks in the product backlog concern the management of pictograms. This is a collection of issues that all concern the management of pictograms in the application.","title":"Pictogram Management"},{"location":"Legacy/Handover/2020E/feature_overview/#issues_3","text":"Issue Description weekplanner#134 As a guardian, I would like a way to add pictograms by taking a picture from my camera so that I can quickly improvise if the system does not have the activity I want weekplanner#219 As a guardian I would like a visual representation of where the pictogram I'm dragging is going to end up on the weekplan so that I can easily place it correctly weekplanner#227 As a guardian, I would like the search for pictograms to be ordered by how popular a pictogram is, so that I can find the most commonly used pictogram quickly weekplanner#266 As a guardian I would like to be able to edit a text to a pictogram in a weekplan so that other guardians know what I mean by it weekplanner#572 As a user i would like to sort my pictograms by categories when searching. weekplanner#631 As a guardian I would like to delete a pictogram, but any activities that use that pictogram should stay, and their picture should be replace by a red warning weekplanner#634 As a guardian I would like to receive a warning if I\u2019m copying a week plan with an activity that is missing a pictogram weekplanner#635 As a guardian I would like to delete a pictogram without changing the name of activities that use it weekplanner#639 As a guardian I would like a page for managing pictograms without selecting a citizen weekplanner#640 As a guardian I would like a page for managing pictograms after selecting a citizen weekplanner#643 As a guardian I would like to be able to view more pages when searching for pictograms weekplanner#644 As a guardian I would like the images that I upload to be cropped to fit the activity box weekplanner#652 As a guardian I would like to be able to have pictograms in upper case and lower case weekplanner#666 As a guardian I would like for \"Offentlig\" to be renamed to \"Institution\" when uploading pictograms weekplanner#667 As a guardian I would like for the bar on the \"Add pictogram\" page to be consistent with the rest of the application weekplanner#671 As a guardian I would like to have names under the pictograms when searching for them weekplanner#672 As a guardian I would like for photos added as pictograms to have the same aspect ratio as standard pictograms weekplanner#676 As a guardian I would like to be able to decide how a photo added as a pictogram should be cut to have the same aspect ratio as standard pictograms","title":"Issues"},{"location":"Legacy/Handover/2020E/Issues/","text":"Overview \u00b6 This section describes any issues that are important yet unfinished at the end of a semester. These issues are documented with the purpose of making it easier for future GIRAF teams to continue working on them. The issues at the time of writing are: GIRAF Offline database","title":"Overview"},{"location":"Legacy/Handover/2020E/Issues/#overview","text":"This section describes any issues that are important yet unfinished at the end of a semester. These issues are documented with the purpose of making it easier for future GIRAF teams to continue working on them. The issues at the time of writing are: GIRAF Offline database","title":"Overview"},{"location":"Legacy/Handover/2020E/Issues/issue_418/","text":"Issue 418 \u00b6 This issue has a branch in the weekplanner repository. What currently exists \u00b6 At the time of writing (December 2020) the functionality requested in #418 has been implemented. However, the design of the implementation is not made in accordance to the prototype given in the issue. The design is oriented more towards the Material Design principles, as this is better for interaction on smartphones compared to the design presented in the prototypes. In the case you want the solution to be more oriented towards the Material Design principles, it is possible to move the search bar up into the app bar. Implementing prototypes \u00b6 If you want to implement the prototypes given on the issue page, you will need to implement a complete refactor of the weekplan view. Furthermore, in this case you cannot use the solution on the branch, except for the search method. This refactor will have to implement a state to contain the weekplans which the user should be viewing. Then have the weekplan screen reactively alter the view to reflect the weekplans contained in the state.","title":"Issue 418"},{"location":"Legacy/Handover/2020E/Issues/issue_418/#issue-418","text":"This issue has a branch in the weekplanner repository.","title":"Issue 418"},{"location":"Legacy/Handover/2020E/Issues/issue_418/#what-currently-exists","text":"At the time of writing (December 2020) the functionality requested in #418 has been implemented. However, the design of the implementation is not made in accordance to the prototype given in the issue. The design is oriented more towards the Material Design principles, as this is better for interaction on smartphones compared to the design presented in the prototypes. In the case you want the solution to be more oriented towards the Material Design principles, it is possible to move the search bar up into the app bar.","title":"What currently exists"},{"location":"Legacy/Handover/2020E/Issues/issue_418/#implementing-prototypes","text":"If you want to implement the prototypes given on the issue page, you will need to implement a complete refactor of the weekplan view. Furthermore, in this case you cannot use the solution on the branch, except for the search method. This refactor will have to implement a state to contain the weekplans which the user should be viewing. Then have the weekplan screen reactively alter the view to reflect the weekplans contained in the state.","title":"Implementing prototypes"},{"location":"Legacy/Handover/2020E/Issues/issue_526/","text":"Issue 526 \u00b6 There is created a branch (feature/526) for this issue. The first thing that has been created is a gradient that is behind the elements, the gradient goes from white to transparent and then to white. There are two transparent colors to make it prettier. 1 2 3 4 5 6 7 8 9 10 child: Container( decoration: BoxDecoration( gradient: LinearGradient( stops: [0.0, 0.2, 0.8, 1.0], colors: [ Color(color1), const Color(0x00000000), const Color(0x00000000), Color(color2), ], This is how it will look like. The other thing that has been added is a notifier that makes it possible to know when the user has scrolled to the top and bottom. There are three if statements: The first if statement checks if the scoll is at the bottom The second if statement checks if the scroll is at the top The third if statement checks if the scroll is both on top and bottom (The third if statements has not been checked if it works, but the other two has been checked with prints) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 onNotification: (ScrollNotification scrollInfo) { if (scrollInfo.metrics.pixels == scrollInfo.metrics.maxScrollExtent) { print('at the bottom'); //print(scrollInfo.metrics.pixels); color1 = 0xFFFFFFFF; color2 = 0x00000000; //print(color1.toString()); //print(color2.toString()); return true; } else if (scrollInfo.metrics.pixels == scrollInfo.metrics.minScrollExtent) { print('at the top'); print(scrollInfo.metrics.pixels); color1 = 0x00000000; color2 = 0xFFFFFFFF; print(color1.toString()); print(color2.toString()); return true; } else if (scrollInfo.metrics.pixels == scrollInfo.metrics.maxScrollExtent && scrollInfo.metrics.pixels == scrollInfo.metrics.minScrollExtent){ color1 = 0x00000000; color2 = 0x00000000; print(color1.toString()); print(color2.toString()); } This is an example if the user scrolls to the bottom. What the next developer should do \u00b6 You should connect these two features together so that they communicate. We have tried changing the values of the gradient through the notifier (second feature), but the gradient (first feature) won't change color because it has to be updated. And to do that we have to use a stateful widget, since this is built on a stateless widget we couldn't just update the information or just update the widget. We would have to update the whole screen which might reset the scrolling (which would send the scrolling back to the top). We have also tried making the whole class stateful, but that broke the screen and the columns became red and had error messages on them. Maybe try and create a new stateful class, where the columns can be created and update without disturbing the other widgets.","title":"Issue 526"},{"location":"Legacy/Handover/2020E/Issues/issue_526/#issue-526","text":"There is created a branch (feature/526) for this issue. The first thing that has been created is a gradient that is behind the elements, the gradient goes from white to transparent and then to white. There are two transparent colors to make it prettier. 1 2 3 4 5 6 7 8 9 10 child: Container( decoration: BoxDecoration( gradient: LinearGradient( stops: [0.0, 0.2, 0.8, 1.0], colors: [ Color(color1), const Color(0x00000000), const Color(0x00000000), Color(color2), ], This is how it will look like. The other thing that has been added is a notifier that makes it possible to know when the user has scrolled to the top and bottom. There are three if statements: The first if statement checks if the scoll is at the bottom The second if statement checks if the scroll is at the top The third if statement checks if the scroll is both on top and bottom (The third if statements has not been checked if it works, but the other two has been checked with prints) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 onNotification: (ScrollNotification scrollInfo) { if (scrollInfo.metrics.pixels == scrollInfo.metrics.maxScrollExtent) { print('at the bottom'); //print(scrollInfo.metrics.pixels); color1 = 0xFFFFFFFF; color2 = 0x00000000; //print(color1.toString()); //print(color2.toString()); return true; } else if (scrollInfo.metrics.pixels == scrollInfo.metrics.minScrollExtent) { print('at the top'); print(scrollInfo.metrics.pixels); color1 = 0x00000000; color2 = 0xFFFFFFFF; print(color1.toString()); print(color2.toString()); return true; } else if (scrollInfo.metrics.pixels == scrollInfo.metrics.maxScrollExtent && scrollInfo.metrics.pixels == scrollInfo.metrics.minScrollExtent){ color1 = 0x00000000; color2 = 0x00000000; print(color1.toString()); print(color2.toString()); } This is an example if the user scrolls to the bottom.","title":"Issue 526"},{"location":"Legacy/Handover/2020E/Issues/issue_526/#what-the-next-developer-should-do","text":"You should connect these two features together so that they communicate. We have tried changing the values of the gradient through the notifier (second feature), but the gradient (first feature) won't change color because it has to be updated. And to do that we have to use a stateful widget, since this is built on a stateless widget we couldn't just update the information or just update the widget. We would have to update the whole screen which might reset the scrolling (which would send the scrolling back to the top). We have also tried making the whole class stateful, but that broke the screen and the columns became red and had error messages on them. Maybe try and create a new stateful class, where the columns can be created and update without disturbing the other widgets.","title":"What the next developer should do"},{"location":"Legacy/Handover/2020E/Issues/offline_mode/","text":"GIRAF Offline database \u00b6 The customers wish to use the application when offline. This could for example be when they are on a trip in the forest, where Wi-Fi is not an option and cell service might be unavailable. Current solution \u00b6 We have chosen an offline first approach because if Giraf wants to be a widely used app on different app markets, it is expected to be highly functional without internet connection and offline first is generally the way to do this. This model describes the dataflow of an offline first database. The current offline database implementation is based on this model. Whenever the UI needs data, it first communicates with the BLoC. The BLoC then queries the API for the data. If the data is already in memory we can just return it. If it is not, we try to find it in our local database. If it is not in the local database either, we look in the online database where it should exist. When you get data from the online database, you also save it in the local database and in memory, such that you can have fast retrieval and also have data available if you are offline. Currently, the in memory part has not been the focus of any kind of development, and the plan is to simply get the offline mode to work with an offline database. This setup allows reading from the local database when offline. However, in order to edit, delete or insert data, without synchronization issues a record of which of these actions were performed offline will need to be kept such that they can be completed when you are online again. This will also require regular checks of whether a connection to the database is available. To best retain the BLoC pattern, and make the offline database and its functionality available to potential future apps, the offline database is implemented in the api_client. The offline database is an SQLite database. The reason for this is that mobile devices already have an SQLite database running in memory for apps to use, and since the online database uses a MySQL database, the mapping between them is relatively easy. Flutter also provides a package for using SQLite called SQFlite , easing the process. Below is a prioritized list of the different features the customers would like to have available offline, along with what issues in the weekplanner deals with that piece of functionality. Obviously the first thing which needs to be implemented is the ability to log in offline. Here is the prioritized list for the offline features \u00b6 Citizen features: View weekplan. ( Issue 414 ) View activity. ( Issue 414 ) Mark activity as completed. ( Issue 400 ) Timer functionality. ( Issue 402 ) Guardian features: View weekplan. ( Issue 415 ) View activity. ( Issue 415 ) Cancel activity. ( Issue 407 ) Timer functionality. ( Issue 406 ) Edit weekplan. ( Issue 407 ) Take picture as pictogram. ( Issue 632 ) Create/delete weekplans. ( Issue 410 ) It is to be noted, that most of these issues specify that they would like the functionality to be available for the current week. With the current implementation of the SQLite database, showing more than one week should not be a problem however. Likewise, adding, editing and deleting weekplans are implemented in the offline database, complete with keeping track of what methods needs to be called again when online. What is lacking in the implementation is simply when this functionality needs to be used. Issues / considerations \u00b6 As part of the design, it is necessary to find a way to handle how and which users are able to login to the offline database. This could possibly be achieved by making it so that only the most recently logged in user is able to use the device online. For the pictograms you would also have to figure out some system for determining which pictograms should be saved locally, and how to save them. They are currently saved to a folder called \"pictograms\" on the device's local storage. Saving every single pictogram might become problematic. Multiple strategies for which pictograms should be saved can be implemented. It is possible to simply only save pictograms which have been used on any screen since the last time a user logged in. This might however result in some images not being available if they are suddenly needed after going offline, even though they are connected to the user. A possibility could also be allowing a guardian to choose which pictograms should be saved for the specific citizen. Another option would be to simply save x amount of most used pictograms to make it as likely as possible that what you would need is available. One possible way of storing these pictograms is using the class ImageCache from Flutter. Currently a self made cache is used in the BLoC used for loading images for pictograms. The implementation of this cache can be found in the pictogram_image_bloc . Another way this could be handled, is using the functionality which is already implemented in the offline database, to save images in a folder onthe device, keeping it available for use even after the app is shut down. This will require some kind of further management of the saved images, as this will result in a lot of images being saved on the device over time. Syncing the local database to match the online database \u00b6 A number of different issues could also appear in relation to synchronization of different instances of data. Cache invalidation scenario: Citizen 1 logs in on their device and downloads their weekplans from the server. Guardian logs in on another device and changes Citizen 1's weekplan for a week. Citizen 1 looks at this weekplan on their device, but this is not the updated version since there is a version in the cache/offline database. One possible solution is to use a timestamp to check if there is changes in the online version. This timestamp would be downloaded and checked whenever a weekplan is opened on a device with an internet connection. This timestamp is then compared with the local version. Another solution would be to automatically check for changes every 30 seconds or something similar to avoid having to reopen weekplans to update them. A guardian could also have a \"Refresh\" button for the citizens, that would download the new changes. Time stamps could also solve the update conflicts since it is possible to compare two versions and save the newest. Currently with the offline database, the idea is to simply update the offline database, each time a call to the offline database. It might be a good idea to check at other times too however. Synchronizing offline changes scenario: Citizen's tablet is offline. A guardian logs in on the same (offline) tablet and changes the citizen's settings. The citizen logs in and can see the local updates. The citizen's device gets internet connection and now the changes has to be synced with the database. But the guardian is not logged in anymore and the citizen does not have permission to update their changes through the web-api. A possible solution would be to give a citizen permissions in the wep-api to make changes. This could be implemented with some kind of token or similar signifying if the changes is coming from the offline database, and then only allowing citizens to make changes if it comes from their offline database. The system should automatically perform the offline changes, and the citizens themselves should not be allowed to directly perform these changes. A potentially very large issue is if offline changes for the same data, e.g an activity is made on two different devices - which of the changes should be saved in the online database, when they both come online. The PO-group has talked to the customers and they want \"last write wins\". To accommodate this, it might be necessary to add more attributes in the offline and online databases in order to deal with of the synchronization. Examples could be \u201clast_updated_on\u201d, \u201ccreated_on\u201d, \u201cdeleted_on\u201d, \u201cedited_offline\u201d which are timestamps used to see if data should be synched or not and the \u201cedited_offline\u201d could be a boolean. It is also an option to use UUID with/instead of timestamps to make the synchronization have a unique id. What currently exists \u00b6 Currently an initial class for communication with an offline database exists in the api_client repository. The api_client maps JSON output from the web-api into models which the weekplanner uses for displaying data models. Thus, it would be essential to implement the offline repository feature in the api_client. Every model in the api_client implements an abstract class called Model which provides a from_json() and to_json() method for the models to interact with the web-api, as well as a from_database() to convert data from the database into a model. The 1-1 approach \u00b6 The solution chosen for the database design is a 1-1 relational database with the web-api. For this approach the online database scheme was cloned to a sqlite scheme creation file. From this file, most of the sql was copied into the dbhandler in the createTables method. Some of the tables and rows were not imported though, as they were either not used, or unnecessary for the app to be able to run offline. If more tables or columns are needed they can simply be added to the table creation function. If something is to be changed in the model layer, this will most likely be your workload: Alter the modellayer in the web-api Migrate the database Customize the unittests Customize the integration tests Alter the models in the api_client Customize the unittests Integration test between web_api and api_client Alter the unittests Alter the weekplanner to use the new feature Integration test between weekplanner and the offline repository Whenever a major change is made to the online database, it is important to also remember to change the offline database to match it, such that everything is saved correctly. The dbHandler \u00b6 In terms of implementation, there currently is a class called dbhandler . This class is created as a static object within itself, such that there is only ever one instance of it, and it does not need to compete for access to the SQLite database. It contains a function for each of the calls in the different API's. What we envisioned is that when one of these are called, it should try and get the data from offline first, and then if they do not find a result in offline, the APIs should make an HTTP request instead. The methods which creates new objects, needs to always make the HTTP POST requests, as we need to get an ID from the online database as soon as possible. Until online responds with the object with an updated ID, we generate an UUID for all objects when they are created, in an attempt to ensure that all objects have a unique ID. If a mutating transaction to the online database fails, it should be saved in the database with the function saveFailedTransactions() in the dbHandler. It would also be necessary to find a good time to call retryFailedTransactions() , which tries to send all the mutating transactions to the database again. The Api's \u00b6 All of the Api's should first call the offline database. If nothing was found there, then call the online and use the what is returned from online to hydrate the offline database. For this to work, all methods in the api's need to be made async* which means that it is a stream. In order to use these there is a few important things to notice: Streams use yield instead of return When the function yields something, the it continues to run The unittests for the functions are not created with async in mind, so they need to be changed Unit tests \u00b6 Most of the functions in the dbhandler has been tested in the OfflineDatabase_test file. Some are still missing though, and they need to be created before the offline database can be merged with develop. The database tests relies on SQFlite ffi which is a library that overwrites the factory in sqflite, to allow the database to work on windows, linux and MacOs. The unit tests which currently tests the APIs only works if the APIs are async. This can for the most part be fixed by delaying the expect. An example of this from account_api_test.dart is: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 test ( 'Should call login endpoint' , () async { accountApi . login ( 'username' , 'password' ) . listen ( expectAsync1 (( bool success ) { expect ( success , isTrue ); })); httpMock . expectOne ( url: '/login' , method: Method . post ) . flush ( < String , dynamic > { 'data' : 'TestToken' , 'message' : '' , 'errorKey' : 'NoError' , }); }); In this example a login request is made in the account API. The mocked httpclient inside the accountApi then stores that request in a list in the httpclient. Then the expectone on the httpMock checks if there is exactly one post method to login inside it. This was fine when the API was not async, but even though it is made async, we still cant use await to ensure that the values are in the httpMock before we expect. The solution we came up with seems a bit hacky, but it was the best solution we could come up with. Because of the way Flutter does async, we created a very short delay, to allow the http request to be set into the httpMock before checking it. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 test ( 'Should call login endpoint' , () async { accountApi . login ( 'username' , 'password' ) . listen ( expectAsync1 (( bool success ) { expect ( success , isTrue ); })); Future < void > . delayed ( const Duration ( milliseconds: 10 )). then (( _ ) => httpMock . expectOne ( url: '/login' , method: Method . post ) . flush ( < String , dynamic > { 'data' : 'TestToken' , 'message' : '' , 'errorKey' : 'NoError' , })); });","title":"GIRAF Offline database"},{"location":"Legacy/Handover/2020E/Issues/offline_mode/#giraf-offline-database","text":"The customers wish to use the application when offline. This could for example be when they are on a trip in the forest, where Wi-Fi is not an option and cell service might be unavailable.","title":"GIRAF Offline database"},{"location":"Legacy/Handover/2020E/Issues/offline_mode/#current-solution","text":"We have chosen an offline first approach because if Giraf wants to be a widely used app on different app markets, it is expected to be highly functional without internet connection and offline first is generally the way to do this. This model describes the dataflow of an offline first database. The current offline database implementation is based on this model. Whenever the UI needs data, it first communicates with the BLoC. The BLoC then queries the API for the data. If the data is already in memory we can just return it. If it is not, we try to find it in our local database. If it is not in the local database either, we look in the online database where it should exist. When you get data from the online database, you also save it in the local database and in memory, such that you can have fast retrieval and also have data available if you are offline. Currently, the in memory part has not been the focus of any kind of development, and the plan is to simply get the offline mode to work with an offline database. This setup allows reading from the local database when offline. However, in order to edit, delete or insert data, without synchronization issues a record of which of these actions were performed offline will need to be kept such that they can be completed when you are online again. This will also require regular checks of whether a connection to the database is available. To best retain the BLoC pattern, and make the offline database and its functionality available to potential future apps, the offline database is implemented in the api_client. The offline database is an SQLite database. The reason for this is that mobile devices already have an SQLite database running in memory for apps to use, and since the online database uses a MySQL database, the mapping between them is relatively easy. Flutter also provides a package for using SQLite called SQFlite , easing the process. Below is a prioritized list of the different features the customers would like to have available offline, along with what issues in the weekplanner deals with that piece of functionality. Obviously the first thing which needs to be implemented is the ability to log in offline.","title":"Current solution"},{"location":"Legacy/Handover/2020E/Issues/offline_mode/#here-is-the-prioritized-list-for-the-offline-features","text":"Citizen features: View weekplan. ( Issue 414 ) View activity. ( Issue 414 ) Mark activity as completed. ( Issue 400 ) Timer functionality. ( Issue 402 ) Guardian features: View weekplan. ( Issue 415 ) View activity. ( Issue 415 ) Cancel activity. ( Issue 407 ) Timer functionality. ( Issue 406 ) Edit weekplan. ( Issue 407 ) Take picture as pictogram. ( Issue 632 ) Create/delete weekplans. ( Issue 410 ) It is to be noted, that most of these issues specify that they would like the functionality to be available for the current week. With the current implementation of the SQLite database, showing more than one week should not be a problem however. Likewise, adding, editing and deleting weekplans are implemented in the offline database, complete with keeping track of what methods needs to be called again when online. What is lacking in the implementation is simply when this functionality needs to be used.","title":"Here is the prioritized list for the offline features"},{"location":"Legacy/Handover/2020E/Issues/offline_mode/#issues-considerations","text":"As part of the design, it is necessary to find a way to handle how and which users are able to login to the offline database. This could possibly be achieved by making it so that only the most recently logged in user is able to use the device online. For the pictograms you would also have to figure out some system for determining which pictograms should be saved locally, and how to save them. They are currently saved to a folder called \"pictograms\" on the device's local storage. Saving every single pictogram might become problematic. Multiple strategies for which pictograms should be saved can be implemented. It is possible to simply only save pictograms which have been used on any screen since the last time a user logged in. This might however result in some images not being available if they are suddenly needed after going offline, even though they are connected to the user. A possibility could also be allowing a guardian to choose which pictograms should be saved for the specific citizen. Another option would be to simply save x amount of most used pictograms to make it as likely as possible that what you would need is available. One possible way of storing these pictograms is using the class ImageCache from Flutter. Currently a self made cache is used in the BLoC used for loading images for pictograms. The implementation of this cache can be found in the pictogram_image_bloc . Another way this could be handled, is using the functionality which is already implemented in the offline database, to save images in a folder onthe device, keeping it available for use even after the app is shut down. This will require some kind of further management of the saved images, as this will result in a lot of images being saved on the device over time.","title":"Issues / considerations"},{"location":"Legacy/Handover/2020E/Issues/offline_mode/#syncing-the-local-database-to-match-the-online-database","text":"A number of different issues could also appear in relation to synchronization of different instances of data. Cache invalidation scenario: Citizen 1 logs in on their device and downloads their weekplans from the server. Guardian logs in on another device and changes Citizen 1's weekplan for a week. Citizen 1 looks at this weekplan on their device, but this is not the updated version since there is a version in the cache/offline database. One possible solution is to use a timestamp to check if there is changes in the online version. This timestamp would be downloaded and checked whenever a weekplan is opened on a device with an internet connection. This timestamp is then compared with the local version. Another solution would be to automatically check for changes every 30 seconds or something similar to avoid having to reopen weekplans to update them. A guardian could also have a \"Refresh\" button for the citizens, that would download the new changes. Time stamps could also solve the update conflicts since it is possible to compare two versions and save the newest. Currently with the offline database, the idea is to simply update the offline database, each time a call to the offline database. It might be a good idea to check at other times too however. Synchronizing offline changes scenario: Citizen's tablet is offline. A guardian logs in on the same (offline) tablet and changes the citizen's settings. The citizen logs in and can see the local updates. The citizen's device gets internet connection and now the changes has to be synced with the database. But the guardian is not logged in anymore and the citizen does not have permission to update their changes through the web-api. A possible solution would be to give a citizen permissions in the wep-api to make changes. This could be implemented with some kind of token or similar signifying if the changes is coming from the offline database, and then only allowing citizens to make changes if it comes from their offline database. The system should automatically perform the offline changes, and the citizens themselves should not be allowed to directly perform these changes. A potentially very large issue is if offline changes for the same data, e.g an activity is made on two different devices - which of the changes should be saved in the online database, when they both come online. The PO-group has talked to the customers and they want \"last write wins\". To accommodate this, it might be necessary to add more attributes in the offline and online databases in order to deal with of the synchronization. Examples could be \u201clast_updated_on\u201d, \u201ccreated_on\u201d, \u201cdeleted_on\u201d, \u201cedited_offline\u201d which are timestamps used to see if data should be synched or not and the \u201cedited_offline\u201d could be a boolean. It is also an option to use UUID with/instead of timestamps to make the synchronization have a unique id.","title":"Syncing the local database to match the online database"},{"location":"Legacy/Handover/2020E/Issues/offline_mode/#what-currently-exists","text":"Currently an initial class for communication with an offline database exists in the api_client repository. The api_client maps JSON output from the web-api into models which the weekplanner uses for displaying data models. Thus, it would be essential to implement the offline repository feature in the api_client. Every model in the api_client implements an abstract class called Model which provides a from_json() and to_json() method for the models to interact with the web-api, as well as a from_database() to convert data from the database into a model.","title":"What currently exists"},{"location":"Legacy/Handover/2020E/Issues/offline_mode/#the-1-1-approach","text":"The solution chosen for the database design is a 1-1 relational database with the web-api. For this approach the online database scheme was cloned to a sqlite scheme creation file. From this file, most of the sql was copied into the dbhandler in the createTables method. Some of the tables and rows were not imported though, as they were either not used, or unnecessary for the app to be able to run offline. If more tables or columns are needed they can simply be added to the table creation function. If something is to be changed in the model layer, this will most likely be your workload: Alter the modellayer in the web-api Migrate the database Customize the unittests Customize the integration tests Alter the models in the api_client Customize the unittests Integration test between web_api and api_client Alter the unittests Alter the weekplanner to use the new feature Integration test between weekplanner and the offline repository Whenever a major change is made to the online database, it is important to also remember to change the offline database to match it, such that everything is saved correctly.","title":"The 1-1 approach"},{"location":"Legacy/Handover/2020E/Issues/offline_mode/#the-dbhandler","text":"In terms of implementation, there currently is a class called dbhandler . This class is created as a static object within itself, such that there is only ever one instance of it, and it does not need to compete for access to the SQLite database. It contains a function for each of the calls in the different API's. What we envisioned is that when one of these are called, it should try and get the data from offline first, and then if they do not find a result in offline, the APIs should make an HTTP request instead. The methods which creates new objects, needs to always make the HTTP POST requests, as we need to get an ID from the online database as soon as possible. Until online responds with the object with an updated ID, we generate an UUID for all objects when they are created, in an attempt to ensure that all objects have a unique ID. If a mutating transaction to the online database fails, it should be saved in the database with the function saveFailedTransactions() in the dbHandler. It would also be necessary to find a good time to call retryFailedTransactions() , which tries to send all the mutating transactions to the database again.","title":"The dbHandler"},{"location":"Legacy/Handover/2020E/Issues/offline_mode/#the-apis","text":"All of the Api's should first call the offline database. If nothing was found there, then call the online and use the what is returned from online to hydrate the offline database. For this to work, all methods in the api's need to be made async* which means that it is a stream. In order to use these there is a few important things to notice: Streams use yield instead of return When the function yields something, the it continues to run The unittests for the functions are not created with async in mind, so they need to be changed","title":"The Api's"},{"location":"Legacy/Handover/2020E/Issues/offline_mode/#unit-tests","text":"Most of the functions in the dbhandler has been tested in the OfflineDatabase_test file. Some are still missing though, and they need to be created before the offline database can be merged with develop. The database tests relies on SQFlite ffi which is a library that overwrites the factory in sqflite, to allow the database to work on windows, linux and MacOs. The unit tests which currently tests the APIs only works if the APIs are async. This can for the most part be fixed by delaying the expect. An example of this from account_api_test.dart is: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 test ( 'Should call login endpoint' , () async { accountApi . login ( 'username' , 'password' ) . listen ( expectAsync1 (( bool success ) { expect ( success , isTrue ); })); httpMock . expectOne ( url: '/login' , method: Method . post ) . flush ( < String , dynamic > { 'data' : 'TestToken' , 'message' : '' , 'errorKey' : 'NoError' , }); }); In this example a login request is made in the account API. The mocked httpclient inside the accountApi then stores that request in a list in the httpclient. Then the expectone on the httpMock checks if there is exactly one post method to login inside it. This was fine when the API was not async, but even though it is made async, we still cant use await to ensure that the values are in the httpMock before we expect. The solution we came up with seems a bit hacky, but it was the best solution we could come up with. Because of the way Flutter does async, we created a very short delay, to allow the http request to be set into the httpMock before checking it. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 test ( 'Should call login endpoint' , () async { accountApi . login ( 'username' , 'password' ) . listen ( expectAsync1 (( bool success ) { expect ( success , isTrue ); })); Future < void > . delayed ( const Duration ( milliseconds: 10 )). then (( _ ) => httpMock . expectOne ( url: '/login' , method: Method . post ) . flush ( < String , dynamic > { 'data' : 'TestToken' , 'message' : '' , 'errorKey' : 'NoError' , })); });","title":"Unit tests"},{"location":"Legacy/Handover/2020E/Role_Structure/","text":"Overview \u00b6 Below are the roles that have been used in the 2020E semester: PO Process Development Team","title":"Overview"},{"location":"Legacy/Handover/2020E/Role_Structure/#overview","text":"Below are the roles that have been used in the 2020E semester: PO Process Development Team","title":"Overview"},{"location":"Legacy/Handover/2020E/Role_Structure/development_team/","text":"Development Team \u00b6 A Development team in GIRAF corresponds directly to a Development team in Scrum. Definition \u00b6 A group is defined as a Development team if they are neither PO or Process. Responsibility \u00b6 The following things are the responsibility of the members within a Development team. Work on issues Create new issues Review pull requests (This should always be the main priority)","title":"Development Team"},{"location":"Legacy/Handover/2020E/Role_Structure/development_team/#development-team","text":"A Development team in GIRAF corresponds directly to a Development team in Scrum.","title":"Development Team"},{"location":"Legacy/Handover/2020E/Role_Structure/development_team/#definition","text":"A group is defined as a Development team if they are neither PO or Process.","title":"Definition"},{"location":"Legacy/Handover/2020E/Role_Structure/development_team/#responsibility","text":"The following things are the responsibility of the members within a Development team. Work on issues Create new issues Review pull requests (This should always be the main priority)","title":"Responsibility"},{"location":"Legacy/Handover/2020E/Role_Structure/po/","text":"PO \u00b6 PO in GIRAF corresponds directly to the PO in Scrum. Definition \u00b6 The PO group consists of a single group, which has contact to the stakeholders. Responsibility \u00b6 The following things are the responsibility of the members within the PO group: Contact with stakeholders Maintain Product Backlog Create new issues Create prototypes if an issue needs one Create and conduct usability tests Maintain design guidelines Decide together with the Process group, which issues that should be in the Sprint Backlog Add release descriptions to the Wiki as described in Release Guide Can work on issues if necessary","title":"PO"},{"location":"Legacy/Handover/2020E/Role_Structure/po/#po","text":"PO in GIRAF corresponds directly to the PO in Scrum.","title":"PO"},{"location":"Legacy/Handover/2020E/Role_Structure/po/#definition","text":"The PO group consists of a single group, which has contact to the stakeholders.","title":"Definition"},{"location":"Legacy/Handover/2020E/Role_Structure/po/#responsibility","text":"The following things are the responsibility of the members within the PO group: Contact with stakeholders Maintain Product Backlog Create new issues Create prototypes if an issue needs one Create and conduct usability tests Maintain design guidelines Decide together with the Process group, which issues that should be in the Sprint Backlog Add release descriptions to the Wiki as described in Release Guide Can work on issues if necessary","title":"Responsibility"},{"location":"Legacy/Handover/2020E/Role_Structure/process/","text":"Process \u00b6 In the 2020E semester, there has been used a term called Process group. It corresponds to the Scrum master in Scrum. Definition \u00b6 The Process group consists of a single group, which administers the process of the GIRAF team but not within the individual groups. Responsibility \u00b6 The following things are the responsibility of the members in the Process group. Maintain the process in the GIRAF team Structure of sprint of events Plan meetings within the GIRAF team (E.g. location and time for the sprint events) Moderator in the sprint events Distribute issues from GitHub Add reviewers to pull requests Decide together with PO, which issues that should be in the Sprint Backlog Can work on issues if necessary","title":"Process"},{"location":"Legacy/Handover/2020E/Role_Structure/process/#process","text":"In the 2020E semester, there has been used a term called Process group. It corresponds to the Scrum master in Scrum.","title":"Process"},{"location":"Legacy/Handover/2020E/Role_Structure/process/#definition","text":"The Process group consists of a single group, which administers the process of the GIRAF team but not within the individual groups.","title":"Definition"},{"location":"Legacy/Handover/2020E/Role_Structure/process/#responsibility","text":"The following things are the responsibility of the members in the Process group. Maintain the process in the GIRAF team Structure of sprint of events Plan meetings within the GIRAF team (E.g. location and time for the sprint events) Moderator in the sprint events Distribute issues from GitHub Add reviewers to pull requests Decide together with PO, which issues that should be in the Sprint Backlog Can work on issues if necessary","title":"Responsibility"},{"location":"Legacy/Handover/2020E/Sprint_Events/","text":"Overview \u00b6 This section gives an explanation of the sprint events that has been used in the 2020E semester. Timeline \u00b6 Sprint Events \u00b6 Sprint Planning Development Phase Cross Stand-up Meetings Sprint Review Sprint Retrospective Release Preparation","title":"Overview"},{"location":"Legacy/Handover/2020E/Sprint_Events/#overview","text":"This section gives an explanation of the sprint events that has been used in the 2020E semester.","title":"Overview"},{"location":"Legacy/Handover/2020E/Sprint_Events/#timeline","text":"","title":"Timeline"},{"location":"Legacy/Handover/2020E/Sprint_Events/#sprint-events","text":"Sprint Planning Development Phase Cross Stand-up Meetings Sprint Review Sprint Retrospective Release Preparation","title":"Sprint Events"},{"location":"Legacy/Handover/2020E/Sprint_Events/cross_stand_up_meetings/","text":"Cross Stand-up Meetings \u00b6 This event is held twice during the Development Phase. Each group in the GIRAF team should be represented by a single member except the Process group which is represented by two members. Expected Duration \u00b6 Max 15 minutes. Roles \u00b6 One Process group member acts as moderator. One Process group takes makes a summary of the meeting. Agenda \u00b6 Each attendant goes through the following: Status of the work in their group. Are there things that are stopping the progress. After the Cross Stand-up Meeting \u00b6 Each group in the GIRAF team, goes through the information given to the meeting. This is first and foremost to see if anything can be improved within a group in relation to the process in a sprint e.g. missing reviews on pull requests.","title":"Cross Stand-up Meetings"},{"location":"Legacy/Handover/2020E/Sprint_Events/cross_stand_up_meetings/#cross-stand-up-meetings","text":"This event is held twice during the Development Phase. Each group in the GIRAF team should be represented by a single member except the Process group which is represented by two members.","title":"Cross Stand-up Meetings"},{"location":"Legacy/Handover/2020E/Sprint_Events/cross_stand_up_meetings/#expected-duration","text":"Max 15 minutes.","title":"Expected Duration"},{"location":"Legacy/Handover/2020E/Sprint_Events/cross_stand_up_meetings/#roles","text":"One Process group member acts as moderator. One Process group takes makes a summary of the meeting.","title":"Roles"},{"location":"Legacy/Handover/2020E/Sprint_Events/cross_stand_up_meetings/#agenda","text":"Each attendant goes through the following: Status of the work in their group. Are there things that are stopping the progress.","title":"Agenda"},{"location":"Legacy/Handover/2020E/Sprint_Events/cross_stand_up_meetings/#after-the-cross-stand-up-meeting","text":"Each group in the GIRAF team, goes through the information given to the meeting. This is first and foremost to see if anything can be improved within a group in relation to the process in a sprint e.g. missing reviews on pull requests.","title":"After the Cross Stand-up Meeting"},{"location":"Legacy/Handover/2020E/Sprint_Events/development_phase/","text":"Development Phase \u00b6 The Development Phase starts after the Sprint Planning and ends on the day of the Sprint Review. Work with Issues \u00b6 In the Development Phase, the Development teams work with the issues they have been given by the Process group after the Sprint Planning. Need More Issues to Work On \u00b6 If you have time to work on a additional issues, than the ones that were distributed after Sprint Planning, you can get a new one by following these steps: Find an issue you want to work on in the Sprint Backlog e.g. 2020E 3. Sprint Ask the Process group if you can work on that issue The Process group might say no for various reasons and they have the final say, as they have a better overview. There is usually a greater chance of getting a yes, if the issue you have picked is either highest or high priority. If you don't have a preferred issue you can ask the PO group to be assigned the most pressing issue, as they have a good overview of the project and they will most likely have some issues that they would like you to work with. If a Development team need more issues to work on, they have to contact the Process group. Code Review \u00b6 As a developer in the GIRAF team you will have to review code made by other developers. This should always be the main priority to further the development process. GitHub \u00b6 The primary tool for the Development Phase is GitHub. The usage of GitHub in GIRAF is described here .","title":"Development Phase"},{"location":"Legacy/Handover/2020E/Sprint_Events/development_phase/#development-phase","text":"The Development Phase starts after the Sprint Planning and ends on the day of the Sprint Review.","title":"Development Phase"},{"location":"Legacy/Handover/2020E/Sprint_Events/development_phase/#work-with-issues","text":"In the Development Phase, the Development teams work with the issues they have been given by the Process group after the Sprint Planning.","title":"Work with Issues"},{"location":"Legacy/Handover/2020E/Sprint_Events/development_phase/#need-more-issues-to-work-on","text":"If you have time to work on a additional issues, than the ones that were distributed after Sprint Planning, you can get a new one by following these steps: Find an issue you want to work on in the Sprint Backlog e.g. 2020E 3. Sprint Ask the Process group if you can work on that issue The Process group might say no for various reasons and they have the final say, as they have a better overview. There is usually a greater chance of getting a yes, if the issue you have picked is either highest or high priority. If you don't have a preferred issue you can ask the PO group to be assigned the most pressing issue, as they have a good overview of the project and they will most likely have some issues that they would like you to work with. If a Development team need more issues to work on, they have to contact the Process group.","title":"Need More Issues to Work On"},{"location":"Legacy/Handover/2020E/Sprint_Events/development_phase/#code-review","text":"As a developer in the GIRAF team you will have to review code made by other developers. This should always be the main priority to further the development process.","title":"Code Review"},{"location":"Legacy/Handover/2020E/Sprint_Events/development_phase/#github","text":"The primary tool for the Development Phase is GitHub. The usage of GitHub in GIRAF is described here .","title":"GitHub"},{"location":"Legacy/Handover/2020E/Sprint_Events/release_preparation/","text":"Release Preparation \u00b6 If a release is made a Release Preparation event should be held at the end of the sprint, before the release. Expected Duration \u00b6 2 days. Purpose \u00b6 To find system breaking bugs and then fixing them before making a release. Before Release Preparation \u00b6 The Process group will set up a release branch for each of the repositories. Agenda \u00b6 The release branches are tested and fixed continuously in all their respective repositories. All the Development teams will test all the time and when they find a bug, they must create an issue on github. Testing \u00b6 The whole system should be tested by the development teams, with the intention of finding bugs or deficiencies in the code base. If a bug or a missing functionality is discovered: If it is a system breaking bug: Create an issue in the relevant repository, using the ReleaseFix label. If it is a bug that does not hinder release, a missing functionality, or badly written code etc.: Create an issue in the relevant repository, using the relevant labels. The GitHub bot posts a notification in the #issues channel on slack whenever a new issue is created. If the issue is labelled ReleaseFix, the Process group will assign the issue to a Development team. Fixing Release Bugs \u00b6 You get ReleaseFix issues from the Process group. Then you must make a new branch named releasefix/#IssueNumber, branched out from the release branch. After fixing the issue in said branch, you should make a pull request for it. After Release Preparation \u00b6 If all the releases are ready at the end of the Release Preparation, then the publish state will begin. This is done by merging the release branches into the master branches. Afterwards, you need to do the following things in these repositories: weekplanner , the app is automatically published onto the Google Play Store, but not onto the Apple App Store. Here, you need to log onto App Store Connect and create a new publish where you select the new build that has been created. web-api , a docker image is automatically created. However, the docker containers are only restarted every second week. This means that the docker container has to be restarted manually in order to get the server to use the newly released web-api immediately. It is done by logging onto Portainer (Credentials can be found here ) where you need to restart all the containers that have a name that starts with Giraf_API_PROD .","title":"Release Preparation"},{"location":"Legacy/Handover/2020E/Sprint_Events/release_preparation/#release-preparation","text":"If a release is made a Release Preparation event should be held at the end of the sprint, before the release.","title":"Release Preparation"},{"location":"Legacy/Handover/2020E/Sprint_Events/release_preparation/#expected-duration","text":"2 days.","title":"Expected Duration"},{"location":"Legacy/Handover/2020E/Sprint_Events/release_preparation/#purpose","text":"To find system breaking bugs and then fixing them before making a release.","title":"Purpose"},{"location":"Legacy/Handover/2020E/Sprint_Events/release_preparation/#before-release-preparation","text":"The Process group will set up a release branch for each of the repositories.","title":"Before Release Preparation"},{"location":"Legacy/Handover/2020E/Sprint_Events/release_preparation/#agenda","text":"The release branches are tested and fixed continuously in all their respective repositories. All the Development teams will test all the time and when they find a bug, they must create an issue on github.","title":"Agenda"},{"location":"Legacy/Handover/2020E/Sprint_Events/release_preparation/#testing","text":"The whole system should be tested by the development teams, with the intention of finding bugs or deficiencies in the code base. If a bug or a missing functionality is discovered: If it is a system breaking bug: Create an issue in the relevant repository, using the ReleaseFix label. If it is a bug that does not hinder release, a missing functionality, or badly written code etc.: Create an issue in the relevant repository, using the relevant labels. The GitHub bot posts a notification in the #issues channel on slack whenever a new issue is created. If the issue is labelled ReleaseFix, the Process group will assign the issue to a Development team.","title":"Testing"},{"location":"Legacy/Handover/2020E/Sprint_Events/release_preparation/#fixing-release-bugs","text":"You get ReleaseFix issues from the Process group. Then you must make a new branch named releasefix/#IssueNumber, branched out from the release branch. After fixing the issue in said branch, you should make a pull request for it.","title":"Fixing Release Bugs"},{"location":"Legacy/Handover/2020E/Sprint_Events/release_preparation/#after-release-preparation","text":"If all the releases are ready at the end of the Release Preparation, then the publish state will begin. This is done by merging the release branches into the master branches. Afterwards, you need to do the following things in these repositories: weekplanner , the app is automatically published onto the Google Play Store, but not onto the Apple App Store. Here, you need to log onto App Store Connect and create a new publish where you select the new build that has been created. web-api , a docker image is automatically created. However, the docker containers are only restarted every second week. This means that the docker container has to be restarted manually in order to get the server to use the newly released web-api immediately. It is done by logging onto Portainer (Credentials can be found here ) where you need to restart all the containers that have a name that starts with Giraf_API_PROD .","title":"After Release Preparation"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_planning/","text":"Sprint Planning \u00b6 Everyone on the GIRAF Scrum team (PO, Process and Development teams) attend this event that marks the start of a sprint. Expected Duration \u00b6 Max 4 hours (Adjusted for 2 week sprint). Result \u00b6 A prioritized list of issues for each Development team to work on during the sprint. Before the Sprint Planning \u00b6 Prior to Sprint Planning the Process group and the PO group have decided which issues to include in the Sprint Backlog. Only those issues will be time estimated and prioritized during Sprint Planning. Minor adjustments may be made afterwards based on the outcome of the Sprint Planning. Next, everyone is divided into Sprint Planning Cross-groups. Each Sprint Planning Cross-group has at least one Process group member and one PO group member. This means that the number of Sprint Planning Cross-groups cannot exceed the number of members in the Process or PO group. The Sprint Planning Cross-groups are created randomly for each sprint. The Sprint Planning Cross-groups are send out to the whole GIRAF team prior to the Sprint Planning. After the Sprint Planning Cross-groups are made, the issues in the Sprint Backlog should be divided evenly between the groups. This process has been automated through the Sprint Planning Tool . Agenda \u00b6 Process group member presents the following: Changes made to the process, since the previous sprint, based on the information from Sprint Retrospective. PO group member presents the following: Semester goal The overall goal of the project e.g. has the goal changed since last Sprint Planning. Sprint goal The goal of this sprint. New and/or important issues in the sprint Time estimation begins. Time Estimation \u00b6 Time estimation is done through Planning Poker . Remember to include testing, review, documentation and usability test design in your estimations. Everyone has ten cards with these numbers: 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144 (the fibonacci numbers). 5 is equal to an 8-hour work day. The purpose of this is to not think of the numbers as hours but as relative to one day's workload. The Process group member has a list of issues the group should time estimate. Each issue is only estimated by one group. For every issue follow these steps: Make sure everyone understands a given issue Everyone reads the issue Discuss the issue If anyone is in doubt about the nature of an issue ask the PO member. Guess the time required to complete an issue. Everyone picks a card and holds the value secret. Everyone shows their cards simultaneously. If the cards are more than two steps apart, then the ones with highest and lowest numbers present their point of view. By more than two steps means that e.g 3 and 8 are two steps apart. 3 and 5 are not . Repeat steps 2.a-2.c up to 3 times or until an agreement is reached. These scenarios are seen as agreements: If everyone has estimated the issue to the same number. This number is noted. If there is one between the highest and lowest estimate. The highest number is noted. The following is done, if there is no agreement after 3 times : The median or the number closest to the median is noted e.g. with the numbers 3, 3, 5, 8 the median is 4 but 5 is noted since that is the closest fibonacci number. After Sprint Planning \u00b6 When all Sprint Planning Cross-groups have finished time estimation each Development team gets back together to prioritize the issues. The prioritization are made according to what the Development team would like to work with. All issues in the sprint should prioritized as either: High (Want the most) Medium Low (Want the least) This list should be sent to the Process group by the end of Sprint Planning. The Process group will then assign the Development teams to certain issues. The assignment will use the following constraints: As many high priority issues as possible. As few low priority issues as possible. Equal workload between Development Teams. The issue assignments will be sent out at most 24 hours later along with a list of which types of issues to solve first e.g. bug fixes before features.","title":"Sprint Planning"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_planning/#sprint-planning","text":"Everyone on the GIRAF Scrum team (PO, Process and Development teams) attend this event that marks the start of a sprint.","title":"Sprint Planning"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_planning/#expected-duration","text":"Max 4 hours (Adjusted for 2 week sprint).","title":"Expected Duration"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_planning/#result","text":"A prioritized list of issues for each Development team to work on during the sprint.","title":"Result"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_planning/#before-the-sprint-planning","text":"Prior to Sprint Planning the Process group and the PO group have decided which issues to include in the Sprint Backlog. Only those issues will be time estimated and prioritized during Sprint Planning. Minor adjustments may be made afterwards based on the outcome of the Sprint Planning. Next, everyone is divided into Sprint Planning Cross-groups. Each Sprint Planning Cross-group has at least one Process group member and one PO group member. This means that the number of Sprint Planning Cross-groups cannot exceed the number of members in the Process or PO group. The Sprint Planning Cross-groups are created randomly for each sprint. The Sprint Planning Cross-groups are send out to the whole GIRAF team prior to the Sprint Planning. After the Sprint Planning Cross-groups are made, the issues in the Sprint Backlog should be divided evenly between the groups. This process has been automated through the Sprint Planning Tool .","title":"Before the Sprint Planning"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_planning/#agenda","text":"Process group member presents the following: Changes made to the process, since the previous sprint, based on the information from Sprint Retrospective. PO group member presents the following: Semester goal The overall goal of the project e.g. has the goal changed since last Sprint Planning. Sprint goal The goal of this sprint. New and/or important issues in the sprint Time estimation begins.","title":"Agenda"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_planning/#time-estimation","text":"Time estimation is done through Planning Poker . Remember to include testing, review, documentation and usability test design in your estimations. Everyone has ten cards with these numbers: 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144 (the fibonacci numbers). 5 is equal to an 8-hour work day. The purpose of this is to not think of the numbers as hours but as relative to one day's workload. The Process group member has a list of issues the group should time estimate. Each issue is only estimated by one group. For every issue follow these steps: Make sure everyone understands a given issue Everyone reads the issue Discuss the issue If anyone is in doubt about the nature of an issue ask the PO member. Guess the time required to complete an issue. Everyone picks a card and holds the value secret. Everyone shows their cards simultaneously. If the cards are more than two steps apart, then the ones with highest and lowest numbers present their point of view. By more than two steps means that e.g 3 and 8 are two steps apart. 3 and 5 are not . Repeat steps 2.a-2.c up to 3 times or until an agreement is reached. These scenarios are seen as agreements: If everyone has estimated the issue to the same number. This number is noted. If there is one between the highest and lowest estimate. The highest number is noted. The following is done, if there is no agreement after 3 times : The median or the number closest to the median is noted e.g. with the numbers 3, 3, 5, 8 the median is 4 but 5 is noted since that is the closest fibonacci number.","title":"Time Estimation"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_planning/#after-sprint-planning","text":"When all Sprint Planning Cross-groups have finished time estimation each Development team gets back together to prioritize the issues. The prioritization are made according to what the Development team would like to work with. All issues in the sprint should prioritized as either: High (Want the most) Medium Low (Want the least) This list should be sent to the Process group by the end of Sprint Planning. The Process group will then assign the Development teams to certain issues. The assignment will use the following constraints: As many high priority issues as possible. As few low priority issues as possible. Equal workload between Development Teams. The issue assignments will be sent out at most 24 hours later along with a list of which types of issues to solve first e.g. bug fixes before features.","title":"After Sprint Planning"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_retrospective/","text":"Sprint Retrospective \u00b6 After Sprint Review, all the groups (PO and Development teams) will go into separate rooms. At least one Process group member will join each individual group and ask some predefined questions. Expected Duration \u00b6 Max 2 hours (Adjusted for 2 week sprint). Result \u00b6 Find potential problems with process used in GIRAF team e.g. problems with the review process of pull requests. Before Sprint Retrospective \u00b6 Process group prepares questions to ask the other groups in the GIRAF team. Roles \u00b6 A Process group member acts as the moderator and writes down the answers to the questions. Agenda \u00b6 Process group member asks the predefined questions. The Sprint Retrospective is done when all questions has been answered. After Sprint Retrospective \u00b6 The Process group goes through all the answers to the questions to see if there is something that can be improved with process e.g. structure of a sprint event.","title":"Sprint Retrospective"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_retrospective/#sprint-retrospective","text":"After Sprint Review, all the groups (PO and Development teams) will go into separate rooms. At least one Process group member will join each individual group and ask some predefined questions.","title":"Sprint Retrospective"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_retrospective/#expected-duration","text":"Max 2 hours (Adjusted for 2 week sprint).","title":"Expected Duration"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_retrospective/#result","text":"Find potential problems with process used in GIRAF team e.g. problems with the review process of pull requests.","title":"Result"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_retrospective/#before-sprint-retrospective","text":"Process group prepares questions to ask the other groups in the GIRAF team.","title":"Before Sprint Retrospective"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_retrospective/#roles","text":"A Process group member acts as the moderator and writes down the answers to the questions.","title":"Roles"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_retrospective/#agenda","text":"Process group member asks the predefined questions. The Sprint Retrospective is done when all questions has been answered.","title":"Agenda"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_retrospective/#after-sprint-retrospective","text":"The Process group goes through all the answers to the questions to see if there is something that can be improved with process e.g. structure of a sprint event.","title":"After Sprint Retrospective"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_review/","text":"Sprint Review \u00b6 At the end of each sprint, there will a meeting, where all teams meet in one room to discuss all issues from the finished sprint. Expected Duration \u00b6 Max 2 hours (Adjusted for 2 week sprint). Result \u00b6 All GIRAF team members get an overview of the issues that has been worked on during the sprint. Before the Sprint Review \u00b6 The Process group books a room for this meeting, and informs everyone about it through the Slack channel projekt-opdateringer and then makes the event on the Google Calendar. The PO group prepares the next sprint goal and an overview of the Product Backlog. Every Development team , checks their issues and prepares some notes which includes the following: For every issue, that is defined as closed: Describe the solution. Code can be shown to help explain the solution. If the issue was closed, without making any changes, explain the reason behind this. Describe the problems that were faced, and how it was solved. If a new feature is implemented, prepare to show it, if it makes sense. If an issue is in the review process at the time of the Sprint Review, it will be seen as done for the Sprint Review. For every open issue: Has there been worked on the issue Yes Describe the state of it and what there might be. No Describe why. Roles \u00b6 One member from the Process group will act as the moderator. This person ensures every Development team gets around all their issues. One member from the Process group will make a summary of the Sprint Review. Agenda \u00b6 The moderator picks a Development team. When a Development team is picked, they go to the front of the other teams and talk about the prepared notes, one issue at a time. The Development team makes it clear which issues they are talking and type of issue it is and its status. When a Development team explains their issues, everyone else may raise their hand to ask a question about the issue. The moderator picks the next Development team to go through the issues they worked on. At the end of the meeting, the PO group will present the next sprint goal and answer questions about the Product Backlog from the Development teams.","title":"Sprint Review"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_review/#sprint-review","text":"At the end of each sprint, there will a meeting, where all teams meet in one room to discuss all issues from the finished sprint.","title":"Sprint Review"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_review/#expected-duration","text":"Max 2 hours (Adjusted for 2 week sprint).","title":"Expected Duration"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_review/#result","text":"All GIRAF team members get an overview of the issues that has been worked on during the sprint.","title":"Result"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_review/#before-the-sprint-review","text":"The Process group books a room for this meeting, and informs everyone about it through the Slack channel projekt-opdateringer and then makes the event on the Google Calendar. The PO group prepares the next sprint goal and an overview of the Product Backlog. Every Development team , checks their issues and prepares some notes which includes the following: For every issue, that is defined as closed: Describe the solution. Code can be shown to help explain the solution. If the issue was closed, without making any changes, explain the reason behind this. Describe the problems that were faced, and how it was solved. If a new feature is implemented, prepare to show it, if it makes sense. If an issue is in the review process at the time of the Sprint Review, it will be seen as done for the Sprint Review. For every open issue: Has there been worked on the issue Yes Describe the state of it and what there might be. No Describe why.","title":"Before the Sprint Review"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_review/#roles","text":"One member from the Process group will act as the moderator. This person ensures every Development team gets around all their issues. One member from the Process group will make a summary of the Sprint Review.","title":"Roles"},{"location":"Legacy/Handover/2020E/Sprint_Events/sprint_review/#agenda","text":"The moderator picks a Development team. When a Development team is picked, they go to the front of the other teams and talk about the prepared notes, one issue at a time. The Development team makes it clear which issues they are talking and type of issue it is and its status. When a Development team explains their issues, everyone else may raise their hand to ask a question about the issue. The moderator picks the next Development team to go through the issues they worked on. At the end of the meeting, the PO group will present the next sprint goal and answer questions about the Product Backlog from the Development teams.","title":"Agenda"},{"location":"Legacy/Handover/2020E/Tools/","text":"Overview \u00b6 This section describes the tools used during the semester of 2020E. An overview of the different tools can be seen below: Tools used by the Process group Tools used by the PO group Google Calendar Slack","title":"Overview"},{"location":"Legacy/Handover/2020E/Tools/#overview","text":"This section describes the tools used during the semester of 2020E. An overview of the different tools can be seen below: Tools used by the Process group Tools used by the PO group Google Calendar Slack","title":"Overview"},{"location":"Legacy/Handover/2020E/Tools/google_calendar/","text":"Google Calendar \u00b6 In order to keep everyone on the GIRAF team updated on when and where the different Sprint events took place there was made a shared Google Calendar for the whole GIRAF team. This calendar was used to track: Sprint events Supervisor meetings Courses The calendar was also connected to a channel on slack. On this channel there would be daily updates at 8:00 in the morning showing what happened that day.","title":"Google Calendar"},{"location":"Legacy/Handover/2020E/Tools/google_calendar/#google-calendar","text":"In order to keep everyone on the GIRAF team updated on when and where the different Sprint events took place there was made a shared Google Calendar for the whole GIRAF team. This calendar was used to track: Sprint events Supervisor meetings Courses The calendar was also connected to a channel on slack. On this channel there would be daily updates at 8:00 in the morning showing what happened that day.","title":"Google Calendar"},{"location":"Legacy/Handover/2020E/Tools/slack/","text":"Slack \u00b6 Slack was used as the main communication tool, as it allows for the creation of channels wherein each individual message can be threaded with more messages. The names and descriptions for each of the channels can be seen underneath. Channel name Description backend The channel is used for discussing and asking questions about anything backend related. filer_og_links The channels is used to share relevant files and links between the GIRAF team. frontend The channel is used for discussing and asking questions about anything frontend related. general The channel is used for anything that does not fit in any other channel. issues The channel is connected to GitHub and shows whenever an issue is created or closed in the different repositories. calender This channel is used to give a notification on upcoming events in the shared Google calender po This channel is used to ask questions to the PO group process This channel is used to ask questions to the process group project_opdateringer This channel is used for project updates and to remind everyone about upcoming events. review The channel is connected to GitHub and gives notifications whenever a pull request has been opened, closed, approved, and if someone has requested changes. server The channel is used for discussing and asking questions about anything server related. test The channel is used for discussing and asking questions about anything test related. wiki The channel is used for discussing and asking questions about anything wiki related.","title":"Slack"},{"location":"Legacy/Handover/2020E/Tools/slack/#slack","text":"Slack was used as the main communication tool, as it allows for the creation of channels wherein each individual message can be threaded with more messages. The names and descriptions for each of the channels can be seen underneath. Channel name Description backend The channel is used for discussing and asking questions about anything backend related. filer_og_links The channels is used to share relevant files and links between the GIRAF team. frontend The channel is used for discussing and asking questions about anything frontend related. general The channel is used for anything that does not fit in any other channel. issues The channel is connected to GitHub and shows whenever an issue is created or closed in the different repositories. calender This channel is used to give a notification on upcoming events in the shared Google calender po This channel is used to ask questions to the PO group process This channel is used to ask questions to the process group project_opdateringer This channel is used for project updates and to remind everyone about upcoming events. review The channel is connected to GitHub and gives notifications whenever a pull request has been opened, closed, approved, and if someone has requested changes. server The channel is used for discussing and asking questions about anything server related. test The channel is used for discussing and asking questions about anything test related. wiki The channel is used for discussing and asking questions about anything wiki related.","title":"Slack"},{"location":"Legacy/Handover/2020E/Tools/PO_Group/","text":"Overview \u00b6 This section describes the tools developed and used by the PO Group during the semester of 2020E. An overview of the different tools can be seen below: Release designer","title":"Overview"},{"location":"Legacy/Handover/2020E/Tools/PO_Group/#overview","text":"This section describes the tools developed and used by the PO Group during the semester of 2020E. An overview of the different tools can be seen below: Release designer","title":"Overview"},{"location":"Legacy/Handover/2020E/Tools/PO_Group/release_designer/","text":"Release designer \u00b6 To fetch the issues from GitHub which have been completed in a release, the Issue Table Generator was used to fetch the relevant issues and make them into a markdown or LaTeX table. A guide on how to use it can be seen on the GitHub repository.","title":"Release designer"},{"location":"Legacy/Handover/2020E/Tools/PO_Group/release_designer/#release-designer","text":"To fetch the issues from GitHub which have been completed in a release, the Issue Table Generator was used to fetch the relevant issues and make them into a markdown or LaTeX table. A guide on how to use it can be seen on the GitHub repository.","title":"Release designer"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/","text":"Overview \u00b6 This section describes the tools developed and used by the Process group during the semester of 2020E. An overview of the different tools can be seen below: Sprint planning tool Pull requests overview tool Individual pull request tool Issues overview tool Sprint points overview tool Sprint retrospective tool","title":"Overview"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/#overview","text":"This section describes the tools developed and used by the Process group during the semester of 2020E. An overview of the different tools can be seen below: Sprint planning tool Pull requests overview tool Individual pull request tool Issues overview tool Sprint points overview tool Sprint retrospective tool","title":"Overview"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/individual_pull_request_tool/","text":"Individual Pull Request Tool \u00b6 This tool consists of a Google Sheets file , and accompanying Google Scripts. The sheets involved are: Individual pull request Pull requests overview Reviewers Purpose \u00b6 This tool is used for assigning specific reviewers to a specific pull request. This might be necessary in cases where multiple issues are linked. Setup \u00b6 This tool requires that the Pull requests overview tool is in use, and has updated its accompanying sheet. Following this, it is possible to select any active pull request in the Individual pull request sheet. The Reviewers sheet must contain the GitHub account names of all members of the current GIRAF team. This data can be filled in with the Update button at the top of the sheet. Usage \u00b6 This tool is used by selecting the specific pull request and reviewers with the dropdowns in the Individual pull request sheet. They are then assigned as reviewers by pressing the Assign Reviewers button which has the assignReviewersToPullRequest script assigned to it, which is found in the IndividualPullRequest.gs file. The selection can then be cleared by pressing the Clear button which has the clearIndividualPullRequestSheet script assigned to it, which is found in the IndividualPullRequest.gs file.","title":"Individual Pull Request Tool"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/individual_pull_request_tool/#individual-pull-request-tool","text":"This tool consists of a Google Sheets file , and accompanying Google Scripts. The sheets involved are: Individual pull request Pull requests overview Reviewers","title":"Individual Pull Request Tool"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/individual_pull_request_tool/#purpose","text":"This tool is used for assigning specific reviewers to a specific pull request. This might be necessary in cases where multiple issues are linked.","title":"Purpose"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/individual_pull_request_tool/#setup","text":"This tool requires that the Pull requests overview tool is in use, and has updated its accompanying sheet. Following this, it is possible to select any active pull request in the Individual pull request sheet. The Reviewers sheet must contain the GitHub account names of all members of the current GIRAF team. This data can be filled in with the Update button at the top of the sheet.","title":"Setup"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/individual_pull_request_tool/#usage","text":"This tool is used by selecting the specific pull request and reviewers with the dropdowns in the Individual pull request sheet. They are then assigned as reviewers by pressing the Assign Reviewers button which has the assignReviewersToPullRequest script assigned to it, which is found in the IndividualPullRequest.gs file. The selection can then be cleared by pressing the Clear button which has the clearIndividualPullRequestSheet script assigned to it, which is found in the IndividualPullRequest.gs file.","title":"Usage"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/issues_overview_tool/","text":"Issues Overview Tool \u00b6 This tool consists of a Google Sheets file , and accompanying Google Scripts. The sheets involved are: Issues overview Milestones Purpose \u00b6 This tool is meant to give an overview of all issues in the current sprint. It finds the issues by looking for a specific milestone, and it includes both open and closed issues. Setup \u00b6 The tool needs to be setup with data about the current GIRAF team in order to function. The data is entered into some of the sheets, and the Google Scripts found at Tools>Script Editor in the menus bar. The Milestones sheet must contain any milestones used for issues by the GIRAF team on GitHub. These milestones are meant to represent the individual sprints. In the API.gs file found with the script editor, the SEMESTER_START_DATE needs to be updated to the start date of the current GIRAF teams semester. In the same file, the ACCESS_TOKEN needs to be updated with a GitHub access token from one of the team members. An access token can be generated here . Usage \u00b6 The tool is used with the Issues overview sheet. First, the milestone must be set with the dropdown. Then the sheet is updated with information about the isuses by pressing the Update button. This button has the getIssuesOverview script assigned, which can be found in the IssuesOverview.gs file.","title":"Issues Overview Tool"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/issues_overview_tool/#issues-overview-tool","text":"This tool consists of a Google Sheets file , and accompanying Google Scripts. The sheets involved are: Issues overview Milestones","title":"Issues Overview Tool"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/issues_overview_tool/#purpose","text":"This tool is meant to give an overview of all issues in the current sprint. It finds the issues by looking for a specific milestone, and it includes both open and closed issues.","title":"Purpose"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/issues_overview_tool/#setup","text":"The tool needs to be setup with data about the current GIRAF team in order to function. The data is entered into some of the sheets, and the Google Scripts found at Tools>Script Editor in the menus bar. The Milestones sheet must contain any milestones used for issues by the GIRAF team on GitHub. These milestones are meant to represent the individual sprints. In the API.gs file found with the script editor, the SEMESTER_START_DATE needs to be updated to the start date of the current GIRAF teams semester. In the same file, the ACCESS_TOKEN needs to be updated with a GitHub access token from one of the team members. An access token can be generated here .","title":"Setup"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/issues_overview_tool/#usage","text":"The tool is used with the Issues overview sheet. First, the milestone must be set with the dropdown. Then the sheet is updated with information about the isuses by pressing the Update button. This button has the getIssuesOverview script assigned, which can be found in the IssuesOverview.gs file.","title":"Usage"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/pull_requests_overview_tool/","text":"Pull Requests Overview Tool \u00b6 This tool consists of a Google Sheets file , and accompanying Google Scripts. The sheets involved are: Pull requests overview GitHub teams Reviewers Unavailable reviewers Purpose \u00b6 This tool is meant to give an overview of all active pull requests in the GIRAF GitHub repositories. It is also used to assign reviewers to pull requests, and to post the checklists found here alongside them. Setup \u00b6 The tool needs to be setup with data about the current GIRAF team in order to function. The data is entered into some of the sheets, and the Google Scripts found at Tools>Script Editor in the menus bar. The GitHub teams sheet must contain data about the current GIRAF team groups and their roles regarding process and PO. The Reviewers sheet must contain the GitHub account names of all members of the current GIRAF team. This data can be filled in with the Update button at the top of the sheet. The Unavailable reviewers sheet must contain the GitHub account names of any members who are currently unavailable. They will be exempt from being assigned as reviewers as long as they are listed here. In the API.gs file found with the script editor, the SEMESTER_START_DATE needs to be updated to the start date of the current GIRAF team's semester. In the same file, the ACCESS_TOKEN needs to be updated with a GitHub access token from one of the team members. An access token can be generated here . Usage \u00b6 The tool is used with the two buttons in the Pull request overview sheet. The Update button updates the overview with information about active pull requests. It has the getPullRequestsOverview script assigned to it, which is found in the PullRequestOverview.gs file. The Add Reviewers button adds reviewers to the pull requests that currently have no reviewers assigned. It has the addReviewersToPullRequests script assigned to it, which is found in the PullRequestOverview.gs file.","title":"Pull Requests Overview Tool"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/pull_requests_overview_tool/#pull-requests-overview-tool","text":"This tool consists of a Google Sheets file , and accompanying Google Scripts. The sheets involved are: Pull requests overview GitHub teams Reviewers Unavailable reviewers","title":"Pull Requests Overview Tool"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/pull_requests_overview_tool/#purpose","text":"This tool is meant to give an overview of all active pull requests in the GIRAF GitHub repositories. It is also used to assign reviewers to pull requests, and to post the checklists found here alongside them.","title":"Purpose"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/pull_requests_overview_tool/#setup","text":"The tool needs to be setup with data about the current GIRAF team in order to function. The data is entered into some of the sheets, and the Google Scripts found at Tools>Script Editor in the menus bar. The GitHub teams sheet must contain data about the current GIRAF team groups and their roles regarding process and PO. The Reviewers sheet must contain the GitHub account names of all members of the current GIRAF team. This data can be filled in with the Update button at the top of the sheet. The Unavailable reviewers sheet must contain the GitHub account names of any members who are currently unavailable. They will be exempt from being assigned as reviewers as long as they are listed here. In the API.gs file found with the script editor, the SEMESTER_START_DATE needs to be updated to the start date of the current GIRAF team's semester. In the same file, the ACCESS_TOKEN needs to be updated with a GitHub access token from one of the team members. An access token can be generated here .","title":"Setup"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/pull_requests_overview_tool/#usage","text":"The tool is used with the two buttons in the Pull request overview sheet. The Update button updates the overview with information about active pull requests. It has the getPullRequestsOverview script assigned to it, which is found in the PullRequestOverview.gs file. The Add Reviewers button adds reviewers to the pull requests that currently have no reviewers assigned. It has the addReviewersToPullRequests script assigned to it, which is found in the PullRequestOverview.gs file.","title":"Usage"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/sprint_planning_tool/","text":"Sprint Planning Tool \u00b6 This tool consists of a Google Sheets file , and accompanying Google Scripts. The sheets involved are: Sprint planning Issue overview Group tags Time estimations Milestones Purpose \u00b6 This tool is meant to assist the Process group during the Sprint Planning event. The tool generates Sprint planning cross groups, and assigns them the issues labeled with the current Sprint milestone and time estimation needed . In the Issue overview sheet, time estimations of an issue can be set and which group is assigned each issue can be set. Setup \u00b6 The tool needs to be setup with data about the current GIRAF team in order to function. The Sprint planning sheet must contain the number of groups that should be made for the event, and which milestone is used for the current sprint. The Group tags sheet must be updated with the current GIRAF teams group tags. The Milestones sheet must be updated with the current set of milestones used in GitHub. The Google Script can be found at Tools>Script Editor in the menus bar. In the API.gs file found with the script editor, the ACCESS_TOKEN needs to be updated with a GitHub access token from one of the team members. An access token can be generated here . Usage \u00b6 As preparation to the Sprint Planning event the Generate button under the Sprint planning sheet should be pressed in order to generate Sprint planning cross groups. After an issue has been time estimated its time estimation can be set in the Issues overview . When all issues have been time estimated the Set time estimations button should be pressed, in order to set the corresponding time estimation tags on GitHub. The Issues overview sheet can also be used in order to assign the different issues to the Development teams on GitHub by pressing the Set group tags .","title":"Sprint Planning Tool"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/sprint_planning_tool/#sprint-planning-tool","text":"This tool consists of a Google Sheets file , and accompanying Google Scripts. The sheets involved are: Sprint planning Issue overview Group tags Time estimations Milestones","title":"Sprint Planning Tool"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/sprint_planning_tool/#purpose","text":"This tool is meant to assist the Process group during the Sprint Planning event. The tool generates Sprint planning cross groups, and assigns them the issues labeled with the current Sprint milestone and time estimation needed . In the Issue overview sheet, time estimations of an issue can be set and which group is assigned each issue can be set.","title":"Purpose"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/sprint_planning_tool/#setup","text":"The tool needs to be setup with data about the current GIRAF team in order to function. The Sprint planning sheet must contain the number of groups that should be made for the event, and which milestone is used for the current sprint. The Group tags sheet must be updated with the current GIRAF teams group tags. The Milestones sheet must be updated with the current set of milestones used in GitHub. The Google Script can be found at Tools>Script Editor in the menus bar. In the API.gs file found with the script editor, the ACCESS_TOKEN needs to be updated with a GitHub access token from one of the team members. An access token can be generated here .","title":"Setup"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/sprint_planning_tool/#usage","text":"As preparation to the Sprint Planning event the Generate button under the Sprint planning sheet should be pressed in order to generate Sprint planning cross groups. After an issue has been time estimated its time estimation can be set in the Issues overview . When all issues have been time estimated the Set time estimations button should be pressed, in order to set the corresponding time estimation tags on GitHub. The Issues overview sheet can also be used in order to assign the different issues to the Development teams on GitHub by pressing the Set group tags .","title":"Usage"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/sprint_points_overview_tool/","text":"Sprint Points Tool \u00b6 This tool consists of a Google Sheets file , and accompanying Google Scripts. The sheets involved are: Sprint points Milestones Purpose \u00b6 This tool is meant to give a quick overlook over how many story points have been completed during a sprint. The sprint is represented by a milestone, and start and end dates. Setup \u00b6 The tool needs to be setup with data about the current GIRAF team in order to function. The data is entered into some of the sheets, and the Google Scripts found at Tools>Script Editor in the menus bar. The Milestones sheet must contain any milestones used for issues by the GIRAF team on GitHub. These milestones are meant to represent the individual sprints. In the API.gs file found with the script editor, the SEMESTER_START_DATE needs to be updated to the start date of the current GIRAF teams semester. In the same file, the ACCESS_TOKEN needs to be updated with a GitHub access token from one of the team members. An access token can be generated here . Usage \u00b6 The tool is used with the Sprint points sheet, where the information about point completion is also shown. The milestone, start, and end dates are input, and then the Update button is pressed to fill in the sheet. The Update button has the getIssuesSprintConclusion function assigned, which can be found in the SprintPoints.gs file.","title":"Sprint Points Tool"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/sprint_points_overview_tool/#sprint-points-tool","text":"This tool consists of a Google Sheets file , and accompanying Google Scripts. The sheets involved are: Sprint points Milestones","title":"Sprint Points Tool"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/sprint_points_overview_tool/#purpose","text":"This tool is meant to give a quick overlook over how many story points have been completed during a sprint. The sprint is represented by a milestone, and start and end dates.","title":"Purpose"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/sprint_points_overview_tool/#setup","text":"The tool needs to be setup with data about the current GIRAF team in order to function. The data is entered into some of the sheets, and the Google Scripts found at Tools>Script Editor in the menus bar. The Milestones sheet must contain any milestones used for issues by the GIRAF team on GitHub. These milestones are meant to represent the individual sprints. In the API.gs file found with the script editor, the SEMESTER_START_DATE needs to be updated to the start date of the current GIRAF teams semester. In the same file, the ACCESS_TOKEN needs to be updated with a GitHub access token from one of the team members. An access token can be generated here .","title":"Setup"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/sprint_points_overview_tool/#usage","text":"The tool is used with the Sprint points sheet, where the information about point completion is also shown. The milestone, start, and end dates are input, and then the Update button is pressed to fill in the sheet. The Update button has the getIssuesSprintConclusion function assigned, which can be found in the SprintPoints.gs file.","title":"Usage"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/sprint_retrospective_tool/","text":"Sprint Retrospective Tool \u00b6 This tool consists of a Google Sheets file , and accompanying Google Scripts. The sheets involved are: Sprint retrospective Purpose \u00b6 This tool is meant to randomly distribute Process group members as interviewers for the other groups during sprint retrospective. Setup \u00b6 The tool needs to be setup with data about the current GIRAF team in order to function. The data is entered into some of the sheets, and the Google Scripts found at Tools>Script Editor in the menus bar. In the API.gs file found with the script editor, the SEMESTER_START_DATE needs to be updated to the start date of the current GIRAF teams semester. In the same file, the ACCESS_TOKEN needs to be updated with a GitHub access token from one of the team members. An access token can be generated here . Usage \u00b6 The tool is used with the Sprint retrospective sheet by pressing the Update button. This button has been assigned the dividePGMembersBetweenGroups script from the SprintRetrospective.gs file.","title":"Sprint Retrospective Tool"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/sprint_retrospective_tool/#sprint-retrospective-tool","text":"This tool consists of a Google Sheets file , and accompanying Google Scripts. The sheets involved are: Sprint retrospective","title":"Sprint Retrospective Tool"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/sprint_retrospective_tool/#purpose","text":"This tool is meant to randomly distribute Process group members as interviewers for the other groups during sprint retrospective.","title":"Purpose"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/sprint_retrospective_tool/#setup","text":"The tool needs to be setup with data about the current GIRAF team in order to function. The data is entered into some of the sheets, and the Google Scripts found at Tools>Script Editor in the menus bar. In the API.gs file found with the script editor, the SEMESTER_START_DATE needs to be updated to the start date of the current GIRAF teams semester. In the same file, the ACCESS_TOKEN needs to be updated with a GitHub access token from one of the team members. An access token can be generated here .","title":"Setup"},{"location":"Legacy/Handover/2020E/Tools/Process_Group/sprint_retrospective_tool/#usage","text":"The tool is used with the Sprint retrospective sheet by pressing the Update button. This button has been assigned the dividePGMembersBetweenGroups script from the SprintRetrospective.gs file.","title":"Usage"},{"location":"Legacy/Handover/2020F/","text":"Introduction \u00b6 The following will describe the process used in the 2020 GIRAF project. It contains: Advice : Advice to take into consideration before starting the project. Giraf Events : What events we hold, how and why. Git and Github : How to use GitHub correctly for GIRAF. Ownership Transfer : How to gain access to the different platforms used. We have tried to keep it as concise as possible. All the sections are relevant for everyone involved in the project, as every individual should be aware of the processes.","title":"Introduction"},{"location":"Legacy/Handover/2020F/#introduction","text":"The following will describe the process used in the 2020 GIRAF project. It contains: Advice : Advice to take into consideration before starting the project. Giraf Events : What events we hold, how and why. Git and Github : How to use GitHub correctly for GIRAF. Ownership Transfer : How to gain access to the different platforms used. We have tried to keep it as concise as possible. All the sections are relevant for everyone involved in the project, as every individual should be aware of the processes.","title":"Introduction"},{"location":"Legacy/Handover/2020F/advice/","text":"Advice for Future Process Groups \u00b6 This section will describe our views and experiences, and what we found out that worked. Previous advice can be found here . The Process in General \u00b6 The project is everyone's, the process and PO group does not have more authority than everyone else, just more responsibility. Take your time to understand the process and project and read the wiki. It is time well spent. Use the process exactly as we have described it on this wiki and then modify it to your liking. This is to avoid confusion about what to do. Use Gitflow, as described in the wiki. Be aware of the process changing and make an active decision about what to do. Use the tools provided on this wiki and improve them for yourselves and the next years. Use some time at the end of your project making it ready to hand off to the next year. Communication \u00b6 Use Discord or Slack as the communication medium. Use it to broadcast event start times and deadlines. Have a channel dedicated to announcements to make sure important messages does not get lost in the general chat. Events should be held at a physical or remote location with all developers present. We strongly recommend physical meetings. Start the meeting with the agenda. Remind people what the purpose is. Do this every time. Take short breaks between the different activities to keep the groups focused. If you do activities in smaller groups eg. Retrospective, remember to keep the groups small (<8) if you can. This prevents people from hiding behind others who speaks the entire time. Keep things transparent. Files should be public and so should the thoughts behind process changes. Respond to questions in timely manner. Do not be condescending or hostile in your answers. You need to repeat a lot of information because people will forget or be confused. Always be clear on what you say. Do not expect people to understand you implicitly. Discuss the responsibilities of the process and PO group with the other developers at the start of the project. Maybe try discussing the responsibilities before choosing who is process and PO group. Keeping an Overview \u00b6 Create a calendar available for all the groups in the project. Fill the calendar with all dates and times planned from the start. Decide where all your documents should be stored and be sure to be transparent with the other developer teams with the documents. Find an effective way to monitor and handle pull requests and reviews. We used Google Sheets and GitHub's API to automate it. Clean up branches after every sprint. Delete those that weren't deleted when merging and make sure every branch has a purpose. Decide if you want META-groups, which are groups focused on different aspects of the project such as front-end, back-end, server, and security.","title":"Advice for Future Process Groups"},{"location":"Legacy/Handover/2020F/advice/#advice-for-future-process-groups","text":"This section will describe our views and experiences, and what we found out that worked. Previous advice can be found here .","title":"Advice for Future Process Groups"},{"location":"Legacy/Handover/2020F/advice/#the-process-in-general","text":"The project is everyone's, the process and PO group does not have more authority than everyone else, just more responsibility. Take your time to understand the process and project and read the wiki. It is time well spent. Use the process exactly as we have described it on this wiki and then modify it to your liking. This is to avoid confusion about what to do. Use Gitflow, as described in the wiki. Be aware of the process changing and make an active decision about what to do. Use the tools provided on this wiki and improve them for yourselves and the next years. Use some time at the end of your project making it ready to hand off to the next year.","title":"The Process in General"},{"location":"Legacy/Handover/2020F/advice/#communication","text":"Use Discord or Slack as the communication medium. Use it to broadcast event start times and deadlines. Have a channel dedicated to announcements to make sure important messages does not get lost in the general chat. Events should be held at a physical or remote location with all developers present. We strongly recommend physical meetings. Start the meeting with the agenda. Remind people what the purpose is. Do this every time. Take short breaks between the different activities to keep the groups focused. If you do activities in smaller groups eg. Retrospective, remember to keep the groups small (<8) if you can. This prevents people from hiding behind others who speaks the entire time. Keep things transparent. Files should be public and so should the thoughts behind process changes. Respond to questions in timely manner. Do not be condescending or hostile in your answers. You need to repeat a lot of information because people will forget or be confused. Always be clear on what you say. Do not expect people to understand you implicitly. Discuss the responsibilities of the process and PO group with the other developers at the start of the project. Maybe try discussing the responsibilities before choosing who is process and PO group.","title":"Communication"},{"location":"Legacy/Handover/2020F/advice/#keeping-an-overview","text":"Create a calendar available for all the groups in the project. Fill the calendar with all dates and times planned from the start. Decide where all your documents should be stored and be sure to be transparent with the other developer teams with the documents. Find an effective way to monitor and handle pull requests and reviews. We used Google Sheets and GitHub's API to automate it. Clean up branches after every sprint. Delete those that weren't deleted when merging and make sure every branch has a purpose. Decide if you want META-groups, which are groups focused on different aspects of the project such as front-end, back-end, server, and security.","title":"Keeping an Overview"},{"location":"Legacy/Handover/2020F/giraf_events/","text":"GIRAF Events \u00b6 This page lists and explains the events was used during GIRAF 2020. We took inspiration from Scrum , but changed some of the events to fit the GIRAF project better. Sprint Planning Cross-group Standups Sprint Release Preparation Release Party Sprint Review Sprint Retrospective The semester consists of several 3-4 week sprints. Before each sprint we hold the sprint planning . During the sprints, we do 1-2 cross-group standups every week. In the last 2-3 days, we do sprint release preparation . At the end of the sprint we hold a release party , followed by sprint Review and sprint retrospective the day after. Sometimes we hold release party after review and retrospective, to have it on a friday. Sprint Planning \u00b6 The sprint planning happens before the start of every sprint. Before the sprint planning, the PO group has planned a sprint goal for the sprint. Purpose \u00b6 The purpose of the sprint planning is to plan the next sprint by giving all groups a number of issues to start working on. Practice \u00b6 All members of the development teams should be present in the same room. The meeting follows the structure presented below Presentations \u00b6 The process group presents changes made to the process. The changes are based on the retrospective. The PO group presents the following: The project vision. What is the overall goal for the project? Did this change since last meeting? (This will be a recap, if there are no changes) The sprint vision. What do we want to acomplish with this release? New and important issues for this sprint. These are typically based on usability tests made with the customer. Time Estimation \u00b6 All development teams are split into crossgroups. The crossgroups are made at random. Each crossgroup are assigned a number of issues to time estimate using planning poker . Some crossgroups might only be assigned a single issue, which they then have to split into smaller issues and then time estimate. Planning Poker Rules \u00b6 The Fibonacci numbers are used as weights 0, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144 and \u221e 5 is equal to a single person's workload for a full day A full day's workload is 8 hours If no cards are available, the participants think of a number and say them one by one Cards are the most optimal solution. If the participants have to say their estimations aloud, it is beneficial to start with a different person each time. If the participants are not in agreement, the person with the lowest and highest estimation presents their arguments for their estimation. The \"game\" is then played again. If there are still disagreements, it can be beneficial to have an open discussion. When an estimation has been decided upon, it should be added as a label to the issue on GitHub. Groups Choose Issues \u00b6 When all crossgroups are done with their estimation, the development teams should meet again and decide which issues they would like to work with. Each development team creates a prioritized list of issues they would like to work with. The list should also include the estimations found on GitHub. The list is sent to the PO group along with the total number of estimated work. Remember 5 points is equal to one day. When all development teams have sent their list to the PO group, they will distribute the issues and send the final assignments on Discord. Cross-group Stand-ups \u00b6 The cross-group stand-ups are held once or twice a week. This is the GIRAF counterpart to the Daily Scrum. Purpose \u00b6 The purpose of the cross-group standup meeting is to share knowledge between the groups, figure out what the other groups are working on, and talk about issues that groups have run into. Practice \u00b6 One representative from each group. The process group might be two: one as develop team, and one as process group. Every group goes through: What have you done since last time? What will you do until next time? Do you have any hindrances that stops you from solving a problem? Sprint Release Preparation \u00b6 The last two work-days before a sprint ends should be spent actively hunting for, and fixing bugs. We strongly recommend that all groups are present in the same rooms. Purpose \u00b6 The purpose of the release preparation is to find bugs and prepare the applications for release. Practice \u00b6 The release preparation is split into two phases: Testing Bugfixing The bugfixing phase might start while the testing phase is still ongoing. Testing Solved Issues \u00b6 During this phase the applications are tested. This includes issued that have been solved, but also the application as a whole . Each issue closed in the sprint is handed out to two groups, none of them being the author group. Each group will then be responsible for testing the issue according to the following checklist. Note that you should focus on the issues and not on the pull requests! A new issue should be created on GitHub if 'no' is the answer to any of the following questions: Can the screen be reached through navigation in the application? Can you perform all the functionality defined in the issue? Can it be used without crashing? Does it run without bugs? Does all of the above still look acceptable if you change to a new device or change orientation? If a bug is discovered: If it is related to the release: Create an issue in the relevant repository, using the label. If it is unrelated to the release, maybe a bug that does not hinder release or some badly written code: Create an issue in the relavant repository, using the relevant labels. After an issue has been created, contact the PO group, who will then assign the releaseFix issues. Fixing Release Bugs \u00b6 You get a releaseFix issues from the PO group. Make a new branch named releasefix/#IssueNumber , branched out from the release branch. Write a regression test A regression test should fail before solving the issue and pass after Remember to help each other out! If there is extra time, groups might be asked to write new tests. These tests will be assigned by the process or the PO group. This could be along the lines of \"test file X according to issues Y.\" Release Party \u00b6 The release party is mainly a social event. The purpose of this meeting is to push the features and fixes onto the master branch. Afterwards, people are encouraged to participate in a social event. This is of course optional, but we highly encourage people to talk to their fellow students to make new friends or to get an insight of what they are working on. Sprint Review \u00b6 All developers meets and discuss problems they have experienced while fixing their assigned issues. This event should last a maximum of 4 hours. Purpose \u00b6 To get an understanding of difficulties other developer teams have met, and what their solutions were. To get an overview of all the issues fixed in this sprint. To get an overview of the project as a whole, and what condition it is currently in. Practice \u00b6 All developers meets in a meeting room or on discord. The process group notes which developer teams are present, and assigns a person to write a summary. A person from each developer team starts off with explaining: Issues: What were they meant to solve, and how did they solve it. Problems: Which problems did they face in solving the issues, and how did they resolve it. Noteworthy changes that needs to be mentioned to all the developers. PO presents the status on the product backlog. Remember to ask questions to other developers if you are confused or would like more information about their solutions! Sprint Retrospective \u00b6 All developers meet and gives feedback to the previous sprint about the process and what can be improved. Purpose \u00b6 To improve the quality of life for the developers by gathering feedback from the developers. To make sure everyone gets a chance to say what they have on their mind. Practice \u00b6 All developers split into multiple rooms with a member of the process group in each group. The process group have prepared the subjects to go through for feedback, which will be gone through sequential. Through each of the subjects can the developers give feedback, positive and negative, to improve the sprints overall. When done, the meeting is over. Remember not to discuss the feedback brought up. When the retrospective is over, the process group will make a questionnaire to judge what feedback should be looked into. After Review and Retrospective \u00b6 Make sure that you don't have leftover branches from the sprint. Look through the branches and delete branches that was created by you and that you don't use or plan to use anymore.","title":"GIRAF Events"},{"location":"Legacy/Handover/2020F/giraf_events/#giraf-events","text":"This page lists and explains the events was used during GIRAF 2020. We took inspiration from Scrum , but changed some of the events to fit the GIRAF project better. Sprint Planning Cross-group Standups Sprint Release Preparation Release Party Sprint Review Sprint Retrospective The semester consists of several 3-4 week sprints. Before each sprint we hold the sprint planning . During the sprints, we do 1-2 cross-group standups every week. In the last 2-3 days, we do sprint release preparation . At the end of the sprint we hold a release party , followed by sprint Review and sprint retrospective the day after. Sometimes we hold release party after review and retrospective, to have it on a friday.","title":"GIRAF Events"},{"location":"Legacy/Handover/2020F/giraf_events/#sprint-planning","text":"The sprint planning happens before the start of every sprint. Before the sprint planning, the PO group has planned a sprint goal for the sprint.","title":"Sprint Planning"},{"location":"Legacy/Handover/2020F/giraf_events/#purpose","text":"The purpose of the sprint planning is to plan the next sprint by giving all groups a number of issues to start working on.","title":"Purpose"},{"location":"Legacy/Handover/2020F/giraf_events/#practice","text":"All members of the development teams should be present in the same room. The meeting follows the structure presented below","title":"Practice"},{"location":"Legacy/Handover/2020F/giraf_events/#presentations","text":"The process group presents changes made to the process. The changes are based on the retrospective. The PO group presents the following: The project vision. What is the overall goal for the project? Did this change since last meeting? (This will be a recap, if there are no changes) The sprint vision. What do we want to acomplish with this release? New and important issues for this sprint. These are typically based on usability tests made with the customer.","title":"Presentations"},{"location":"Legacy/Handover/2020F/giraf_events/#time-estimation","text":"All development teams are split into crossgroups. The crossgroups are made at random. Each crossgroup are assigned a number of issues to time estimate using planning poker . Some crossgroups might only be assigned a single issue, which they then have to split into smaller issues and then time estimate.","title":"Time Estimation"},{"location":"Legacy/Handover/2020F/giraf_events/#planning-poker-rules","text":"The Fibonacci numbers are used as weights 0, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144 and \u221e 5 is equal to a single person's workload for a full day A full day's workload is 8 hours If no cards are available, the participants think of a number and say them one by one Cards are the most optimal solution. If the participants have to say their estimations aloud, it is beneficial to start with a different person each time. If the participants are not in agreement, the person with the lowest and highest estimation presents their arguments for their estimation. The \"game\" is then played again. If there are still disagreements, it can be beneficial to have an open discussion. When an estimation has been decided upon, it should be added as a label to the issue on GitHub.","title":"Planning Poker Rules"},{"location":"Legacy/Handover/2020F/giraf_events/#groups-choose-issues","text":"When all crossgroups are done with their estimation, the development teams should meet again and decide which issues they would like to work with. Each development team creates a prioritized list of issues they would like to work with. The list should also include the estimations found on GitHub. The list is sent to the PO group along with the total number of estimated work. Remember 5 points is equal to one day. When all development teams have sent their list to the PO group, they will distribute the issues and send the final assignments on Discord.","title":"Groups Choose Issues"},{"location":"Legacy/Handover/2020F/giraf_events/#cross-group-stand-ups","text":"The cross-group stand-ups are held once or twice a week. This is the GIRAF counterpart to the Daily Scrum.","title":"Cross-group Stand-ups"},{"location":"Legacy/Handover/2020F/giraf_events/#purpose_1","text":"The purpose of the cross-group standup meeting is to share knowledge between the groups, figure out what the other groups are working on, and talk about issues that groups have run into.","title":"Purpose"},{"location":"Legacy/Handover/2020F/giraf_events/#practice_1","text":"One representative from each group. The process group might be two: one as develop team, and one as process group. Every group goes through: What have you done since last time? What will you do until next time? Do you have any hindrances that stops you from solving a problem?","title":"Practice"},{"location":"Legacy/Handover/2020F/giraf_events/#sprint-release-preparation","text":"The last two work-days before a sprint ends should be spent actively hunting for, and fixing bugs. We strongly recommend that all groups are present in the same rooms.","title":"Sprint Release Preparation"},{"location":"Legacy/Handover/2020F/giraf_events/#purpose_2","text":"The purpose of the release preparation is to find bugs and prepare the applications for release.","title":"Purpose"},{"location":"Legacy/Handover/2020F/giraf_events/#practice_2","text":"The release preparation is split into two phases: Testing Bugfixing The bugfixing phase might start while the testing phase is still ongoing.","title":"Practice"},{"location":"Legacy/Handover/2020F/giraf_events/#testing-solved-issues","text":"During this phase the applications are tested. This includes issued that have been solved, but also the application as a whole . Each issue closed in the sprint is handed out to two groups, none of them being the author group. Each group will then be responsible for testing the issue according to the following checklist. Note that you should focus on the issues and not on the pull requests! A new issue should be created on GitHub if 'no' is the answer to any of the following questions: Can the screen be reached through navigation in the application? Can you perform all the functionality defined in the issue? Can it be used without crashing? Does it run without bugs? Does all of the above still look acceptable if you change to a new device or change orientation? If a bug is discovered: If it is related to the release: Create an issue in the relevant repository, using the label. If it is unrelated to the release, maybe a bug that does not hinder release or some badly written code: Create an issue in the relavant repository, using the relevant labels. After an issue has been created, contact the PO group, who will then assign the releaseFix issues.","title":"Testing Solved Issues"},{"location":"Legacy/Handover/2020F/giraf_events/#fixing-release-bugs","text":"You get a releaseFix issues from the PO group. Make a new branch named releasefix/#IssueNumber , branched out from the release branch. Write a regression test A regression test should fail before solving the issue and pass after Remember to help each other out! If there is extra time, groups might be asked to write new tests. These tests will be assigned by the process or the PO group. This could be along the lines of \"test file X according to issues Y.\"","title":"Fixing Release Bugs"},{"location":"Legacy/Handover/2020F/giraf_events/#release-party","text":"The release party is mainly a social event. The purpose of this meeting is to push the features and fixes onto the master branch. Afterwards, people are encouraged to participate in a social event. This is of course optional, but we highly encourage people to talk to their fellow students to make new friends or to get an insight of what they are working on.","title":"Release Party"},{"location":"Legacy/Handover/2020F/giraf_events/#sprint-review","text":"All developers meets and discuss problems they have experienced while fixing their assigned issues. This event should last a maximum of 4 hours.","title":"Sprint Review"},{"location":"Legacy/Handover/2020F/giraf_events/#purpose_3","text":"To get an understanding of difficulties other developer teams have met, and what their solutions were. To get an overview of all the issues fixed in this sprint. To get an overview of the project as a whole, and what condition it is currently in.","title":"Purpose"},{"location":"Legacy/Handover/2020F/giraf_events/#practice_3","text":"All developers meets in a meeting room or on discord. The process group notes which developer teams are present, and assigns a person to write a summary. A person from each developer team starts off with explaining: Issues: What were they meant to solve, and how did they solve it. Problems: Which problems did they face in solving the issues, and how did they resolve it. Noteworthy changes that needs to be mentioned to all the developers. PO presents the status on the product backlog. Remember to ask questions to other developers if you are confused or would like more information about their solutions!","title":"Practice"},{"location":"Legacy/Handover/2020F/giraf_events/#sprint-retrospective","text":"All developers meet and gives feedback to the previous sprint about the process and what can be improved.","title":"Sprint Retrospective"},{"location":"Legacy/Handover/2020F/giraf_events/#purpose_4","text":"To improve the quality of life for the developers by gathering feedback from the developers. To make sure everyone gets a chance to say what they have on their mind.","title":"Purpose"},{"location":"Legacy/Handover/2020F/giraf_events/#practice_4","text":"All developers split into multiple rooms with a member of the process group in each group. The process group have prepared the subjects to go through for feedback, which will be gone through sequential. Through each of the subjects can the developers give feedback, positive and negative, to improve the sprints overall. When done, the meeting is over. Remember not to discuss the feedback brought up. When the retrospective is over, the process group will make a questionnaire to judge what feedback should be looked into.","title":"Practice"},{"location":"Legacy/Handover/2020F/giraf_events/#after-review-and-retrospective","text":"Make sure that you don't have leftover branches from the sprint. Look through the branches and delete branches that was created by you and that you don't use or plan to use anymore.","title":"After Review and Retrospective"},{"location":"Legacy/Handover/2020F/github/","text":"Use of GitHub in GIRAF \u00b6 As explained in the process manual of 2019 , they changed from GitLab to GitHub. This article will explain how GitHub is used in the 2020 GIRAF project. Issues \u00b6 Issues are created by the development teams as well as the PO group. An issue can be a bug report or a task creation request. The PO group prioritises, assigns and adds milestones to issues. The list of issues can be seen at each repository, eg. https://github.com/aau-giraf/weekplanner/issues , or a complete list for the whole organization. Getting an Issue to Work on \u00b6 If you have time to work on a new issue, you can get a new one by following these steps: Find an issue you want to work on Ask the PO group if you can work on that issue The PO group might say no for various reasons and they have the final say, as they have a better overview. There is usually a greater chance of getting a yes if the issue you've picked is either highest or high priority. If you don't have a preferred issue you can ask the PO group to be assigned the most pressing issue, as they have a good overview of the project and they will most likely have some issues that they would like you to work with. Creating an Issue \u00b6 If you find a bug, or have a task creation request you can create an issue: Go the the \"Issues\" tab of the relevant repository (E.g. https://github.com/aau-giraf/weekplanner/issues ). Press the green \"New issue\" button. Choose whether to submit a bug report or task creation request, and press \"Get started\". Create a title and description for the issue. Please follow the template, and don't delete the headers! The title for the Task Creation Request should tell what functionality you would like added using the shown form \"As a developer I would like the docker config file to automatically update so that I don\u00b4t have to manually update the config file\". Instead of the task being for the developer, guardian or user is also frequently used. Label the issue with appropriate labels. It can be a good idea to inform the PO group when you are done, so they can assign and refine the issue. Branches and Pull Requests \u00b6 We follow the GitFlow Workflow as explained in the process manual of 2019 . During the sprints, all development is done in feature branches, branching out from the develop branch. The naming convention for feature branches is feature/xx where xx is replaced by the issue number. When the Release Preparation phase begins, a release branch is created from the develop branch. This branch is now used instead of develop until the sprint is over. The naming convention for release branches is release/YYYYsXrZ where YYYY is replaced by year, X with the sprint number and Z with the release number. E.g. release/2020s1r1 for 2020, sprint 1, release 1. Creating a Branch \u00b6 During Sprints \u00b6 When you start working on an issue, you create a branch from develop called feature/xx where xx is the issue number. From the terminal: 1 2 git checkout develop git checkout -b feature/xx Or from GitHub: Make sure develop is selected. Input the name of the branch (e.g. for feature 400). Press \"Create branch: feature/xx from 'develop'\" During Release Preparation \u00b6 When you start working on a release fix, you create a branch from release/* called releasefix/xx where xx is the issue number. 1 2 git checkout release/* git checkout -b releasefix/xx Or from GitHub using the same procedure as above, but with the release branch as base instead, and with the release fix naming convention. Creating a Pull Request \u00b6 When you have finished your issue, it is time to create a pull request. A pull request is a request to merge your branch into another branch. Before making the pull request, make sure that the code: Only relates to a single issue. (One PR per user story) Is fully tested. Is reachable when opening the application. Fully tested means that if any piece of the functionality is removed, a test should fail. Creating a pull request on GitHub: Open the \"Pull requests\" tab in the repository (e.g. https://github.com/aau-giraf/weekplanner/pulls ) Press \"New pull request\" Select the appropriate branch as base. develop if during sprint release/* if during release preparation Select your branch for as compare Press \"Create pull request\" Name the pull request Feature xx or Feature xx: A title describing changes Write a description If you write closes #xx or fixes #xx , issue xx will be linked to the PR and will close when the PR is merged. ( All keywords ) Code Review \u00b6 After being assigned a pull request, the group should review the code under the Files changed tab. Look for code that may be deprecated, unnecessary, non-optimized or has weird formatting. Start at https://github.com/aau-giraf/ Choose repository eg. weekplanner. Click on the Pull Request tab. Choose an open pull request from the list. Click on the Files Changed Tab . All the changes can be seen in these files. Make a comment or suggestion on a single line or multiple lines by pressing the blue + icon (move the cursor to a line). The red square marks the selection icon which can be used to suggest code that replaces the line(s). You can view what the author will see by clicking Preview . Having looked over all the files, click Review changes . If you made comments, make sure the author looks them through by choosing Request changes before clicking Submit review . If changes are made, you have to re-review the pull request! If the changes makes sense, click Approve .","title":"Use of GitHub in GIRAF"},{"location":"Legacy/Handover/2020F/github/#use-of-github-in-giraf","text":"As explained in the process manual of 2019 , they changed from GitLab to GitHub. This article will explain how GitHub is used in the 2020 GIRAF project.","title":"Use of GitHub in GIRAF"},{"location":"Legacy/Handover/2020F/github/#issues","text":"Issues are created by the development teams as well as the PO group. An issue can be a bug report or a task creation request. The PO group prioritises, assigns and adds milestones to issues. The list of issues can be seen at each repository, eg. https://github.com/aau-giraf/weekplanner/issues , or a complete list for the whole organization.","title":"Issues"},{"location":"Legacy/Handover/2020F/github/#getting-an-issue-to-work-on","text":"If you have time to work on a new issue, you can get a new one by following these steps: Find an issue you want to work on Ask the PO group if you can work on that issue The PO group might say no for various reasons and they have the final say, as they have a better overview. There is usually a greater chance of getting a yes if the issue you've picked is either highest or high priority. If you don't have a preferred issue you can ask the PO group to be assigned the most pressing issue, as they have a good overview of the project and they will most likely have some issues that they would like you to work with.","title":"Getting an Issue to Work on"},{"location":"Legacy/Handover/2020F/github/#creating-an-issue","text":"If you find a bug, or have a task creation request you can create an issue: Go the the \"Issues\" tab of the relevant repository (E.g. https://github.com/aau-giraf/weekplanner/issues ). Press the green \"New issue\" button. Choose whether to submit a bug report or task creation request, and press \"Get started\". Create a title and description for the issue. Please follow the template, and don't delete the headers! The title for the Task Creation Request should tell what functionality you would like added using the shown form \"As a developer I would like the docker config file to automatically update so that I don\u00b4t have to manually update the config file\". Instead of the task being for the developer, guardian or user is also frequently used. Label the issue with appropriate labels. It can be a good idea to inform the PO group when you are done, so they can assign and refine the issue.","title":"Creating an Issue"},{"location":"Legacy/Handover/2020F/github/#branches-and-pull-requests","text":"We follow the GitFlow Workflow as explained in the process manual of 2019 . During the sprints, all development is done in feature branches, branching out from the develop branch. The naming convention for feature branches is feature/xx where xx is replaced by the issue number. When the Release Preparation phase begins, a release branch is created from the develop branch. This branch is now used instead of develop until the sprint is over. The naming convention for release branches is release/YYYYsXrZ where YYYY is replaced by year, X with the sprint number and Z with the release number. E.g. release/2020s1r1 for 2020, sprint 1, release 1.","title":"Branches and Pull Requests"},{"location":"Legacy/Handover/2020F/github/#creating-a-branch","text":"","title":"Creating a Branch"},{"location":"Legacy/Handover/2020F/github/#during-sprints","text":"When you start working on an issue, you create a branch from develop called feature/xx where xx is the issue number. From the terminal: 1 2 git checkout develop git checkout -b feature/xx Or from GitHub: Make sure develop is selected. Input the name of the branch (e.g. for feature 400). Press \"Create branch: feature/xx from 'develop'\"","title":"During Sprints"},{"location":"Legacy/Handover/2020F/github/#during-release-preparation","text":"When you start working on a release fix, you create a branch from release/* called releasefix/xx where xx is the issue number. 1 2 git checkout release/* git checkout -b releasefix/xx Or from GitHub using the same procedure as above, but with the release branch as base instead, and with the release fix naming convention.","title":"During Release Preparation"},{"location":"Legacy/Handover/2020F/github/#creating-a-pull-request","text":"When you have finished your issue, it is time to create a pull request. A pull request is a request to merge your branch into another branch. Before making the pull request, make sure that the code: Only relates to a single issue. (One PR per user story) Is fully tested. Is reachable when opening the application. Fully tested means that if any piece of the functionality is removed, a test should fail. Creating a pull request on GitHub: Open the \"Pull requests\" tab in the repository (e.g. https://github.com/aau-giraf/weekplanner/pulls ) Press \"New pull request\" Select the appropriate branch as base. develop if during sprint release/* if during release preparation Select your branch for as compare Press \"Create pull request\" Name the pull request Feature xx or Feature xx: A title describing changes Write a description If you write closes #xx or fixes #xx , issue xx will be linked to the PR and will close when the PR is merged. ( All keywords )","title":"Creating a Pull Request"},{"location":"Legacy/Handover/2020F/github/#code-review","text":"After being assigned a pull request, the group should review the code under the Files changed tab. Look for code that may be deprecated, unnecessary, non-optimized or has weird formatting. Start at https://github.com/aau-giraf/ Choose repository eg. weekplanner. Click on the Pull Request tab. Choose an open pull request from the list. Click on the Files Changed Tab . All the changes can be seen in these files. Make a comment or suggestion on a single line or multiple lines by pressing the blue + icon (move the cursor to a line). The red square marks the selection icon which can be used to suggest code that replaces the line(s). You can view what the author will see by clicking Preview . Having looked over all the files, click Review changes . If you made comments, make sure the author looks them through by choosing Request changes before clicking Submit review . If changes are made, you have to re-review the pull request! If the changes makes sense, click Approve .","title":"Code Review"},{"location":"Legacy/Handover/2020F/kubernetes/","text":"Kubernetes \u00b6 This is outdated, but is kept to give a view of the former GIRAF project (3/11-2020) In this article we describe what Kubernetes is, how it works and how to use it. First we describe the basic principles of getting a minimum usable Kubernetes infrastructure, with services, up and running. Afterwards we describe the advanced principles of accessing Kubernetes externally, setting up persistent storage, storing secrets securely and setting up certificates. The official Kubernetes documentation defines Kubernetes in the following way: \"Kubernetes is an open-source platform for automating deployment, scaling, and operations of application containers across clusters of hosts, providing container-centric infrastructure\" Basic Principles \u00b6 In this section we describe how to set up a minimum usable Kubernetes infrastructure. In this context a minimum usable Kubernetes infrastructure means that it can run our applications, but doesn't necessarily support making the applications available externally. To give a better understanding of Kubernetes we have an explanation of some of its basic concepts. We start by introducing the concepts from top to bottom, which means explaining what a Kubernetes cluster is, then explain what a cluster depends on and what the cluster's dependencies depend on. Physical (hardware) manifestation of Kubernetes setup will be explained from the three following concepts. Cluster: A group of physical and/or virtual machines. A cluster contains at least one master and zero or more node servers. In the picture, an example of a cluster with a single master and three nodes is shown. Master: A server which manages the nodes running in the cluster. There can be multiple master servers in a cluster, which can give better performance for huge clusters and better uptime. The reason this gives better performance is that each master server can concentrate on managing one part of a larger system. The reason it gives better uptime is because configuration files are also distributed between all master servers, so if one master server goes down, all the other master servers can take over for it. Node: A server which runs and manages the actual applications. It can be controlled by a master server through the kubelet service. Now we need to understand the system services that run on master and node servers. While some of these are part of Kubernetes, they are not Kubernetes services, which is a separate type of abstraction over pods to make them accessible. The following system services run on master: API-server: System service that validates and configures request to alter the state of the cluster. It provides a web API server to perform the requests. Part of Kubernetes. Scheduler: System service that schedules how workloads should be distributed between nodes. Part of Kubernetes. Controller Manager: System service that attempts to change the current cluster state towards the desired state. It runs the loop for various controllers, that control various areas of the cluster. Part of Kubernetes. Etcd: System service that provides a configuration store that can be distributed between multiple servers. Kubernetes stores its configuration in etcd. Third party program. Flannel: System service that provides a overlay network for communication between containers on different nodes. Flannel also uses etcd to store its configuration. Third party program. The following system services run on node: Flannel: System service that is setup to connect to a Flannel network. It fetches its network configuration from etcd. Third party program. Kubelet: System service that is responsible for the communication between masters and nodes. It ensures the appropriate containers are started and remain healthy. Part of Kubernetes. Proxy: System service that provides a network proxy and load balancer for services on a node. Part of Kubernetes. As the Kubernetes cluster is a network of servers, it's important to understand the usage of different IP-addresses and subnets. Subnets are to divide IP-addresses into networks, so that a specific range of IP-addresses are allocated within the network. A subnet can be formally defined using the cidr notation. An example of cidr notation is 192.168.0.0/16, which refers to the subnet with IP-addresses that range from 192.168.0.0 to 192.168.255.255. The \"/16\" suffix implies that the first 16 bits of the total 32 bits of the IP-addresses are shared in the subnet. The shared part of an IP-address is also called the network prefix. Cluster Subnet: The subnet for the local network. Flannel Subnet: The subnet for the Flannel overlay network, which allows communication between containers on different nodes, without using the node's IP-address. This is useful, since the containers don't have to negotiate usage of ports on the nodes, as they can get their own IP-address. Service Subnet: The subnet where Kubernetes services get an dedicated IP-address. Kubernetes Service IP: The IP-address on the service subnet, that can be used to read and write Kubernetes configuration over the network. DNS Service IP: The IP-address on the service subnet, that points to Kubernetes DNS server, that creates network hostnames for Kubernetes services. Kubernetes Manifest Files \u00b6 In this section we explain what manifest files are and how they are used to deploy cluster objects. A manifest file is a recipe describing what an arbitrary cluster object is and the data required to deploy it. There are three important manifest files in a minimal usable Kubernetes set up, which are the following. Pod: A group of one or more containers. Kubernetes abstracts away the container format. The Docker container format is currently the de facto standard, but this could change since many new container formats are emerging. Containers running in the same pod share IP-address space, so they can address each other through localhost. But running multiple containers in a single pod should only be done if the applications in the containers are tightly coupled. Pods do not have any persistent storage, so when a pod is killed all the data will be removed, for this reason persistent volumes are used. This is shown partly in the Deployment code. Deployment: A template for defining the deployment of multiple instances of the same type of pod. It makes it easier to deploy and manage a lot of the same pods as it makes it possible to attach a common name to the collection of pods. This is shown fully in the Deployment code. Service: An abstraction over a collection of the same pod. It's possible to access the pods through a service by binding a service to the pods' deployment. The service abstracts over the pods, making it look like, to an external observer, that there is only a single pod running. Services can also implement load balancing. This is shown in the Service code. In the Deployment code section below we show part of our deployment for Artifactory. It's split into four parts: apiVersion, kind, metadata and spec. The apiVersion (line 1) indicates which version of the Kubernetes API is used, in this case it's the extensions/v1beta1 as the deployment uses some experimental features. Then the kind (line 2) indicates which class of cluster object it falls into, which is a Deployment in this case. Then metadata (lines 3-4) is defined which is used to give a unique name for the deployment. And finally the most important part is the outermost spec (lines 5-10) which is a template for generating an arbitrary amount of pods, defined by the variable \"replicas\", in this case one pod would be generated. The template -> metadata -> labels -> app tree is simply to apply a label stating which app runs in the pods. These pods are based on the innermost spec (lines 11-16) which is a definition of a pod. Code: Deployment \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : extensions/v1beta1 kind : Deployment metadata : name : artifactory spec : replicas : 1 template : metadata : labels : app : artifactory spec : containers : name : artifactory image : docker.bintray.io/jfrog/artifactory-oss:latest ports : containerPort : 8081 In the Service code we show an example of our service for Artifactory. Like the deployment it's split into four parts: apiVersion, kind, metadata and spec. The first three parts work like they did in the Deployment code. The new things are specified inside the spec. Notice the definition of ports, which is used to expose the deployment to other pods in a cluster. The targetPort is the port which the container exposes internally, such the Artifactory (which runs in the container) can see it. It is connected to the Port port, which can be used to access the pod internally in a cluster, and thus Artifactory. Code: Service \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : v1 kind : Service metadata : name : artifactory labels : app : artifactory spec : selector : app : artifactory ports : port : 8080 targetPort : 8081 Advanced Principles \u00b6 In this section we describe how to set up a more advanced Kubernetes infrastructure, as the Kubernetes infrastructure described above does not restrict access to the API server, cannot be accessed easily from outside of the cluster and does not have persistent data storage. Control access to API server \u00b6 The default access control in Kubernetes allows anyone to perform any request on the API server. This is problematic if anyone untrusted is on the same network as the API server. Fortunately Kubernetes has a rich framework for controlling access to the API server. When making a request to the API server, the request runs through several stages that control access: Authentication: Decides the identity of the requester. Authorization: Decides whether the identity has permissions to make the request. Admission control: Decides if the request complies with various rules. Authentication of API requests is divided into two categories by the type of the requester, Service Accounts and normal users. Service Accounts allow pods to communicate with the API server. Kubernetes automatically creates Service Accounts and mounts Service Account credential files into the pods. With the credential files requests can be performed. Normal users on the other hand are not managed by Kubernetes. They are used to access the API from frontend tools such as the Kubernetes command line tool kubectl or a Web UI. There are several ways to authenticate normal users. Using certificates, bearer tokens, an authenticating proxy, or HTTP basic authentication. Authorization of API requests can happen in four different modes: ABAC: Attribute-based access control authorizes using policies that combine attributes of users and cluster objects into boolean expressions. As of Kubernetes 1.6 this mode is considered deprecated as it is difficult to manage and understand. RBAC: Role-based access control authorizes using role groups that expand its members permissions. Webhook: Authorize with HTTP callbacks. This mode is probably deprecated since its documentation page is deleted. Custom: Authorize with a custom mode. Admission control restricts API requests by intercepting the requests in various plug-ins. Access for a request is only allowed if all plug-ins accept it. Here is a list of commonly used plug-ins: NamespaceLifecycle: Rejects a request, if it tries to create objects in a terminating Kubernetes namespace. (Kubernetes divides cluster objects into namespaces) NamespaceExists: Rejects a request, if it tries to create cluster objects in nonexistent namespaces.* LimitRanger: Rejects a request, if it demands CPU and memory resources for a pod, that is beyond CPU and memory limit policies on a namespace. ResourceQuota: Rejects a request, if it demands CPU and memory resources for a namespace, that is beyond demand resource quotas for a namespace. SecurityContextDeny: Reject a request, if it attempts to change SecurityContext fields, that allows privilege escalation. ServiceAccount: Accepts all requests. The plug-in makes sure that a Service Account is always available on pods. Setting up these controls is done when configuring the API server. The following arguments should be included in the configuration file, under KUBE\\_API\\_ARGS: client-ca: This argument ensures that any client presenting a signed certificate is authenticated as per the CommonName attribute in that certificate. The client-ca-file contains a list of authorities that can sign certificates. tls-cert: Points to the certificates used to provide HTTPS serving. If no certificates are specified (using --tls-private-key), it generates and uses self-signed ones automatically. tls-private-key Specifies the private key used for HTTPS serving, which should match the certificate file (specified using --tls-cert-file). tls-cert-file Specifies the x509 certificate used for HTTPS, which should match the private key for the same (specified using --tls-private-key). service-account-key Specifies the PEM-encoded x509 RSA or ECDSA private or public key file, for verifying ServiceAccount tokens. External Access \u00b6 In this section we explain how it's possible to externally access a cluster and routing domains to applications in a cluster. To make a cluster and routing domains to applications at the same time, we needed to use what's called an Ingress controller. An Ingress controller exposes a Kubernetes service so it is accessible outside the cluster network. An Ingress controller created with a manifest file, by specifying the kind attribute as Ingress. Kubernetes doesn't provide a default Ingress implementation, it only defines an interface where other people can then write their own implementation. The following Ingress controller implementations were investigated: Cloud implementation: Large cloud providers such as Microsoft, Amazon and Google provide Ingress controllers specific to their clouds. These Ingress controllers are not usable in self-hosted Kubernetes clusters. Nginx implementation: This Ingress controller implementation is part of the official Kubernetes git repositories. It uses the Nginx reverse proxy capabilities to direct incoming traffic. Nginx is lightweight and has a rich dsl, but it requires additional configuration to work with Kubernetes. When using ssl encryption service kube-lego can be used to automatically update ssl certificates. Trafik implementation: Trafik is a reverse proxy that integrates with several different container orchestration technology such as Swarm, Mesos and Kubernetes. It provides a web interface to display and configure its configuration. It is well integrated into Kubernetes, so Ingress rules works without additional configuration. Trafik also provides fully automated administration of ssl encryption. In the picture below it is illustrated how Trafik works. The blue arrows represent HTTP request to various domains, the green arrows represent how Trafik redirects the request to services and the red arrow shows that Trafik is controlled the cloud orchestration technology. Persistent Storage \u00b6 In this section we explain how persistent storage is achieved with Kubernetes. In Kubernetes there are two concepts when dealing with storage, which are described as the following. We have our information about storage in Kubernetes from the official documentation. PersistentVolume: A volume that is persisted throughout a pod's lifespan. It is an abstraction over different storage types like nfs, GlusterFS and local hosted storage. It is recommended to use network storage with a persistent volume and not local hosted storage, as local hosted storage option was only introduced for testing a Kubernetes set up. On its own a persistent volume cannot be used by a pod, a pod must have a claim for a persistent volume before it can be used. PersistentVolumeClaim: A claim for a persistent volume. If a pod needs to use storage, then it has to use a claim. In the picture above we show a 100 GB PersistentVolume (PV) for a 200 GB nfs storage, where two 20 GB PersistentVolumeClaims are claiming 20 GB storage each. This illustrates a PersistentVolume as a finite resource, which can be used to set a limit on how much storage there can be used from an external storage. A PersistentVolumeClaim has to be used if a pod needs to use a part of the PersistentVolume's storage. This model means an external physical storage can be partitioned into multiple PersistentVolumes, which then can also be partitioned into multiple PersistentVolumeClaim. In the PersistentVolume code we show an example of a PersistentVolume manifest file. This manifest file specifically describes a PersistentVolume for nfs. The essential parts of the manifest files are the capacity, accessModes and nfs labels. The capacity contains the definition of how much storage our PersistentVolume can use. The accessModes label contains the definition of the cardinality between the storage and nodes, which can be the following. ReadWriteOnce: The volume can be mounted as read-write by a single node. ReadOnlyMany: The volume can be mounted read-only by many nodes. ReadWriteMany: The volume can be mounted as read-write by many nodes. The last attribute is the nfs attribute. This defines the address of the server hosting the nfs storage, the path to the nfs folder on the server and whether or not the folder is readonly. Code: PersistentVolume example \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : v1 kind : PersistentVolume metadata : name : ci labels : volume : ci spec : capacity : storage : 10Gi accessModes : ReadWriteMany nfs : server : master.giraf.cs.aau.dk path : /var/nfsshare readOnly : false In the PersistentVolumeClaim code we show an example of a PersistentVolumeClaim, which can claim a part of the PersistentVolume shown in the PersistentVolume code. To ensure a PersistentVolumeClaim is a claim for a specific PersistentVolume, a selector has to be used. A selector can filter what PersistentVolume a PersistentVolumeClaim can be matched with, by using the label defined in one or more PersistentVolumes. In this example we have a label called volume: ci in the PersistentVolume, which the selector in the PersistentVolumeClaim then match on, such that the PersistentVolumeClaim will be assigned only to a PersistentVolume with the same label and sufficient space. If a selector is not specified, the PersistentVolumeClaim will be matched based on its access mode to any PersistentVolume with sufficient space to host the PersistentVolumeClaim. Code: PersistentVolumeClaim example \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : v1 kind : PersistentVolumeClaim metadata : name : artifactory-pvc spec : selector : matchLabels : volume : ci accessModes : ReadWriteMany resources : requests : storage : 5Gi After defining a PersistentVolumeClaim it is then possible to use it from a deployment manifest file. In the Deployment code we show an example of how volumes are used in a deployment. In this example we have added two new specs which are volumeMounts (lines 19-22) and volumes (lines 23-26). The attribute volumes describes that we want to make a volume named artifactory-vol which uses the PersistentVolumeClaim artifactory-pvc (the PersistentVolumeClaim shown in the PersistentVulmeClaim code). Then we use the volumeMounts to specify where we want to mount artifactory-vol, the mountPath is where we mount our volume inside the container and subPath is the name of the folder, in our nfs storage, we want to mount at the mountPath. Code: Deployment example \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion : extensions/v1beta1 kind : Deployment metadata : name : artifactory labels : app : artifactory spec : replicas : 1 template : metadata : labels : app : artifactory spec : containers : name : artifactory image : docker.bintray.io/jfrog/artifactory-oss:latest ports : containerPort : 8081 volumeMounts : name : artifactory-vol mountPath : /var/opt/jfrog/artifactory subPath : artifactory volumes : name : artifactory-vol persistentVolumeClaim : claimName : artifactory-pvc","title":"Kubernetes"},{"location":"Legacy/Handover/2020F/kubernetes/#kubernetes","text":"This is outdated, but is kept to give a view of the former GIRAF project (3/11-2020) In this article we describe what Kubernetes is, how it works and how to use it. First we describe the basic principles of getting a minimum usable Kubernetes infrastructure, with services, up and running. Afterwards we describe the advanced principles of accessing Kubernetes externally, setting up persistent storage, storing secrets securely and setting up certificates. The official Kubernetes documentation defines Kubernetes in the following way: \"Kubernetes is an open-source platform for automating deployment, scaling, and operations of application containers across clusters of hosts, providing container-centric infrastructure\"","title":"Kubernetes"},{"location":"Legacy/Handover/2020F/kubernetes/#basic-principles","text":"In this section we describe how to set up a minimum usable Kubernetes infrastructure. In this context a minimum usable Kubernetes infrastructure means that it can run our applications, but doesn't necessarily support making the applications available externally. To give a better understanding of Kubernetes we have an explanation of some of its basic concepts. We start by introducing the concepts from top to bottom, which means explaining what a Kubernetes cluster is, then explain what a cluster depends on and what the cluster's dependencies depend on. Physical (hardware) manifestation of Kubernetes setup will be explained from the three following concepts. Cluster: A group of physical and/or virtual machines. A cluster contains at least one master and zero or more node servers. In the picture, an example of a cluster with a single master and three nodes is shown. Master: A server which manages the nodes running in the cluster. There can be multiple master servers in a cluster, which can give better performance for huge clusters and better uptime. The reason this gives better performance is that each master server can concentrate on managing one part of a larger system. The reason it gives better uptime is because configuration files are also distributed between all master servers, so if one master server goes down, all the other master servers can take over for it. Node: A server which runs and manages the actual applications. It can be controlled by a master server through the kubelet service. Now we need to understand the system services that run on master and node servers. While some of these are part of Kubernetes, they are not Kubernetes services, which is a separate type of abstraction over pods to make them accessible. The following system services run on master: API-server: System service that validates and configures request to alter the state of the cluster. It provides a web API server to perform the requests. Part of Kubernetes. Scheduler: System service that schedules how workloads should be distributed between nodes. Part of Kubernetes. Controller Manager: System service that attempts to change the current cluster state towards the desired state. It runs the loop for various controllers, that control various areas of the cluster. Part of Kubernetes. Etcd: System service that provides a configuration store that can be distributed between multiple servers. Kubernetes stores its configuration in etcd. Third party program. Flannel: System service that provides a overlay network for communication between containers on different nodes. Flannel also uses etcd to store its configuration. Third party program. The following system services run on node: Flannel: System service that is setup to connect to a Flannel network. It fetches its network configuration from etcd. Third party program. Kubelet: System service that is responsible for the communication between masters and nodes. It ensures the appropriate containers are started and remain healthy. Part of Kubernetes. Proxy: System service that provides a network proxy and load balancer for services on a node. Part of Kubernetes. As the Kubernetes cluster is a network of servers, it's important to understand the usage of different IP-addresses and subnets. Subnets are to divide IP-addresses into networks, so that a specific range of IP-addresses are allocated within the network. A subnet can be formally defined using the cidr notation. An example of cidr notation is 192.168.0.0/16, which refers to the subnet with IP-addresses that range from 192.168.0.0 to 192.168.255.255. The \"/16\" suffix implies that the first 16 bits of the total 32 bits of the IP-addresses are shared in the subnet. The shared part of an IP-address is also called the network prefix. Cluster Subnet: The subnet for the local network. Flannel Subnet: The subnet for the Flannel overlay network, which allows communication between containers on different nodes, without using the node's IP-address. This is useful, since the containers don't have to negotiate usage of ports on the nodes, as they can get their own IP-address. Service Subnet: The subnet where Kubernetes services get an dedicated IP-address. Kubernetes Service IP: The IP-address on the service subnet, that can be used to read and write Kubernetes configuration over the network. DNS Service IP: The IP-address on the service subnet, that points to Kubernetes DNS server, that creates network hostnames for Kubernetes services.","title":"Basic Principles"},{"location":"Legacy/Handover/2020F/kubernetes/#kubernetes-manifest-files","text":"In this section we explain what manifest files are and how they are used to deploy cluster objects. A manifest file is a recipe describing what an arbitrary cluster object is and the data required to deploy it. There are three important manifest files in a minimal usable Kubernetes set up, which are the following. Pod: A group of one or more containers. Kubernetes abstracts away the container format. The Docker container format is currently the de facto standard, but this could change since many new container formats are emerging. Containers running in the same pod share IP-address space, so they can address each other through localhost. But running multiple containers in a single pod should only be done if the applications in the containers are tightly coupled. Pods do not have any persistent storage, so when a pod is killed all the data will be removed, for this reason persistent volumes are used. This is shown partly in the Deployment code. Deployment: A template for defining the deployment of multiple instances of the same type of pod. It makes it easier to deploy and manage a lot of the same pods as it makes it possible to attach a common name to the collection of pods. This is shown fully in the Deployment code. Service: An abstraction over a collection of the same pod. It's possible to access the pods through a service by binding a service to the pods' deployment. The service abstracts over the pods, making it look like, to an external observer, that there is only a single pod running. Services can also implement load balancing. This is shown in the Service code. In the Deployment code section below we show part of our deployment for Artifactory. It's split into four parts: apiVersion, kind, metadata and spec. The apiVersion (line 1) indicates which version of the Kubernetes API is used, in this case it's the extensions/v1beta1 as the deployment uses some experimental features. Then the kind (line 2) indicates which class of cluster object it falls into, which is a Deployment in this case. Then metadata (lines 3-4) is defined which is used to give a unique name for the deployment. And finally the most important part is the outermost spec (lines 5-10) which is a template for generating an arbitrary amount of pods, defined by the variable \"replicas\", in this case one pod would be generated. The template -> metadata -> labels -> app tree is simply to apply a label stating which app runs in the pods. These pods are based on the innermost spec (lines 11-16) which is a definition of a pod.","title":"Kubernetes Manifest Files"},{"location":"Legacy/Handover/2020F/kubernetes/#code-deployment","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : extensions/v1beta1 kind : Deployment metadata : name : artifactory spec : replicas : 1 template : metadata : labels : app : artifactory spec : containers : name : artifactory image : docker.bintray.io/jfrog/artifactory-oss:latest ports : containerPort : 8081 In the Service code we show an example of our service for Artifactory. Like the deployment it's split into four parts: apiVersion, kind, metadata and spec. The first three parts work like they did in the Deployment code. The new things are specified inside the spec. Notice the definition of ports, which is used to expose the deployment to other pods in a cluster. The targetPort is the port which the container exposes internally, such the Artifactory (which runs in the container) can see it. It is connected to the Port port, which can be used to access the pod internally in a cluster, and thus Artifactory.","title":"Code: Deployment"},{"location":"Legacy/Handover/2020F/kubernetes/#code-service","text":"1 2 3 4 5 6 7 8 9 10 11 12 apiVersion : v1 kind : Service metadata : name : artifactory labels : app : artifactory spec : selector : app : artifactory ports : port : 8080 targetPort : 8081","title":"Code: Service"},{"location":"Legacy/Handover/2020F/kubernetes/#advanced-principles","text":"In this section we describe how to set up a more advanced Kubernetes infrastructure, as the Kubernetes infrastructure described above does not restrict access to the API server, cannot be accessed easily from outside of the cluster and does not have persistent data storage.","title":"Advanced Principles"},{"location":"Legacy/Handover/2020F/kubernetes/#control-access-to-api-server","text":"The default access control in Kubernetes allows anyone to perform any request on the API server. This is problematic if anyone untrusted is on the same network as the API server. Fortunately Kubernetes has a rich framework for controlling access to the API server. When making a request to the API server, the request runs through several stages that control access: Authentication: Decides the identity of the requester. Authorization: Decides whether the identity has permissions to make the request. Admission control: Decides if the request complies with various rules. Authentication of API requests is divided into two categories by the type of the requester, Service Accounts and normal users. Service Accounts allow pods to communicate with the API server. Kubernetes automatically creates Service Accounts and mounts Service Account credential files into the pods. With the credential files requests can be performed. Normal users on the other hand are not managed by Kubernetes. They are used to access the API from frontend tools such as the Kubernetes command line tool kubectl or a Web UI. There are several ways to authenticate normal users. Using certificates, bearer tokens, an authenticating proxy, or HTTP basic authentication. Authorization of API requests can happen in four different modes: ABAC: Attribute-based access control authorizes using policies that combine attributes of users and cluster objects into boolean expressions. As of Kubernetes 1.6 this mode is considered deprecated as it is difficult to manage and understand. RBAC: Role-based access control authorizes using role groups that expand its members permissions. Webhook: Authorize with HTTP callbacks. This mode is probably deprecated since its documentation page is deleted. Custom: Authorize with a custom mode. Admission control restricts API requests by intercepting the requests in various plug-ins. Access for a request is only allowed if all plug-ins accept it. Here is a list of commonly used plug-ins: NamespaceLifecycle: Rejects a request, if it tries to create objects in a terminating Kubernetes namespace. (Kubernetes divides cluster objects into namespaces) NamespaceExists: Rejects a request, if it tries to create cluster objects in nonexistent namespaces.* LimitRanger: Rejects a request, if it demands CPU and memory resources for a pod, that is beyond CPU and memory limit policies on a namespace. ResourceQuota: Rejects a request, if it demands CPU and memory resources for a namespace, that is beyond demand resource quotas for a namespace. SecurityContextDeny: Reject a request, if it attempts to change SecurityContext fields, that allows privilege escalation. ServiceAccount: Accepts all requests. The plug-in makes sure that a Service Account is always available on pods. Setting up these controls is done when configuring the API server. The following arguments should be included in the configuration file, under KUBE\\_API\\_ARGS: client-ca: This argument ensures that any client presenting a signed certificate is authenticated as per the CommonName attribute in that certificate. The client-ca-file contains a list of authorities that can sign certificates. tls-cert: Points to the certificates used to provide HTTPS serving. If no certificates are specified (using --tls-private-key), it generates and uses self-signed ones automatically. tls-private-key Specifies the private key used for HTTPS serving, which should match the certificate file (specified using --tls-cert-file). tls-cert-file Specifies the x509 certificate used for HTTPS, which should match the private key for the same (specified using --tls-private-key). service-account-key Specifies the PEM-encoded x509 RSA or ECDSA private or public key file, for verifying ServiceAccount tokens.","title":"Control access to API server"},{"location":"Legacy/Handover/2020F/kubernetes/#external-access","text":"In this section we explain how it's possible to externally access a cluster and routing domains to applications in a cluster. To make a cluster and routing domains to applications at the same time, we needed to use what's called an Ingress controller. An Ingress controller exposes a Kubernetes service so it is accessible outside the cluster network. An Ingress controller created with a manifest file, by specifying the kind attribute as Ingress. Kubernetes doesn't provide a default Ingress implementation, it only defines an interface where other people can then write their own implementation. The following Ingress controller implementations were investigated: Cloud implementation: Large cloud providers such as Microsoft, Amazon and Google provide Ingress controllers specific to their clouds. These Ingress controllers are not usable in self-hosted Kubernetes clusters. Nginx implementation: This Ingress controller implementation is part of the official Kubernetes git repositories. It uses the Nginx reverse proxy capabilities to direct incoming traffic. Nginx is lightweight and has a rich dsl, but it requires additional configuration to work with Kubernetes. When using ssl encryption service kube-lego can be used to automatically update ssl certificates. Trafik implementation: Trafik is a reverse proxy that integrates with several different container orchestration technology such as Swarm, Mesos and Kubernetes. It provides a web interface to display and configure its configuration. It is well integrated into Kubernetes, so Ingress rules works without additional configuration. Trafik also provides fully automated administration of ssl encryption. In the picture below it is illustrated how Trafik works. The blue arrows represent HTTP request to various domains, the green arrows represent how Trafik redirects the request to services and the red arrow shows that Trafik is controlled the cloud orchestration technology.","title":"External Access"},{"location":"Legacy/Handover/2020F/kubernetes/#persistent-storage","text":"In this section we explain how persistent storage is achieved with Kubernetes. In Kubernetes there are two concepts when dealing with storage, which are described as the following. We have our information about storage in Kubernetes from the official documentation. PersistentVolume: A volume that is persisted throughout a pod's lifespan. It is an abstraction over different storage types like nfs, GlusterFS and local hosted storage. It is recommended to use network storage with a persistent volume and not local hosted storage, as local hosted storage option was only introduced for testing a Kubernetes set up. On its own a persistent volume cannot be used by a pod, a pod must have a claim for a persistent volume before it can be used. PersistentVolumeClaim: A claim for a persistent volume. If a pod needs to use storage, then it has to use a claim. In the picture above we show a 100 GB PersistentVolume (PV) for a 200 GB nfs storage, where two 20 GB PersistentVolumeClaims are claiming 20 GB storage each. This illustrates a PersistentVolume as a finite resource, which can be used to set a limit on how much storage there can be used from an external storage. A PersistentVolumeClaim has to be used if a pod needs to use a part of the PersistentVolume's storage. This model means an external physical storage can be partitioned into multiple PersistentVolumes, which then can also be partitioned into multiple PersistentVolumeClaim. In the PersistentVolume code we show an example of a PersistentVolume manifest file. This manifest file specifically describes a PersistentVolume for nfs. The essential parts of the manifest files are the capacity, accessModes and nfs labels. The capacity contains the definition of how much storage our PersistentVolume can use. The accessModes label contains the definition of the cardinality between the storage and nodes, which can be the following. ReadWriteOnce: The volume can be mounted as read-write by a single node. ReadOnlyMany: The volume can be mounted read-only by many nodes. ReadWriteMany: The volume can be mounted as read-write by many nodes. The last attribute is the nfs attribute. This defines the address of the server hosting the nfs storage, the path to the nfs folder on the server and whether or not the folder is readonly.","title":"Persistent Storage"},{"location":"Legacy/Handover/2020F/kubernetes/#code-persistentvolume-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : v1 kind : PersistentVolume metadata : name : ci labels : volume : ci spec : capacity : storage : 10Gi accessModes : ReadWriteMany nfs : server : master.giraf.cs.aau.dk path : /var/nfsshare readOnly : false In the PersistentVolumeClaim code we show an example of a PersistentVolumeClaim, which can claim a part of the PersistentVolume shown in the PersistentVolume code. To ensure a PersistentVolumeClaim is a claim for a specific PersistentVolume, a selector has to be used. A selector can filter what PersistentVolume a PersistentVolumeClaim can be matched with, by using the label defined in one or more PersistentVolumes. In this example we have a label called volume: ci in the PersistentVolume, which the selector in the PersistentVolumeClaim then match on, such that the PersistentVolumeClaim will be assigned only to a PersistentVolume with the same label and sufficient space. If a selector is not specified, the PersistentVolumeClaim will be matched based on its access mode to any PersistentVolume with sufficient space to host the PersistentVolumeClaim.","title":"Code: PersistentVolume example"},{"location":"Legacy/Handover/2020F/kubernetes/#code-persistentvolumeclaim-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : v1 kind : PersistentVolumeClaim metadata : name : artifactory-pvc spec : selector : matchLabels : volume : ci accessModes : ReadWriteMany resources : requests : storage : 5Gi After defining a PersistentVolumeClaim it is then possible to use it from a deployment manifest file. In the Deployment code we show an example of how volumes are used in a deployment. In this example we have added two new specs which are volumeMounts (lines 19-22) and volumes (lines 23-26). The attribute volumes describes that we want to make a volume named artifactory-vol which uses the PersistentVolumeClaim artifactory-pvc (the PersistentVolumeClaim shown in the PersistentVulmeClaim code). Then we use the volumeMounts to specify where we want to mount artifactory-vol, the mountPath is where we mount our volume inside the container and subPath is the name of the folder, in our nfs storage, we want to mount at the mountPath.","title":"Code: PersistentVolumeClaim example"},{"location":"Legacy/Handover/2020F/kubernetes/#code-deployment-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion : extensions/v1beta1 kind : Deployment metadata : name : artifactory labels : app : artifactory spec : replicas : 1 template : metadata : labels : app : artifactory spec : containers : name : artifactory image : docker.bintray.io/jfrog/artifactory-oss:latest ports : containerPort : 8081 volumeMounts : name : artifactory-vol mountPath : /var/opt/jfrog/artifactory subPath : artifactory volumes : name : artifactory-vol persistentVolumeClaim : claimName : artifactory-pvc","title":"Code: Deployment example"},{"location":"Legacy/Handover/2020F/ownership_transfer/","text":"Ownership Transfer \u00b6 This document will describe how the next generation of GIRAF developers can get ownership of the different services. GIRAF Passwords \u00b6 All relevant passwords should be found in the password manager KeePass . Contact Ulrik Nyman ( ulrik@cs.aau.dk ) to get access to the it. The manager works by loading a file ( Giraf_Logins.kdbx ) and using the master password. Remember to have the file available some place public (eg. Drive) so all members can get access if they need to. Any changes made means that the file will have to be re-uploaded. We recommend that the process group manages the passwords and the file. GitHub \u00b6 To get access to the GitHub repositories, an owner needs to add the new members. Use the following credentials to login and add all the relevant members. Username: giraf@lists.aau.dk Password: see KeePass Google Play Store \u00b6 To access the Google Play Store use the following credentials. Email: aaugiraf@gmail.com Password: see KeePass App Store \u00b6 To access the App Store use the following credentials. Email: giraf@lists.aau.dk Password: see KeePass Google Drive \u00b6 To access the Google Drive use the same credentials as for the Google Play Store . Email: aaugiraf@gmail.com Password: see KeePass The Drive contains, among other things of interest, a dump of the Google Drive used by GIRAF 2020F Server Access \u00b6 The server-meta group must compile a list of student emails who needs sudo access to the server. The list should be sent to support@its.aau.dk after Semester Coordinater Ulrik Nyman ( ulrik@cs.aau.dk ) has approved the list. More information can be found here Portainer Access \u00b6 Portainer allows the visualization of the structure for the docker images. It can be helpful to have an UI instead of working in a terminal. Note that you have to be on the AAU network or use the AAU VPN to access it. URL: 192.38.56.151:9000 Username: admin Password: see KeePass","title":"Ownership Transfer"},{"location":"Legacy/Handover/2020F/ownership_transfer/#ownership-transfer","text":"This document will describe how the next generation of GIRAF developers can get ownership of the different services.","title":"Ownership Transfer"},{"location":"Legacy/Handover/2020F/ownership_transfer/#giraf-passwords","text":"All relevant passwords should be found in the password manager KeePass . Contact Ulrik Nyman ( ulrik@cs.aau.dk ) to get access to the it. The manager works by loading a file ( Giraf_Logins.kdbx ) and using the master password. Remember to have the file available some place public (eg. Drive) so all members can get access if they need to. Any changes made means that the file will have to be re-uploaded. We recommend that the process group manages the passwords and the file.","title":"GIRAF Passwords"},{"location":"Legacy/Handover/2020F/ownership_transfer/#github","text":"To get access to the GitHub repositories, an owner needs to add the new members. Use the following credentials to login and add all the relevant members. Username: giraf@lists.aau.dk Password: see KeePass","title":"GitHub"},{"location":"Legacy/Handover/2020F/ownership_transfer/#google-play-store","text":"To access the Google Play Store use the following credentials. Email: aaugiraf@gmail.com Password: see KeePass","title":"Google Play Store"},{"location":"Legacy/Handover/2020F/ownership_transfer/#app-store","text":"To access the App Store use the following credentials. Email: giraf@lists.aau.dk Password: see KeePass","title":"App Store"},{"location":"Legacy/Handover/2020F/ownership_transfer/#google-drive","text":"To access the Google Drive use the same credentials as for the Google Play Store . Email: aaugiraf@gmail.com Password: see KeePass The Drive contains, among other things of interest, a dump of the Google Drive used by GIRAF 2020F","title":"Google Drive"},{"location":"Legacy/Handover/2020F/ownership_transfer/#server-access","text":"The server-meta group must compile a list of student emails who needs sudo access to the server. The list should be sent to support@its.aau.dk after Semester Coordinater Ulrik Nyman ( ulrik@cs.aau.dk ) has approved the list. More information can be found here","title":"Server Access"},{"location":"Legacy/Handover/2020F/ownership_transfer/#portainer-access","text":"Portainer allows the visualization of the structure for the docker images. It can be helpful to have an UI instead of working in a terminal. Note that you have to be on the AAU network or use the AAU VPN to access it. URL: 192.38.56.151:9000 Username: admin Password: see KeePass","title":"Portainer Access"},{"location":"Legacy/Handover/2020F/tools/","text":"Process Group Tools \u00b6 Review Sheet \u00b6 In order to keep track of who should review what, we created a review sheet based on the 2019 process group . Our template for the review sheet can be found here . Open the template, go to \"File\" -> \"Make a copy\" and save it to your own drive. To setup the sheet script: Go to \"Tools\" -> \"Script editor\". If the script is already there go to step (4) If the script is not there, copy the script from here and paste it in. Generate an access token for the AAUGiraf GitHub account, and set token variable. The review sheet has the following functionality: Load pull request information from GitHub. Calculate line change per pull request and total for each development team. Warn about missing pull requests, and provide an overview of them. Review Script \u00b6 In order to make it faster and easier for us to hand out reviews, we created a python script to do most of it for us. The script has to be setup before using it. When it is setup, it can be used to: Tag teams on discord in the #review channel, Add the teams as reviewers on the pull request and Add the review checklist as a comment on the pull request. All in one run. The review script can be found here GitHub Webhooks \u00b6 In order to keep track of what happens in the GitHub repositories, we created webhooks to send a message on our Discord server in a #github channel. This can also be done with Slack. We recommend making the channel opt-in (in Discord by using a role), in order to not spam the members. A webhook can be on GitHub for each relevant repository. For example, for the weekplanner at: https://github.com/aau-giraf/weekplanner/settings/hooks","title":"Process Group Tools"},{"location":"Legacy/Handover/2020F/tools/#process-group-tools","text":"","title":"Process Group Tools"},{"location":"Legacy/Handover/2020F/tools/#review-sheet","text":"In order to keep track of who should review what, we created a review sheet based on the 2019 process group . Our template for the review sheet can be found here . Open the template, go to \"File\" -> \"Make a copy\" and save it to your own drive. To setup the sheet script: Go to \"Tools\" -> \"Script editor\". If the script is already there go to step (4) If the script is not there, copy the script from here and paste it in. Generate an access token for the AAUGiraf GitHub account, and set token variable. The review sheet has the following functionality: Load pull request information from GitHub. Calculate line change per pull request and total for each development team. Warn about missing pull requests, and provide an overview of them.","title":"Review Sheet"},{"location":"Legacy/Handover/2020F/tools/#review-script","text":"In order to make it faster and easier for us to hand out reviews, we created a python script to do most of it for us. The script has to be setup before using it. When it is setup, it can be used to: Tag teams on discord in the #review channel, Add the teams as reviewers on the pull request and Add the review checklist as a comment on the pull request. All in one run. The review script can be found here","title":"Review Script"},{"location":"Legacy/Handover/2020F/tools/#github-webhooks","text":"In order to keep track of what happens in the GitHub repositories, we created webhooks to send a message on our Discord server in a #github channel. This can also be done with Slack. We recommend making the channel opt-in (in Discord by using a role), in order to not spam the members. A webhook can be on GitHub for each relevant repository. For example, for the weekplanner at: https://github.com/aau-giraf/weekplanner/settings/hooks","title":"GitHub Webhooks"},{"location":"Legacy/Handover/Review_Checklists/","text":"Overview \u00b6 These are the checklists used for code review during development on GIRAF.","title":"Overview"},{"location":"Legacy/Handover/Review_Checklists/#overview","text":"These are the checklists used for code review during development on GIRAF.","title":"Overview"},{"location":"Legacy/Handover/Review_Checklists/2020E/","text":"Overview \u00b6 These checklists were used for the GitHub repositories that their names correspond to. They were used by the Pull Request Overview tool and the Individual Pull Request tool. The checklists are: weekplanner web-api api_client wiki","title":"Overview"},{"location":"Legacy/Handover/Review_Checklists/2020E/#overview","text":"These checklists were used for the GitHub repositories that their names correspond to. They were used by the Pull Request Overview tool and the Individual Pull Request tool. The checklists are: weekplanner web-api api_client wiki","title":"Overview"},{"location":"Legacy/Handover/Review_Checklists/2020E/api_client/","text":"api_client \u00b6 Code Design \u00b6 The code is in the right place? (Both in terms of folder structure and class structure) The code is not over-engineered. Examples of over-engineering: Implemented behavior for future problems This code could not, to the best of my knowledge, have reused existing code The code does not introduce functionality that is unnecessary for solving the problem Code Readability \u00b6 Names are meaningful and self-documenting It is understandable, by reading the code, what it does The code is simple and readable. i.e. it contains no \\\"hack\\\", or obscure solution Code Maintainability \u00b6 Has all the added code been properly commented Only check one of the items below: The functionality does not need to be covered by tests The functionality needs to be covered by tests Only check these if the functionality needs to be covered by tests: All functionality is covered by tests The tests has sensible names By reading the tests you know what they cover The tests cover sensible cases. Both happy paths and exception paths The tests cover the full functionality. i.e., The tests fails if some of the requested functionality is missing","title":"api_client"},{"location":"Legacy/Handover/Review_Checklists/2020E/api_client/#api_client","text":"","title":"api_client"},{"location":"Legacy/Handover/Review_Checklists/2020E/api_client/#code-design","text":"The code is in the right place? (Both in terms of folder structure and class structure) The code is not over-engineered. Examples of over-engineering: Implemented behavior for future problems This code could not, to the best of my knowledge, have reused existing code The code does not introduce functionality that is unnecessary for solving the problem","title":"Code Design"},{"location":"Legacy/Handover/Review_Checklists/2020E/api_client/#code-readability","text":"Names are meaningful and self-documenting It is understandable, by reading the code, what it does The code is simple and readable. i.e. it contains no \\\"hack\\\", or obscure solution","title":"Code Readability"},{"location":"Legacy/Handover/Review_Checklists/2020E/api_client/#code-maintainability","text":"Has all the added code been properly commented Only check one of the items below: The functionality does not need to be covered by tests The functionality needs to be covered by tests Only check these if the functionality needs to be covered by tests: All functionality is covered by tests The tests has sensible names By reading the tests you know what they cover The tests cover sensible cases. Both happy paths and exception paths The tests cover the full functionality. i.e., The tests fails if some of the requested functionality is missing","title":"Code Maintainability"},{"location":"Legacy/Handover/Review_Checklists/2020E/web-api/","text":"web-api \u00b6 Code Design \u00b6 The code is in the right place? (Both in terms of folder structure and class structure) The code is not over-engineered. Examples of over-engineering: Implemented behavior for future problems This code could not, to the best of my knowledge, have reused existing code The code does not introduce functionality that is unnecessary for solving the problem Code Readability \u00b6 Names are meaningful and self-documenting It is understandable, by reading the code, what it does The code is simple and readable. i.e. it contains no \\\"hack\\\", or obscure solution Code Maintainability \u00b6 Has all the added code been properly commented Only check one of the items below: The functionality does not need to be covered by tests The functionality needs to be covered by tests Only check these if the functionality needs to be covered by tests: All functionality is covered by tests The tests has sensible names By reading the tests you know what they cover The tests cover sensible cases. Both happy paths and exception paths The tests cover the full functionality. i.e., The tests fails if some of the requested functionality is missing Functionality \u00b6 For this part, run the web-api locally as described in https://aau-giraf.github.io/wiki/development/rest_api_development/BuildAndRunLocally/ The web-api runs without issues The functionality that the code claims to implement, are actually implemented The integration tests runs successfully. Refer to https://github.com/aau-giraf/web-api/tree/master/GirafIntegrationTest for how to run the tests","title":"web-api"},{"location":"Legacy/Handover/Review_Checklists/2020E/web-api/#web-api","text":"","title":"web-api"},{"location":"Legacy/Handover/Review_Checklists/2020E/web-api/#code-design","text":"The code is in the right place? (Both in terms of folder structure and class structure) The code is not over-engineered. Examples of over-engineering: Implemented behavior for future problems This code could not, to the best of my knowledge, have reused existing code The code does not introduce functionality that is unnecessary for solving the problem","title":"Code Design"},{"location":"Legacy/Handover/Review_Checklists/2020E/web-api/#code-readability","text":"Names are meaningful and self-documenting It is understandable, by reading the code, what it does The code is simple and readable. i.e. it contains no \\\"hack\\\", or obscure solution","title":"Code Readability"},{"location":"Legacy/Handover/Review_Checklists/2020E/web-api/#code-maintainability","text":"Has all the added code been properly commented Only check one of the items below: The functionality does not need to be covered by tests The functionality needs to be covered by tests Only check these if the functionality needs to be covered by tests: All functionality is covered by tests The tests has sensible names By reading the tests you know what they cover The tests cover sensible cases. Both happy paths and exception paths The tests cover the full functionality. i.e., The tests fails if some of the requested functionality is missing","title":"Code Maintainability"},{"location":"Legacy/Handover/Review_Checklists/2020E/web-api/#functionality","text":"For this part, run the web-api locally as described in https://aau-giraf.github.io/wiki/development/rest_api_development/BuildAndRunLocally/ The web-api runs without issues The functionality that the code claims to implement, are actually implemented The integration tests runs successfully. Refer to https://github.com/aau-giraf/web-api/tree/master/GirafIntegrationTest for how to run the tests","title":"Functionality"},{"location":"Legacy/Handover/Review_Checklists/2020E/weekplanner/","text":"weekplanner \u00b6 Assets \u00b6 The environments.json file has the following default values: 1 2 3 4 5 6 { \"SERVER_HOST\": \"https://srv.giraf.cs.aau.dk/DEV/API\", \"DEBUG\": true, \"USERNAME\": \"Graatand\", \"PASSWORD\": \"password\" } Code Design \u00b6 The code is in the right place? (Both in terms of folder structure and class structure) The code is not over-engineered. Examples of over-engineering: Implemented behavior for future problems This code could not, to the best of my knowledge, have reused existing code The code does not introduce functionality that is unnecessary for solving the problem Code Readability \u00b6 Names are meaningful and self-documenting It is understandable, by reading the code, what it does The code is simple and readable. i.e. it contains no \\\"hack\\\", or obscure solution Code Maintainability \u00b6 Has all the added code been properly commented Only check one of the items below: The functionality does not need to be covered by tests The functionality needs to be covered by tests Only check these if the functionality needs to be covered by tests: All functionality is covered by tests The tests has sensible names By reading the tests you know what they cover The tests cover sensible cases. Both happy paths and exception paths The tests cover the full functionality. i.e., The tests fails if some of the requested functionality is missing Functionality \u00b6 For this part, open the app, and test: The features that the code claims to implement, are actually implemented The code does not look like it has bugs All the new screens are reachable through in-app navigation","title":"weekplanner"},{"location":"Legacy/Handover/Review_Checklists/2020E/weekplanner/#weekplanner","text":"","title":"weekplanner"},{"location":"Legacy/Handover/Review_Checklists/2020E/weekplanner/#assets","text":"The environments.json file has the following default values: 1 2 3 4 5 6 { \"SERVER_HOST\": \"https://srv.giraf.cs.aau.dk/DEV/API\", \"DEBUG\": true, \"USERNAME\": \"Graatand\", \"PASSWORD\": \"password\" }","title":"Assets"},{"location":"Legacy/Handover/Review_Checklists/2020E/weekplanner/#code-design","text":"The code is in the right place? (Both in terms of folder structure and class structure) The code is not over-engineered. Examples of over-engineering: Implemented behavior for future problems This code could not, to the best of my knowledge, have reused existing code The code does not introduce functionality that is unnecessary for solving the problem","title":"Code Design"},{"location":"Legacy/Handover/Review_Checklists/2020E/weekplanner/#code-readability","text":"Names are meaningful and self-documenting It is understandable, by reading the code, what it does The code is simple and readable. i.e. it contains no \\\"hack\\\", or obscure solution","title":"Code Readability"},{"location":"Legacy/Handover/Review_Checklists/2020E/weekplanner/#code-maintainability","text":"Has all the added code been properly commented Only check one of the items below: The functionality does not need to be covered by tests The functionality needs to be covered by tests Only check these if the functionality needs to be covered by tests: All functionality is covered by tests The tests has sensible names By reading the tests you know what they cover The tests cover sensible cases. Both happy paths and exception paths The tests cover the full functionality. i.e., The tests fails if some of the requested functionality is missing","title":"Code Maintainability"},{"location":"Legacy/Handover/Review_Checklists/2020E/weekplanner/#functionality","text":"For this part, open the app, and test: The features that the code claims to implement, are actually implemented The code does not look like it has bugs All the new screens are reachable through in-app navigation","title":"Functionality"},{"location":"Legacy/Handover/Review_Checklists/2020E/wiki/","text":"wiki \u00b6 Documentation \u00b6 The documentation is well structured. The documentation is grammatically correct. The documentation is understandable. The documentation is not missing information. Rendering the Wiki \u00b6 There are no dead links or references. The markdown renders correctly in MkDocs. The content is placed in the correct page/folder/section.","title":"wiki"},{"location":"Legacy/Handover/Review_Checklists/2020E/wiki/#wiki","text":"","title":"wiki"},{"location":"Legacy/Handover/Review_Checklists/2020E/wiki/#documentation","text":"The documentation is well structured. The documentation is grammatically correct. The documentation is understandable. The documentation is not missing information.","title":"Documentation"},{"location":"Legacy/Handover/Review_Checklists/2020E/wiki/#rendering-the-wiki","text":"There are no dead links or references. The markdown renders correctly in MkDocs. The content is placed in the correct page/folder/section.","title":"Rendering the Wiki"},{"location":"Legacy/Handover/Review_Checklists/2020F/review_checklist_code/","text":"Review Checklist for Code \u00b6 @aau-giraf/team you have been assigned this. Here's your checklist. Please use the following checklist. Feel free to check it off as you go along. Code Design \u00b6 The code is in the right place? (Both in terms of folder structure and class structure) This code could not, to the best of my knowledge, have reused existing code The code is not over-engineered. Examples of over-engineering: Implemented behavior for future problems The code does it introduce functionality that is not necessary for solving the problem Code Readability and Maintainability \u00b6 Names are meaningful and self-documenting It is understandable, by reading the code, what it does The code is simple and readable. i.e. it contains no \"hack\", or obscure solution All new functionality is covered by tests It is understandable, by reading the tests, what they do The tests covers sensible cases. Both happy paths and exception paths The tests cover the full functionality. i.e., The tests fails if some of the requested functionality is missing Error messages in the code are understandable Confusing sections of code are covered by comments Functionality \u00b6 For this part, open the app, and test: The features that the code claims to implement, are actually implemented The code does not look like it has bugs All the new screens are reachable through in-app navigation Points for Discussion and Reflection \u00b6 UI messages, that the user might meet, are actually helpful There are no errors that are bad practice. E.g. hard-coding a test database connection There is well written code in this. Remember to tell the developer!","title":"Review Checklist for Code"},{"location":"Legacy/Handover/Review_Checklists/2020F/review_checklist_code/#review-checklist-for-code","text":"@aau-giraf/team you have been assigned this. Here's your checklist. Please use the following checklist. Feel free to check it off as you go along.","title":"Review Checklist for Code"},{"location":"Legacy/Handover/Review_Checklists/2020F/review_checklist_code/#code-design","text":"The code is in the right place? (Both in terms of folder structure and class structure) This code could not, to the best of my knowledge, have reused existing code The code is not over-engineered. Examples of over-engineering: Implemented behavior for future problems The code does it introduce functionality that is not necessary for solving the problem","title":"Code Design"},{"location":"Legacy/Handover/Review_Checklists/2020F/review_checklist_code/#code-readability-and-maintainability","text":"Names are meaningful and self-documenting It is understandable, by reading the code, what it does The code is simple and readable. i.e. it contains no \"hack\", or obscure solution All new functionality is covered by tests It is understandable, by reading the tests, what they do The tests covers sensible cases. Both happy paths and exception paths The tests cover the full functionality. i.e., The tests fails if some of the requested functionality is missing Error messages in the code are understandable Confusing sections of code are covered by comments","title":"Code Readability and Maintainability"},{"location":"Legacy/Handover/Review_Checklists/2020F/review_checklist_code/#functionality","text":"For this part, open the app, and test: The features that the code claims to implement, are actually implemented The code does not look like it has bugs All the new screens are reachable through in-app navigation","title":"Functionality"},{"location":"Legacy/Handover/Review_Checklists/2020F/review_checklist_code/#points-for-discussion-and-reflection","text":"UI messages, that the user might meet, are actually helpful There are no errors that are bad practice. E.g. hard-coding a test database connection There is well written code in this. Remember to tell the developer!","title":"Points for Discussion and Reflection"},{"location":"Legacy/Handover/Review_Checklists/2020F/review_checklist_wiki/","text":"Review Checklist \u00b6 @aau-giraf/team you have been assigned this. Here's your checklist. Please use the following checklist: Code Documentation Only \u00b6 It is documented which BLoCs/Screens the current class is related to. The responsibility/functionality of the current class is explained. Also regarding the classes it is related to. If any unusual decisions are introduced in the code, they are reasoned about in the documentation. Wiki Related \u00b6 There are no dead links or references. The markdown renders correctly in MkDocs. The content is placed in the correct page/folder/section. You can run MkDocs locally by following this guide .","title":"Review Checklist"},{"location":"Legacy/Handover/Review_Checklists/2020F/review_checklist_wiki/#review-checklist","text":"@aau-giraf/team you have been assigned this. Here's your checklist. Please use the following checklist:","title":"Review Checklist"},{"location":"Legacy/Handover/Review_Checklists/2020F/review_checklist_wiki/#code-documentation-only","text":"It is documented which BLoCs/Screens the current class is related to. The responsibility/functionality of the current class is explained. Also regarding the classes it is related to. If any unusual decisions are introduced in the code, they are reasoned about in the documentation.","title":"Code Documentation Only"},{"location":"Legacy/Handover/Review_Checklists/2020F/review_checklist_wiki/#wiki-related","text":"There are no dead links or references. The markdown renders correctly in MkDocs. The content is placed in the correct page/folder/section. You can run MkDocs locally by following this guide .","title":"Wiki Related"},{"location":"Legacy/Releases/","text":"Overview \u00b6 This section covers all legacy GIRAF files dating from 2018-2020 related to the weekplanner application. As of 2025 the weekplanner is retired and no longer a part of the active GIRAF projects. It therefore serves as inspiration for development of the current (2025) applications. The following pages in this section describes the overall changes made for each release. Some of the links in the releases from 2018 does not work, as GitLab is not in use as of 2019. Releases Released on 2020ES3R1 04-12-2020 2020FS3R1 19-05-2020 2020FS2R1 01-05-2020 2020FS1R1 03-04-2020 2019S3R1 26-04-2019 2019S2R1 05-04-2019 2019s1r1 N/A 2018S4R1 17-05-2018 2018S3R2 08-05-2018","title":"Overview"},{"location":"Legacy/Releases/#overview","text":"This section covers all legacy GIRAF files dating from 2018-2020 related to the weekplanner application. As of 2025 the weekplanner is retired and no longer a part of the active GIRAF projects. It therefore serves as inspiration for development of the current (2025) applications. The following pages in this section describes the overall changes made for each release. Some of the links in the releases from 2018 does not work, as GitLab is not in use as of 2019. Releases Released on 2020ES3R1 04-12-2020 2020FS3R1 19-05-2020 2020FS2R1 01-05-2020 2020FS1R1 03-04-2020 2019S3R1 26-04-2019 2019S2R1 05-04-2019 2019s1r1 N/A 2018S4R1 17-05-2018 2018S3R2 08-05-2018","title":"Overview"},{"location":"Legacy/Releases/release_guide/","text":"Release Guide \u00b6 This guide gives an overview of what to do in the Wiki when making a new release description for GIRAF. It is the PO group's responsibility to make this addition to the Wiki. Placement in Wiki \u00b6 Below is an illustration of where to place a new release description. 1 2 3 4 5 6 7 8 9 10 11 12 Releases \u251c\u2500\u2500 Release Guide \u251c\u2500\u2500 Semester name # New section \u2502 \u251c\u2500\u2500 Release 2 # \u2502 \u251c\u2500\u2500 Release 1 # \u2502 \u2514\u2500\u2500 ... # \u251c\u2500\u2500 ... \u251c\u2500\u2500 2020E \u2502 \u2514\u2500\u2500 2020Es3r1 \u251c\u2500\u2500 2020F \u251c\u2500\u2500 2019 \u2514\u2500\u2500 2018 Semester name is the year a semester is held e.g. 2020. 2020 is followed by E or F because GIRAF team was worked on in both the Spring (F) and Autumn (E) semester. The name of a release should be in the following format: 1 <Semester name>s<Sprint no.>r<Release no.> This means that the first release in Sprint 3 in the 2020E semester is called 2020Es3r1 . The newest release should always be the top most in the table of contents e.g. Release 2 is above Release 1 as seen in the illustration. Release Description \u00b6 A release description consists of these two things: A brief summary of the things that has changed in the new release A complete list of closed issues that is a part of the new release Release Overview \u00b6 When a release description is done a link and date has to be added to the Release Overview . The newest release should be placed as the top most in the table.","title":"Release Guide"},{"location":"Legacy/Releases/release_guide/#release-guide","text":"This guide gives an overview of what to do in the Wiki when making a new release description for GIRAF. It is the PO group's responsibility to make this addition to the Wiki.","title":"Release Guide"},{"location":"Legacy/Releases/release_guide/#placement-in-wiki","text":"Below is an illustration of where to place a new release description. 1 2 3 4 5 6 7 8 9 10 11 12 Releases \u251c\u2500\u2500 Release Guide \u251c\u2500\u2500 Semester name # New section \u2502 \u251c\u2500\u2500 Release 2 # \u2502 \u251c\u2500\u2500 Release 1 # \u2502 \u2514\u2500\u2500 ... # \u251c\u2500\u2500 ... \u251c\u2500\u2500 2020E \u2502 \u2514\u2500\u2500 2020Es3r1 \u251c\u2500\u2500 2020F \u251c\u2500\u2500 2019 \u2514\u2500\u2500 2018 Semester name is the year a semester is held e.g. 2020. 2020 is followed by E or F because GIRAF team was worked on in both the Spring (F) and Autumn (E) semester. The name of a release should be in the following format: 1 <Semester name>s<Sprint no.>r<Release no.> This means that the first release in Sprint 3 in the 2020E semester is called 2020Es3r1 . The newest release should always be the top most in the table of contents e.g. Release 2 is above Release 1 as seen in the illustration.","title":"Placement in Wiki"},{"location":"Legacy/Releases/release_guide/#release-description","text":"A release description consists of these two things: A brief summary of the things that has changed in the new release A complete list of closed issues that is a part of the new release","title":"Release Description"},{"location":"Legacy/Releases/release_guide/#release-overview","text":"When a release description is done a link and date has to be added to the Release Overview . The newest release should be placed as the top most in the table.","title":"Release Overview"},{"location":"Legacy/Releases/2018/2018s3r1/","text":"2018s3r1 \u00b6 Release Schedule \u00b6 Deadline Date Document Notes Release requirements released Mon 2018-04-16 Requirements API development finished Mon 2018-04-23 App/Frontend development finished Thu 2018-04-26 PO approves that requirements are met Mon 2018-04-30 Release Tue 2018-05-01 See the requirements document for a description of what must be implemented before the release can be considered finished. Release \u00b6 Component Source code Weekplanner app https://github.com/aau-giraf/weekplanner/releases/tag/2018S3R1 RestAPI https://github.com/aau-giraf/web-api/releases/tag/2018S3R1 If the source code links for some reason don't work. Look for the release tag 2018S3R1 in wherever the repository is now located. Requirements \u00b6 Scheduled for releases on Tuesday 1st of May 2018 This release is a little special. We are trying out a new four week development and release cycle. The only requirements for this release is that it is in a working state, bug free and able to be released on time.","title":"2018s3r1"},{"location":"Legacy/Releases/2018/2018s3r1/#2018s3r1","text":"","title":"2018s3r1"},{"location":"Legacy/Releases/2018/2018s3r1/#release-schedule","text":"Deadline Date Document Notes Release requirements released Mon 2018-04-16 Requirements API development finished Mon 2018-04-23 App/Frontend development finished Thu 2018-04-26 PO approves that requirements are met Mon 2018-04-30 Release Tue 2018-05-01 See the requirements document for a description of what must be implemented before the release can be considered finished.","title":"Release Schedule"},{"location":"Legacy/Releases/2018/2018s3r1/#release","text":"Component Source code Weekplanner app https://github.com/aau-giraf/weekplanner/releases/tag/2018S3R1 RestAPI https://github.com/aau-giraf/web-api/releases/tag/2018S3R1 If the source code links for some reason don't work. Look for the release tag 2018S3R1 in wherever the repository is now located.","title":"Release"},{"location":"Legacy/Releases/2018/2018s3r1/#requirements","text":"Scheduled for releases on Tuesday 1st of May 2018 This release is a little special. We are trying out a new four week development and release cycle. The only requirements for this release is that it is in a working state, bug free and able to be released on time.","title":"Requirements"},{"location":"Legacy/Releases/2018/2018s3r2/","text":"2018s3r2 \u00b6 Release Schedule \u00b6 Deadline Date Document Notes Release requirements released Mon 2018-04-16 Requirements API development finished Mon 2018-04-30 App/Frontend development finished Thu 2018-05-03 PO approves that requirements are met Mon 2018-05-07 Release Tue 2018-05-08 See the requirements document for a description of what must be implemented before the release can be considered finished. Release \u00b6 Component Source code Weekplanner app https://github.com/aau-giraf/weekplanner/releases/tag/2018S3R2 RestAPI https://github.com/aau-giraf/web-api/releases/tag/2018S3R2 If the source code links for some reason don't work. Look for the release tag 2018S3R2 in wherever the repository is now located. PO Requirements for release 2018S3R2 - User settings \u00b6 Scheduled for release on Tuesday 8th of May 2018. Relevant user stories \u00b6 T1236 , T1237 , T1239 , T1240 , T1241 , T1242 , T1243 and T1261 Requirements \u00b6 The settings menu as shown in the prototype must be implemented and accessible from the weekschedule view when viewing from the guardian perspective. Furthermore, the ability for citizens to mark activities as completed, and for guardians to mark activities as cancelled, must be included in this release. Following user stories must be implemented: Marking an activity as completed (Task T1221 ) Citizens must be able to mark an activity as completed, using a variety of visual options. These include check marks, making the activity appear grey, removing the activity from the view, and moving them to the right. An activity can be marked as completed by accessing the activity page in citizen mode and pressing a button in this page. This release must include the option to checkmark an activity. Marking an activity as cancelled (Task T1221 ) Guardians must be able to mark an activity as cancelled, using a variety of visual options. These include a red cross, and removing the activity from the view. An activity can be marked as cancelled by accessing the activity page in guardian mode, and pressing a button in this page. This release must include the option to cross an activity off with a red cross. The following functionality must be implemented: When exiting a week schedule, a popup window should warn whether or not the guardian would like to save the changes made to the schedule. The following user settings must be implemented: App theme (Valg af tema) Changing the app theme affects the colours of bars and buttons, but not the days or activities. For this release, four themes must be implemented: red, yellow, green, and the standard blue android. The implementation should prepare for more themes to be added in the future. Week Schedule Colours (Farver p\u00e5 ugeplan) Changing the week schedule theme determines the colours of the day-columns in the weekschedule. The guardian must be able to specify the colour of each day in the schedule. Citizens must as a default use the colour-scheme used by the department. Monday: Green, Tuesday: Purple, Wednesday: Orange, Thursday: Blue, Friday: Yellow, Saturday: Red, Sunday: White. The optimal solution would be to choose from at least 16 predefined colours. This release must include at least 8 different colours, 7 of them being the colours used in the colour-scheme mentioned above.","title":"2018s3r2"},{"location":"Legacy/Releases/2018/2018s3r2/#2018s3r2","text":"","title":"2018s3r2"},{"location":"Legacy/Releases/2018/2018s3r2/#release-schedule","text":"Deadline Date Document Notes Release requirements released Mon 2018-04-16 Requirements API development finished Mon 2018-04-30 App/Frontend development finished Thu 2018-05-03 PO approves that requirements are met Mon 2018-05-07 Release Tue 2018-05-08 See the requirements document for a description of what must be implemented before the release can be considered finished.","title":"Release Schedule"},{"location":"Legacy/Releases/2018/2018s3r2/#release","text":"Component Source code Weekplanner app https://github.com/aau-giraf/weekplanner/releases/tag/2018S3R2 RestAPI https://github.com/aau-giraf/web-api/releases/tag/2018S3R2 If the source code links for some reason don't work. Look for the release tag 2018S3R2 in wherever the repository is now located.","title":"Release"},{"location":"Legacy/Releases/2018/2018s3r2/#po-requirements-for-release-2018s3r2-user-settings","text":"Scheduled for release on Tuesday 8th of May 2018.","title":"PO Requirements for release 2018S3R2 - User settings"},{"location":"Legacy/Releases/2018/2018s3r2/#relevant-user-stories","text":"T1236 , T1237 , T1239 , T1240 , T1241 , T1242 , T1243 and T1261","title":"Relevant user stories"},{"location":"Legacy/Releases/2018/2018s3r2/#requirements","text":"The settings menu as shown in the prototype must be implemented and accessible from the weekschedule view when viewing from the guardian perspective. Furthermore, the ability for citizens to mark activities as completed, and for guardians to mark activities as cancelled, must be included in this release. Following user stories must be implemented: Marking an activity as completed (Task T1221 ) Citizens must be able to mark an activity as completed, using a variety of visual options. These include check marks, making the activity appear grey, removing the activity from the view, and moving them to the right. An activity can be marked as completed by accessing the activity page in citizen mode and pressing a button in this page. This release must include the option to checkmark an activity. Marking an activity as cancelled (Task T1221 ) Guardians must be able to mark an activity as cancelled, using a variety of visual options. These include a red cross, and removing the activity from the view. An activity can be marked as cancelled by accessing the activity page in guardian mode, and pressing a button in this page. This release must include the option to cross an activity off with a red cross. The following functionality must be implemented: When exiting a week schedule, a popup window should warn whether or not the guardian would like to save the changes made to the schedule. The following user settings must be implemented: App theme (Valg af tema) Changing the app theme affects the colours of bars and buttons, but not the days or activities. For this release, four themes must be implemented: red, yellow, green, and the standard blue android. The implementation should prepare for more themes to be added in the future. Week Schedule Colours (Farver p\u00e5 ugeplan) Changing the week schedule theme determines the colours of the day-columns in the weekschedule. The guardian must be able to specify the colour of each day in the schedule. Citizens must as a default use the colour-scheme used by the department. Monday: Green, Tuesday: Purple, Wednesday: Orange, Thursday: Blue, Friday: Yellow, Saturday: Red, Sunday: White. The optimal solution would be to choose from at least 16 predefined colours. This release must include at least 8 different colours, 7 of them being the colours used in the colour-scheme mentioned above.","title":"Requirements"},{"location":"Legacy/Releases/2018/2018s4r1/","text":"2018s4r1 \u00b6 Release Schedule \u00b6 Deadline Date Document Notes Release requirements released Mon 2018-04-30 Requirements API development finished Mon 2018-05-07 App/Frontend development finished Thu 2018-05-10 PO approves that requirements are met Mon 2018-05-14 Release Tue 2018-05-15 See the requirements document for a description of what must be implemented before the release can be considered finished. Release \u00b6 Component Source code Weekplanner app https://github.com/aau-giraf/weekplanner/releases/tag/2018S4R1 RestAPI https://github.com/aau-giraf/web-api/releases/tag/2018S4R1 If the source code links for some reason don't work. Look for the release tag 2018S4R1 in wherever the repository is now located. PO Requirements for release 2018S4R1 - Guardian Quality of Life \u00b6 Scheduled for release on Tuesday 15th of May 2018. Relevant user stories \u00b6 T921 , T1005 , T1219 , T1226 , T1238 , T1244 and T1268 . Requirements \u00b6 This release focusses on the quality of life features for the guardians. The overall idea is to implement functionality that makes it faster and easier for guardians to modify or create schedules. These user stories do not share a common page, such as from the last release, so it is important that they are implemented on the correct page. The requirements are all prioritized. The one beneath is the most important requirement. Copy / move \u00b6 The guardian must be able to move activities on the same day. This is done with a drag & drop functionality. When the guardian drags an activity to another day than the one the activity comes from, they must be prompted whether they want to copy, move, or cancel the current action. Choice-board \u00b6 Choice-board is an option that you can choose an activity to be. It is essentially used for the time-slots where a citizen can choose their own activity. Choosing that an activity is a choice-board will enable adding multiple pictograms to the activity. The activity will have a default \"Choose a pictogram\" pictogram, and when pressed upon in citizen mode, a view will pop up where the citizen can choose between the added pictograms. An example would be if a guardian adds a timeslot for when the citizen has to be active. The guardian chooses pictograms for running, walking, biking, and roller-blading. The citizen must then press the activity for being active, and choose what he/she wants to do. If the citizen chooses running, a pictogram for running will replace the default \"Choose a pictogram\" pictogram. Setting that was not part of release 2018S3R2 \u00b6 The following setting must be part of this release. It is a setting that were excluded from the previous release. Number of days/activities shown in a week (Antal dage/aktiviteter vist for en uge) This setting can be set to a number between 1 and 7. The citizens perspective will show the activities on the chosen number of days forward from the current day. Unless the chosen number exceeds the days left in the week. (say you choose to show 3 days ahead and the day is Saturday, then only Saturday and Sunday will be shown.) Completed tasks must still be shown, just as usual. If 1 day is chosen, then the guardian can choose the amount of tasks that is shown in the single day. For instance, a citizen can handle all activities, but only a single day. Another can handle 5 activities in a single day. Default must be all activities, but the guardian must have the ability to specify the amount of tasks that the citizen is limited to, if necessary. This release must include at least 3, 5, 10, and all activities as options. Portrait mode when a single day is shown \u00b6 If 1-day view is chosen in the setting for citizens, the tablet should enter portrait mode in the weekplanner view. This is the case for all options for single-day view, both with limited and unlimited activities shown. This is dependent on the setting above! Templates \u00b6 The guardian must be able to choose a specific template. This must be placed as a button on the NewSchedulePage / NewScheduleViewModel, where the guardian must be able to choose from a selection of templates. Specify number of days in a schedule \u00b6 The guardian must be able to specify the exact days that the schedule should have, when creating a schedule. This must, as with templates, be placed on the NewSchedulePage / NewScheduleViewModel. This must be implemented with checkboxes. A checkbox for each day that is by default checked, and the guardian can then uncheck the specific days that they do not want to include. Save prompts \u00b6 Whenever the guardian leaves the weekplannerpage or enters citizen mode, they must be prompted and asked whether they want to save any changes. The guardian MUST ONLY be prompted when an unsaved change to the schedule is made. Admin panel \u00b6 The admin panel is where new citizens and guardians are created. The admin panel is a web-based application. The application starts with a login screen, where a department administrator can use their login to gain access. The application must have options to create new citizens for the department that the administrator is in. Furthermore, there must also be an option to create new guardians for the department. There must also be an option to configurate already existing guardians and citizens. Master-detail \u00b6 The master-detail page must be accessible from the weekplannerpage in guardian mode only. It must have navigation to settingspage. It must have the possibility to return to choose citizen, and also the ability to log out of the application.","title":"2018s4r1"},{"location":"Legacy/Releases/2018/2018s4r1/#2018s4r1","text":"","title":"2018s4r1"},{"location":"Legacy/Releases/2018/2018s4r1/#release-schedule","text":"Deadline Date Document Notes Release requirements released Mon 2018-04-30 Requirements API development finished Mon 2018-05-07 App/Frontend development finished Thu 2018-05-10 PO approves that requirements are met Mon 2018-05-14 Release Tue 2018-05-15 See the requirements document for a description of what must be implemented before the release can be considered finished.","title":"Release Schedule"},{"location":"Legacy/Releases/2018/2018s4r1/#release","text":"Component Source code Weekplanner app https://github.com/aau-giraf/weekplanner/releases/tag/2018S4R1 RestAPI https://github.com/aau-giraf/web-api/releases/tag/2018S4R1 If the source code links for some reason don't work. Look for the release tag 2018S4R1 in wherever the repository is now located.","title":"Release"},{"location":"Legacy/Releases/2018/2018s4r1/#po-requirements-for-release-2018s4r1-guardian-quality-of-life","text":"Scheduled for release on Tuesday 15th of May 2018.","title":"PO Requirements for release 2018S4R1 - Guardian Quality of Life"},{"location":"Legacy/Releases/2018/2018s4r1/#relevant-user-stories","text":"T921 , T1005 , T1219 , T1226 , T1238 , T1244 and T1268 .","title":"Relevant user stories"},{"location":"Legacy/Releases/2018/2018s4r1/#requirements","text":"This release focusses on the quality of life features for the guardians. The overall idea is to implement functionality that makes it faster and easier for guardians to modify or create schedules. These user stories do not share a common page, such as from the last release, so it is important that they are implemented on the correct page. The requirements are all prioritized. The one beneath is the most important requirement.","title":"Requirements"},{"location":"Legacy/Releases/2018/2018s4r1/#copy-move","text":"The guardian must be able to move activities on the same day. This is done with a drag & drop functionality. When the guardian drags an activity to another day than the one the activity comes from, they must be prompted whether they want to copy, move, or cancel the current action.","title":"Copy / move"},{"location":"Legacy/Releases/2018/2018s4r1/#choice-board","text":"Choice-board is an option that you can choose an activity to be. It is essentially used for the time-slots where a citizen can choose their own activity. Choosing that an activity is a choice-board will enable adding multiple pictograms to the activity. The activity will have a default \"Choose a pictogram\" pictogram, and when pressed upon in citizen mode, a view will pop up where the citizen can choose between the added pictograms. An example would be if a guardian adds a timeslot for when the citizen has to be active. The guardian chooses pictograms for running, walking, biking, and roller-blading. The citizen must then press the activity for being active, and choose what he/she wants to do. If the citizen chooses running, a pictogram for running will replace the default \"Choose a pictogram\" pictogram.","title":"Choice-board"},{"location":"Legacy/Releases/2018/2018s4r1/#setting-that-was-not-part-of-release-2018s3r2","text":"The following setting must be part of this release. It is a setting that were excluded from the previous release. Number of days/activities shown in a week (Antal dage/aktiviteter vist for en uge) This setting can be set to a number between 1 and 7. The citizens perspective will show the activities on the chosen number of days forward from the current day. Unless the chosen number exceeds the days left in the week. (say you choose to show 3 days ahead and the day is Saturday, then only Saturday and Sunday will be shown.) Completed tasks must still be shown, just as usual. If 1 day is chosen, then the guardian can choose the amount of tasks that is shown in the single day. For instance, a citizen can handle all activities, but only a single day. Another can handle 5 activities in a single day. Default must be all activities, but the guardian must have the ability to specify the amount of tasks that the citizen is limited to, if necessary. This release must include at least 3, 5, 10, and all activities as options.","title":"Setting that was not part of release 2018S3R2"},{"location":"Legacy/Releases/2018/2018s4r1/#portrait-mode-when-a-single-day-is-shown","text":"If 1-day view is chosen in the setting for citizens, the tablet should enter portrait mode in the weekplanner view. This is the case for all options for single-day view, both with limited and unlimited activities shown. This is dependent on the setting above!","title":"Portrait mode when a single day is shown"},{"location":"Legacy/Releases/2018/2018s4r1/#templates","text":"The guardian must be able to choose a specific template. This must be placed as a button on the NewSchedulePage / NewScheduleViewModel, where the guardian must be able to choose from a selection of templates.","title":"Templates"},{"location":"Legacy/Releases/2018/2018s4r1/#specify-number-of-days-in-a-schedule","text":"The guardian must be able to specify the exact days that the schedule should have, when creating a schedule. This must, as with templates, be placed on the NewSchedulePage / NewScheduleViewModel. This must be implemented with checkboxes. A checkbox for each day that is by default checked, and the guardian can then uncheck the specific days that they do not want to include.","title":"Specify number of days in a schedule"},{"location":"Legacy/Releases/2018/2018s4r1/#save-prompts","text":"Whenever the guardian leaves the weekplannerpage or enters citizen mode, they must be prompted and asked whether they want to save any changes. The guardian MUST ONLY be prompted when an unsaved change to the schedule is made.","title":"Save prompts"},{"location":"Legacy/Releases/2018/2018s4r1/#admin-panel","text":"The admin panel is where new citizens and guardians are created. The admin panel is a web-based application. The application starts with a login screen, where a department administrator can use their login to gain access. The application must have options to create new citizens for the department that the administrator is in. Furthermore, there must also be an option to create new guardians for the department. There must also be an option to configurate already existing guardians and citizens.","title":"Admin panel"},{"location":"Legacy/Releases/2018/2018s4r1/#master-detail","text":"The master-detail page must be accessible from the weekplannerpage in guardian mode only. It must have navigation to settingspage. It must have the possibility to return to choose citizen, and also the ability to log out of the application.","title":"Master-detail"},{"location":"Legacy/Releases/2019/2019s1r1/","text":"2019s1r1 \u00b6 There was no release at the end of sprint 1 because we decided to change framework from Xamarin to Flutter two weeks before the sprint ended. This meant that we started from scratch and didn't have anything to release at the end of sprint 1. The last part of sprint 1 was instead spent on getting familiar with the Flutter framework, implementing some simple user stories and getting the core functionality ready so that we could build an application around it.","title":"2019s1r1"},{"location":"Legacy/Releases/2019/2019s1r1/#2019s1r1","text":"There was no release at the end of sprint 1 because we decided to change framework from Xamarin to Flutter two weeks before the sprint ended. This meant that we started from scratch and didn't have anything to release at the end of sprint 1. The last part of sprint 1 was instead spent on getting familiar with the Flutter framework, implementing some simple user stories and getting the core functionality ready so that we could build an application around it.","title":"2019s1r1"},{"location":"Legacy/Releases/2019/2019s2r1/","text":"2019s2r1 \u00b6 These are the user stories that were included in 2019s2r1: Issue ID Developed by Weekplanner#23 Group sw602f19 Weekplanner#43 Group sw610f19 Weekplanner#44 Group sw610f19 Weekplanner#46 Group sw611f19 Weekplanner#47 Group sw609f19 Weekplanner#49 Group sw612f19 Weekplanner#50 Group sw613f19 Weekplanner#55 Group sw612f19 Weekplanner#56 Group sw612f19 Web-API#7 Group sw609f19 After this release the most basic functionality of the app were implemented. The features that were implemented were: The user could log in The user could choose a citizen The user could choose a weekplan belonging to that citizen The chosen week plan with its associated activities were shown on the screen","title":"2019s2r1"},{"location":"Legacy/Releases/2019/2019s2r1/#2019s2r1","text":"These are the user stories that were included in 2019s2r1: Issue ID Developed by Weekplanner#23 Group sw602f19 Weekplanner#43 Group sw610f19 Weekplanner#44 Group sw610f19 Weekplanner#46 Group sw611f19 Weekplanner#47 Group sw609f19 Weekplanner#49 Group sw612f19 Weekplanner#50 Group sw613f19 Weekplanner#55 Group sw612f19 Weekplanner#56 Group sw612f19 Web-API#7 Group sw609f19 After this release the most basic functionality of the app were implemented. The features that were implemented were: The user could log in The user could choose a citizen The user could choose a weekplan belonging to that citizen The chosen week plan with its associated activities were shown on the screen","title":"2019s2r1"},{"location":"Legacy/Releases/2019/2019s3r1/","text":"2019s3r1 \u00b6 These are the user stories that were included in 2019s3r1: Issue ID Developed by Weekplanner#45 Group sw613f19 Weekplanner#51 Group sw608f19 Weekplanner#52 Group sw602f19 Weekplanner#53 Group sw609f19 Weekplanner#75 Group sw613f19 Weekplanner#85 Group sw608f19 Weekplanner#88 Group sw609f19 Weekplanner#92 Group sw610f19 Weekplanner#106 Group sw602f19 Weekplanner#112 Group sw602f19 Weekplanner#125 Group sw610f19 Weekplanner#132 Group sw611f19 Weekplanner#133 Group sw610f19 Weekplanner#137 Group sw609f19 Weekplanner#138 Group sw609f19 Weekplanner#146 Group sw608f19 Weekplanner#149 Group sw608f19 Weekplanner#158 Group sw609f19 Weekplanner#165 Group sw608f19 Weekplanner#166 Group sw609f19 Web-API#6 Group sw612f19 Web-API#9 Group sw611f19 Wiki#34 Group sw612f19 Wiki#35 Group sw612f19 After this release some of the most essential features were implemented. These were the features that were implemented: The user could add a new weekplan The user could add an activity to a weekplan The user could move an activity from one day to another The user could press on an activity to see details about the activity The user could mark an activity as done The user was prompted with a dialog box that required him to type in his password when he wanted to change from citizen to guardian mode Even though the dialog box for switching between citizen and guardian mode, the modes themselves were not implemented yet. So the dialog box did not change the mode.","title":"2019s3r1"},{"location":"Legacy/Releases/2019/2019s3r1/#2019s3r1","text":"These are the user stories that were included in 2019s3r1: Issue ID Developed by Weekplanner#45 Group sw613f19 Weekplanner#51 Group sw608f19 Weekplanner#52 Group sw602f19 Weekplanner#53 Group sw609f19 Weekplanner#75 Group sw613f19 Weekplanner#85 Group sw608f19 Weekplanner#88 Group sw609f19 Weekplanner#92 Group sw610f19 Weekplanner#106 Group sw602f19 Weekplanner#112 Group sw602f19 Weekplanner#125 Group sw610f19 Weekplanner#132 Group sw611f19 Weekplanner#133 Group sw610f19 Weekplanner#137 Group sw609f19 Weekplanner#138 Group sw609f19 Weekplanner#146 Group sw608f19 Weekplanner#149 Group sw608f19 Weekplanner#158 Group sw609f19 Weekplanner#165 Group sw608f19 Weekplanner#166 Group sw609f19 Web-API#6 Group sw612f19 Web-API#9 Group sw611f19 Wiki#34 Group sw612f19 Wiki#35 Group sw612f19 After this release some of the most essential features were implemented. These were the features that were implemented: The user could add a new weekplan The user could add an activity to a weekplan The user could move an activity from one day to another The user could press on an activity to see details about the activity The user could mark an activity as done The user was prompted with a dialog box that required him to type in his password when he wanted to change from citizen to guardian mode Even though the dialog box for switching between citizen and guardian mode, the modes themselves were not implemented yet. So the dialog box did not change the mode.","title":"2019s3r1"},{"location":"Legacy/Releases/2020E/2020es3r1/","text":"2020Es3r1 \u00b6 The goal of this release was to improve the general stability of the application, as well as implementing features requested by the customer. Completed Issues \u00b6 In the following, a list of the issues completed in the different repositories is shown. weekplanner \u00b6 Number Title Solved by PR 703 Timer does not get removed when a timer is running 695 Buggy \"fortryd\" \"aflys\" and \"done\" buttons 683 The function checkServerConnection always throws an error 686 681 As a user I would like the stop button on a timer to be a restart button instead 670 App rejects login credentials before accepting them 669 Periodical bug when deleting week plans 668 Inconsistency in settings menu for citizens 682 660 Unit test bugs 661 641 Changing \"Tegn for udf\u00f8relse\" does actually change the sign, it is always a check-mark 622 Searching pictograms should not be case sensitive 619 Error handling for timer_bloc API 618 Error handling for pictogram_image_bloc API 617 Error handling for new_citizen_bloc API 616 As a guardian I would like to be able to unmark completed activities 615 As a citizen I would like to be able to reenter a completed activity 651 614 Error handling for copy_weekplan_bloc API 613 Error handling for weekplan_bloc API 612 Error handling for activity_bloc API 610 Error handling for weekplan_selector_bloc API 608 Error handling for auth_bloc API 607 Error handling for upload_from_gallery_bloc API 605 Error handling for settings_bloc API 604 Error handling for pictogram_bloc API 603 As a guardian I would like for the app to prompt me for access to the Gallery 600 As a guardian I would like to be able to log out from the \"show activity\" screen 597 Error handling for choose_citizen_bloc API 595 Text overflow in \"V\u00e6lg billede fra galleri\" screen 655 592 \"Overst\u00e5ede Uger\" includes current week 663 591 As a developer i would like for the settings to not be saved in the database 589 When copying activities to another day, leaving and reentering the weekplan messes up the order 588 Activity movement is not saved when leaving and reentering the weekplan 587 Moving activities in a weekplan should not cause the whole weekplan to reload 694 586 Changing the weekplan colors does not work in guardian view 583 When deleting the left-most option in a choiceboard the wrong picture is shown 650 580 Newly created citizen does not show up right away in the list of citizens 590 579 As a user I would like for the UI colors to be consistent 576 As a Guardian i would like to be able to rename choiceboards. 620 573 Expand _username validation to prevent special symbols 575 570 Write custom danish error messages based on the error code 574 568 Wrong error message 563 The plus on \"Tilf\u00f8j choiceboard\" scales down and becomes difficult to find when not using tablets. 596 562 Unit test bugs 564 547 Overwrite dialog when copying week plan is not displayed if the week plan is named based on the year and week for the week plan 542 As a guardian I would like to have an indication that informs me that a \"Brugernavn\" is used when adding new user 540 As a guardian i would like to be able to hide or collapse \"Overst\u00e5ede uger\" 665 538 'Overst\u00e5ede Uger' does not update correctly 656 535 Buggy \"Fortryd\" button on activity 585 531 As a guardian i would like for the text under the pictograms not be in all caps. 529 As a guardian I would like to be able to mark activities as completed 602 527 As a guardian I want the design of disabled buttons to more clearly show that a button is disabled 514 Changing 'Tegn for udf\u00f8relse' does not move to previous screen 582 511 Adding activities to a weekplan and then editing the weekplan, removes the added activities 503 Re-navigating to activity where cancelling the activity was undone only show the 'Fortryd' button screen 502 Update buttons to follow the design guide 499 Refactor the code so we don't use the Observable class 609 498 As a guardian I would like to be able to delete a citizen from the system 497 As a guardian I would like to be able to delete myself from the system 495 Pictogram text is cut off instead of being divided over two lines in portrait mode on 600x1024 screen 559 485 There should be defined a errortext color in the custom colors, and this should be used for all errortext 557 482 When uploading a large image to a new weekplan, the image is not uploaded properly 474 With a lot of activities on a single weekplan, the activities sometimes jump to the bottom of the day when moved 598 471 Missing error message when trying to create a citizen with a username already in use 561 468 Activity timer completion sound is still emitted when you are logged out 462 Timers on activities are deleted when copying activities 444 Marking an activity with a timer as complete and then unmark it causes the activity to be deleted. 441 Make the disable buttons have 40% opacity 439 Lines appear around completed activities in citizen mode 438 As a developer I would want a file with the font sizes for the different screens in the weekplan 599 434 As a guardian I would like a citizen to be able to view two days at a time so that they can see today and tomorrow 566 429 As a guardian, I would like to know when I have written a wrong login fast 653 428 As a guardian, I would like to have an error message when creating a user who already exists 561 425 Periodic errors in asynchronous tests 396 As a guardian I would like to have access to x number of pictograms in offline mode using the local database 384 As a developer i would like the app to be deployed to Google Play when it is merged into master 371 As a guardian i would like to resume multiple canceled activities in the weekplan edit screen 664 364 Warning on memory waste/security issue for API lvl 19 647 352 Timer is not saved when you cancel activity and then undo the cancel 327 Artifacts 308 Visible red line when timer has finished. 594 304 The 'add new weekplan'-button is still available in edit-mode, but does nothing. 560 273 As a guardian I would like to be able to delete pictograms I've uploaded so that I can remove wrongly uploaded / outdated pictograms 606 206 API Exceptions are thrown everywhere where an api call was made previously when the log out button is pressed web-api \u00b6 Number Title Solved by PR 170 As a developer I would like support for renaming activities locally 174 168 DEV server no longer works properly 167 Searching pictograms should not be case sensitive 169 162 As a developer I would like for pictograms to have a GirafUser indicating the creating user, so that I can make sure that only the creating user can delete pictograms. 159 Images should be hashed, and saved in the ImageHash row 173 158 Error messages from English to Danish 154 Fix unit and integration tests and startup problems. 155 123 Tests failing because they refer to a file not created by sample data of the database 114 Creating a new activity to a weekplans date with a pictogram with blank title will result in an exception 161 112 Activity timer disappears after pressing 'Fortryd' after cancelling the activity. 93 As a developer I would like to be able to run the integrations tests independent on the sample-data already being in the database 91 Remove redundant endpoint 176 49 Remove hardcoded values from Controller 47 As a developer, I would like the table Pictograms to not include ImageHash . 19 Updating order of activities when reordering in the weekplan 180 5 Illogical endpoints 172 api_client \u00b6 Number Title Solved by PR 81 Updating order of activities when reordering in the weekplan 86 74 As a developer I would like the pictogram model to have a the Id of the creating user 75 56 Refactor the code so we don't use the Observable class 73 52 Fundamental bug in the offline repository 3 Implement updateIcon in user_api.dart Result \u00b6 Based on the implemented issues, the following have been accomplished: Searching is no longer case sensitive Guardians can unmark completed activities Citizens can reenter completed activities Guardians can log out from the \"Show activity\" screens Moving week plans has been optimized, so it no longer reloads the entire week plan Added new and improved existing notifications when errors occur. \"Overst\u00e5ede uger\" can be collapsed Citizens can view two days at the time Multiple cancelled activities can be resumed at once Furthermore several bugs have been ironed out","title":"2020Es3r1"},{"location":"Legacy/Releases/2020E/2020es3r1/#2020es3r1","text":"The goal of this release was to improve the general stability of the application, as well as implementing features requested by the customer.","title":"2020Es3r1"},{"location":"Legacy/Releases/2020E/2020es3r1/#completed-issues","text":"In the following, a list of the issues completed in the different repositories is shown.","title":"Completed Issues"},{"location":"Legacy/Releases/2020E/2020es3r1/#weekplanner","text":"Number Title Solved by PR 703 Timer does not get removed when a timer is running 695 Buggy \"fortryd\" \"aflys\" and \"done\" buttons 683 The function checkServerConnection always throws an error 686 681 As a user I would like the stop button on a timer to be a restart button instead 670 App rejects login credentials before accepting them 669 Periodical bug when deleting week plans 668 Inconsistency in settings menu for citizens 682 660 Unit test bugs 661 641 Changing \"Tegn for udf\u00f8relse\" does actually change the sign, it is always a check-mark 622 Searching pictograms should not be case sensitive 619 Error handling for timer_bloc API 618 Error handling for pictogram_image_bloc API 617 Error handling for new_citizen_bloc API 616 As a guardian I would like to be able to unmark completed activities 615 As a citizen I would like to be able to reenter a completed activity 651 614 Error handling for copy_weekplan_bloc API 613 Error handling for weekplan_bloc API 612 Error handling for activity_bloc API 610 Error handling for weekplan_selector_bloc API 608 Error handling for auth_bloc API 607 Error handling for upload_from_gallery_bloc API 605 Error handling for settings_bloc API 604 Error handling for pictogram_bloc API 603 As a guardian I would like for the app to prompt me for access to the Gallery 600 As a guardian I would like to be able to log out from the \"show activity\" screen 597 Error handling for choose_citizen_bloc API 595 Text overflow in \"V\u00e6lg billede fra galleri\" screen 655 592 \"Overst\u00e5ede Uger\" includes current week 663 591 As a developer i would like for the settings to not be saved in the database 589 When copying activities to another day, leaving and reentering the weekplan messes up the order 588 Activity movement is not saved when leaving and reentering the weekplan 587 Moving activities in a weekplan should not cause the whole weekplan to reload 694 586 Changing the weekplan colors does not work in guardian view 583 When deleting the left-most option in a choiceboard the wrong picture is shown 650 580 Newly created citizen does not show up right away in the list of citizens 590 579 As a user I would like for the UI colors to be consistent 576 As a Guardian i would like to be able to rename choiceboards. 620 573 Expand _username validation to prevent special symbols 575 570 Write custom danish error messages based on the error code 574 568 Wrong error message 563 The plus on \"Tilf\u00f8j choiceboard\" scales down and becomes difficult to find when not using tablets. 596 562 Unit test bugs 564 547 Overwrite dialog when copying week plan is not displayed if the week plan is named based on the year and week for the week plan 542 As a guardian I would like to have an indication that informs me that a \"Brugernavn\" is used when adding new user 540 As a guardian i would like to be able to hide or collapse \"Overst\u00e5ede uger\" 665 538 'Overst\u00e5ede Uger' does not update correctly 656 535 Buggy \"Fortryd\" button on activity 585 531 As a guardian i would like for the text under the pictograms not be in all caps. 529 As a guardian I would like to be able to mark activities as completed 602 527 As a guardian I want the design of disabled buttons to more clearly show that a button is disabled 514 Changing 'Tegn for udf\u00f8relse' does not move to previous screen 582 511 Adding activities to a weekplan and then editing the weekplan, removes the added activities 503 Re-navigating to activity where cancelling the activity was undone only show the 'Fortryd' button screen 502 Update buttons to follow the design guide 499 Refactor the code so we don't use the Observable class 609 498 As a guardian I would like to be able to delete a citizen from the system 497 As a guardian I would like to be able to delete myself from the system 495 Pictogram text is cut off instead of being divided over two lines in portrait mode on 600x1024 screen 559 485 There should be defined a errortext color in the custom colors, and this should be used for all errortext 557 482 When uploading a large image to a new weekplan, the image is not uploaded properly 474 With a lot of activities on a single weekplan, the activities sometimes jump to the bottom of the day when moved 598 471 Missing error message when trying to create a citizen with a username already in use 561 468 Activity timer completion sound is still emitted when you are logged out 462 Timers on activities are deleted when copying activities 444 Marking an activity with a timer as complete and then unmark it causes the activity to be deleted. 441 Make the disable buttons have 40% opacity 439 Lines appear around completed activities in citizen mode 438 As a developer I would want a file with the font sizes for the different screens in the weekplan 599 434 As a guardian I would like a citizen to be able to view two days at a time so that they can see today and tomorrow 566 429 As a guardian, I would like to know when I have written a wrong login fast 653 428 As a guardian, I would like to have an error message when creating a user who already exists 561 425 Periodic errors in asynchronous tests 396 As a guardian I would like to have access to x number of pictograms in offline mode using the local database 384 As a developer i would like the app to be deployed to Google Play when it is merged into master 371 As a guardian i would like to resume multiple canceled activities in the weekplan edit screen 664 364 Warning on memory waste/security issue for API lvl 19 647 352 Timer is not saved when you cancel activity and then undo the cancel 327 Artifacts 308 Visible red line when timer has finished. 594 304 The 'add new weekplan'-button is still available in edit-mode, but does nothing. 560 273 As a guardian I would like to be able to delete pictograms I've uploaded so that I can remove wrongly uploaded / outdated pictograms 606 206 API Exceptions are thrown everywhere where an api call was made previously when the log out button is pressed","title":"weekplanner"},{"location":"Legacy/Releases/2020E/2020es3r1/#web-api","text":"Number Title Solved by PR 170 As a developer I would like support for renaming activities locally 174 168 DEV server no longer works properly 167 Searching pictograms should not be case sensitive 169 162 As a developer I would like for pictograms to have a GirafUser indicating the creating user, so that I can make sure that only the creating user can delete pictograms. 159 Images should be hashed, and saved in the ImageHash row 173 158 Error messages from English to Danish 154 Fix unit and integration tests and startup problems. 155 123 Tests failing because they refer to a file not created by sample data of the database 114 Creating a new activity to a weekplans date with a pictogram with blank title will result in an exception 161 112 Activity timer disappears after pressing 'Fortryd' after cancelling the activity. 93 As a developer I would like to be able to run the integrations tests independent on the sample-data already being in the database 91 Remove redundant endpoint 176 49 Remove hardcoded values from Controller 47 As a developer, I would like the table Pictograms to not include ImageHash . 19 Updating order of activities when reordering in the weekplan 180 5 Illogical endpoints 172","title":"web-api"},{"location":"Legacy/Releases/2020E/2020es3r1/#api_client","text":"Number Title Solved by PR 81 Updating order of activities when reordering in the weekplan 86 74 As a developer I would like the pictogram model to have a the Id of the creating user 75 56 Refactor the code so we don't use the Observable class 73 52 Fundamental bug in the offline repository 3 Implement updateIcon in user_api.dart","title":"api_client"},{"location":"Legacy/Releases/2020E/2020es3r1/#result","text":"Based on the implemented issues, the following have been accomplished: Searching is no longer case sensitive Guardians can unmark completed activities Citizens can reenter completed activities Guardians can log out from the \"Show activity\" screens Moving week plans has been optimized, so it no longer reloads the entire week plan Added new and improved existing notifications when errors occur. \"Overst\u00e5ede uger\" can be collapsed Citizens can view two days at the time Multiple cancelled activities can be resumed at once Furthermore several bugs have been ironed out","title":"Result"},{"location":"Legacy/Releases/2020F/2020Fs1r1/","text":"2020Fs1r1 \u00b6 These are the user stories that were included in 2020s1r1: Issue ID Developed by Weekplanner#301 Group sw616f20 Weekplanner#305 Group sw609f20 Weekplanner#15 Group sw614f20 Weekplanner#323 Group sw614f20 Weekplanner#202 Group sw608f20 Weekplanner#307 Group sw608f20 Weekplanner#284 Group sw612f20 Weekplanner#200 Group sw609f20 Weekplanner#170 Group sw609f20 Weekplanner#309 Group sw611f20 Weekplanner#317 Group sw611f20 Weekplanner#319 Group sw611f20 Weekplanner#207 Group sw611f20 Weekplanner#255 Group sw610f20 Weekplanner#177 Group sw616f20 Weekplanner#318 Group sw610f20 Web-API#30 Group sw610f20 Web-API#46 Group sw610f20 Web-API#64 Group sw615f20 Web-API#67 Group sw607f20 Web-API#71 Group sw615f20 Web-API#73 Group sw607f20 Web-API#70 Group sw615f20 Wiki#4 Group sw610f20 Wiki#40 Group sw610f20 Wiki#114 Group sw612f20 Wiki#118 Group sw617f20 Wiki#123 Group sw610f20 Wiki#133 Group sw610f20 Wiki#147 Group sw610f20 Wiki#149 Group sw613f20 Wiki#159 Group sw610f20 Api_client#2 Group sw607f20 Api_client#23 Group sw615f20 Api_client#28 Group sw615f20 In this sprint we fixed various amounts of bugs and some more features. A user can now restart a timer more naturally. A user can now edit the details of a weekplan. A user can now edit some of the standard settings for a citizen. A guardian can now choose how many days a citizens sees.","title":"2020Fs1r1"},{"location":"Legacy/Releases/2020F/2020Fs1r1/#2020fs1r1","text":"These are the user stories that were included in 2020s1r1: Issue ID Developed by Weekplanner#301 Group sw616f20 Weekplanner#305 Group sw609f20 Weekplanner#15 Group sw614f20 Weekplanner#323 Group sw614f20 Weekplanner#202 Group sw608f20 Weekplanner#307 Group sw608f20 Weekplanner#284 Group sw612f20 Weekplanner#200 Group sw609f20 Weekplanner#170 Group sw609f20 Weekplanner#309 Group sw611f20 Weekplanner#317 Group sw611f20 Weekplanner#319 Group sw611f20 Weekplanner#207 Group sw611f20 Weekplanner#255 Group sw610f20 Weekplanner#177 Group sw616f20 Weekplanner#318 Group sw610f20 Web-API#30 Group sw610f20 Web-API#46 Group sw610f20 Web-API#64 Group sw615f20 Web-API#67 Group sw607f20 Web-API#71 Group sw615f20 Web-API#73 Group sw607f20 Web-API#70 Group sw615f20 Wiki#4 Group sw610f20 Wiki#40 Group sw610f20 Wiki#114 Group sw612f20 Wiki#118 Group sw617f20 Wiki#123 Group sw610f20 Wiki#133 Group sw610f20 Wiki#147 Group sw610f20 Wiki#149 Group sw613f20 Wiki#159 Group sw610f20 Api_client#2 Group sw607f20 Api_client#23 Group sw615f20 Api_client#28 Group sw615f20 In this sprint we fixed various amounts of bugs and some more features. A user can now restart a timer more naturally. A user can now edit the details of a weekplan. A user can now edit some of the standard settings for a citizen. A guardian can now choose how many days a citizens sees.","title":"2020Fs1r1"},{"location":"Legacy/Releases/2020F/2020Fs2r1/","text":"2020Fs2r1 \u00b6 These are the user stories that were included in 2020s2r1: Issue ID Developed by Weekplanner#64 Group sw611f20 Weekplanner#65 Group sw608f20 Weekplanner#66 Group sw616f20 Weekplanner#221 Group sw616f20 Weekplanner#245 Group sw608f20 Weekplanner#267 Group sw611f20 Weekplanner#270 Group sw608f20 Weekplanner#315 Group sw608f20 Weekplanner#348 Group sw609f20 Weekplanner#349 Group sw607f20 Weekplanner#355 Group sw615f20 Weekplanner#359 Group sw616f20 Weekplanner#363 Group sw615f20 Weekplanner#374 Group sw617f20 Weekplanner#376 Group sw615f20 Weekplanner#386 Group sw616f20 Weekplanner#387 Group sw616f20 Weekplanner#413 Group sw612f20 Wiki#132 Group sw609f20 Wiki#150 Group sw613f20 Wiki#155 Group sw612f20 Wiki#164 Group sw613f20 Wiki#168 Group sw617f20 Wiki#170 Group sw617f20 Web-API#23 Group sw607f20 Web-API#27 Group sw607f20 Web-API#48 Group sw607f20 Web-API#52 Group sw607f20 Web-API#54 Group sw612f20 Web-API#61 Group sw610f20 Web-API#62 Group sw610f20 Web-API#78 Group sw607f20 Web-API#94 Group sw616f20 Web-API#96 Group sw616f20 API_client#24 Group sw610f20 API_client#33 Group sw607f20 giraf-production-swarm#10 Group sw616f20 After this release the features that were implemented were: The guardian could make a new citizen The guardian could update the colors for the citizens weekplans The guardian could change the look of a completed activity for the citizen The guardian could lock the timer for the citizens The activities has pictogram text shown If an activity has a timer, an indicator is shown Buttons to mark and unmark all activities for one day The chosen setting shown in the setting page Error popup when pictogram upload failed","title":"2020Fs2r1"},{"location":"Legacy/Releases/2020F/2020Fs2r1/#2020fs2r1","text":"These are the user stories that were included in 2020s2r1: Issue ID Developed by Weekplanner#64 Group sw611f20 Weekplanner#65 Group sw608f20 Weekplanner#66 Group sw616f20 Weekplanner#221 Group sw616f20 Weekplanner#245 Group sw608f20 Weekplanner#267 Group sw611f20 Weekplanner#270 Group sw608f20 Weekplanner#315 Group sw608f20 Weekplanner#348 Group sw609f20 Weekplanner#349 Group sw607f20 Weekplanner#355 Group sw615f20 Weekplanner#359 Group sw616f20 Weekplanner#363 Group sw615f20 Weekplanner#374 Group sw617f20 Weekplanner#376 Group sw615f20 Weekplanner#386 Group sw616f20 Weekplanner#387 Group sw616f20 Weekplanner#413 Group sw612f20 Wiki#132 Group sw609f20 Wiki#150 Group sw613f20 Wiki#155 Group sw612f20 Wiki#164 Group sw613f20 Wiki#168 Group sw617f20 Wiki#170 Group sw617f20 Web-API#23 Group sw607f20 Web-API#27 Group sw607f20 Web-API#48 Group sw607f20 Web-API#52 Group sw607f20 Web-API#54 Group sw612f20 Web-API#61 Group sw610f20 Web-API#62 Group sw610f20 Web-API#78 Group sw607f20 Web-API#94 Group sw616f20 Web-API#96 Group sw616f20 API_client#24 Group sw610f20 API_client#33 Group sw607f20 giraf-production-swarm#10 Group sw616f20 After this release the features that were implemented were: The guardian could make a new citizen The guardian could update the colors for the citizens weekplans The guardian could change the look of a completed activity for the citizen The guardian could lock the timer for the citizens The activities has pictogram text shown If an activity has a timer, an indicator is shown Buttons to mark and unmark all activities for one day The chosen setting shown in the setting page Error popup when pictogram upload failed","title":"2020Fs2r1"},{"location":"Legacy/Releases/2020F/2020Fs3r1/","text":"2020Fs3r1 \u00b6 These are the user stories that were included in 2020s3r1: Issue ID Developed by Weekplanner#220 Group sw614f20 Weekplanner#222 Group sw610f20 Weekplanner#325 Group sw612f20 Weekplanner#326 Group sw610f20 Weekplanner#340 Group sw611f20 Weekplanner#381 Group sw617f20 Weekplanner#416 Group sw612f20 Weekplanner#419 Group sw609f20 Weekplanner#459 Group sw615f20 Weekplanner#461 Group sw616f20 Weekplanner#492 Group sw611f20 Weekplanner#494 Group sw611f20 Weekplanner#496 Group sw613f20 Weekplanner#516 Group sw616f20 Weekplanner#517 Group sw614f20 Web-API#40 Group sw607f20 Web-API#74 Group sw610f20 Web-API#92 Group sw617f20 Web-API#98 Group sw616f20 Web-API#109 Group sw616f20 Web-API#119 Group sw612f20 Web-API#140 Group sw616f20 Wiki#126 Group sw607f20 Wiki#138 Group sw610f20 Wiki#163 Group sw612f20 Wiki#171 Group sw607f20 Wiki#183 Group sw612f20 Wiki#188 Group sw615f20 Wiki#189 Group sw616f20 Wiki#199 Group sw608f20 Wiki#190 Group sw609f20 Wiki#201 Group sw613f20 Wiki#204 Group sw613f20 Wiki#217 Group sw613f20 Wiki#223 Group sw609f20 API_client#55 Group sw614f20 After this release the features that were implemented were: The guardian could make a choice board The users could see their legal rights about their information in the system The guardian could change the look of a timer for the citizen The weekplans are separated in upcoming and old The guardian could copy weekplans The guardian could access the setting for the citizen from the weekplan Text is shown if the guardian is missing some input when creating a citizen","title":"2020Fs3r1"},{"location":"Legacy/Releases/2020F/2020Fs3r1/#2020fs3r1","text":"These are the user stories that were included in 2020s3r1: Issue ID Developed by Weekplanner#220 Group sw614f20 Weekplanner#222 Group sw610f20 Weekplanner#325 Group sw612f20 Weekplanner#326 Group sw610f20 Weekplanner#340 Group sw611f20 Weekplanner#381 Group sw617f20 Weekplanner#416 Group sw612f20 Weekplanner#419 Group sw609f20 Weekplanner#459 Group sw615f20 Weekplanner#461 Group sw616f20 Weekplanner#492 Group sw611f20 Weekplanner#494 Group sw611f20 Weekplanner#496 Group sw613f20 Weekplanner#516 Group sw616f20 Weekplanner#517 Group sw614f20 Web-API#40 Group sw607f20 Web-API#74 Group sw610f20 Web-API#92 Group sw617f20 Web-API#98 Group sw616f20 Web-API#109 Group sw616f20 Web-API#119 Group sw612f20 Web-API#140 Group sw616f20 Wiki#126 Group sw607f20 Wiki#138 Group sw610f20 Wiki#163 Group sw612f20 Wiki#171 Group sw607f20 Wiki#183 Group sw612f20 Wiki#188 Group sw615f20 Wiki#189 Group sw616f20 Wiki#199 Group sw608f20 Wiki#190 Group sw609f20 Wiki#201 Group sw613f20 Wiki#204 Group sw613f20 Wiki#217 Group sw613f20 Wiki#223 Group sw609f20 API_client#55 Group sw614f20 After this release the features that were implemented were: The guardian could make a choice board The users could see their legal rights about their information in the system The guardian could change the look of a timer for the citizen The weekplans are separated in upcoming and old The guardian could copy weekplans The guardian could access the setting for the citizen from the weekplan Text is shown if the guardian is missing some input when creating a citizen","title":"2020Fs3r1"},{"location":"Legacy/weekplanner/","text":"Overview \u00b6 This section contains files specifically meant to be used in the retired weekplanner project. This includes guides on UI design, how to prototype and coding conventions which have all been moved to google drive or left as a future decision for coming GIRAF developers.","title":"Overview"},{"location":"Legacy/weekplanner/#overview","text":"This section contains files specifically meant to be used in the retired weekplanner project. This includes guides on UI design, how to prototype and coding conventions which have all been moved to google drive or left as a future decision for coming GIRAF developers.","title":"Overview"},{"location":"Legacy/weekplanner/Apps/","text":"Overview \u00b6 This section contains information for the Android/iOS apps developed in GIRAF. Active Apps in GIRAF \u00b6 Weekplanner (Ugeplan) Implementation Guidelines \u00b6 Guidelines for different implementations in the GIRAF apps can be found here Manual Release of Giraf \u00b6 Guide for making a Release on both Apple App Store and Google Play Store here Testing \u00b6 A description on how to do unit testing in the apps can be found here .","title":"Overview"},{"location":"Legacy/weekplanner/Apps/#overview","text":"This section contains information for the Android/iOS apps developed in GIRAF.","title":"Overview"},{"location":"Legacy/weekplanner/Apps/#active-apps-in-giraf","text":"Weekplanner (Ugeplan)","title":"Active Apps in GIRAF"},{"location":"Legacy/weekplanner/Apps/#implementation-guidelines","text":"Guidelines for different implementations in the GIRAF apps can be found here","title":"Implementation Guidelines"},{"location":"Legacy/weekplanner/Apps/#manual-release-of-giraf","text":"Guide for making a Release on both Apple App Store and Google Play Store here","title":"Manual Release of Giraf"},{"location":"Legacy/weekplanner/Apps/#testing","text":"A description on how to do unit testing in the apps can be found here .","title":"Testing"},{"location":"Legacy/weekplanner/Apps/release_guide/","text":"Manual Release of Giraf \u00b6 When you have to make a release of Giraf then the pubspec.yaml file must be updated so when building it the bundle gets the correct version number. This means that you have to find the key-word version and update it e.g. version: 1.2.2 to version: 1.3.0 . It is also possible to add a suffix if it is a new build ( +1 , +hotfix ) or a pre-release ( -beta.2 , -rc.1 ) if it should be necessary. In the main.dart file the environment should be changed to assets/environments.prod.json , and the giraf-prod database should be made up-to-date and have at least one working Guardian, Trustee, and Citizen login. If you are unsure what version number your release should have, you can take a look at this page: https://semver.org it is also available in danish https://semver.org/lang/da/ You need to get access to the Giraf project's keepass so that you can use the Apple Developer Account that is on there and also the Android account. It is also possible to add your own personal Apple Developer Account to the Girafs Venner Team. Android Release \u00b6 Here is the official flutter guide to release on Android: https://docs.flutter.dev/deployment/android To build an Android App Bundle, the password for the Keystore has to be added to the android/local.properties file by adding giraf.keystore_password=<password_here> to the file. The command flutter build appbundle is run to generate the Android App Bundle file, which is then uploaded to Google Play Console https://play.google.com/console/ . iOS Release \u00b6 Here is the official flutter guide to release on iOS: https://docs.flutter.dev/deployment/ios Preliminary requirements for creating a build and release a flutter app for iOS is that you need Xcode which means that you must do it on a Mac computer. Remember to have an Apple Developer Account that is connected to the Girafs Venner Team. List over all the different steps for creating a iOS release: Open ios folder in Xcode and check for errors and warnings Click Automatically manage signing Run flutter build ipa Drag and drop the ipa file in Transporter and click deliver Update App Store Connect Update gdpr Update which encryption has been used Ensure that the test user still is active in the database Write text for What's New in This Version Open ios folder in Xcode and check for errors and warnings \u00b6 The first thing that needs to be done is that in the weekplanner repository find the ios folder and open that in Xcode, then select the Runner target then under the Signing & Capabilities tab add the Apple Developer Account in the Teams section. Also under the Signing & Capabilities tab set the Automatically manage signing to true, this makes it easier to create the IPA file. Lastly check/update what the minimum requirements are for running the app, e.g., the iOS version. Run flutter build ipa \u00b6 Open a terminal under the weekplanner repository and run the command flutter build ipa and then fix the possible errors or warnings. The command will create an IPA file if it passes with no exceptions. Now proceed to the next step. Drag and drop the ipa file in Transporter and click deliver \u00b6 If you have not done it already then go to App Store and download Transporter . Open transporter and drag the ipa file from weekplanner/build/ios/ipa and drop it in to the transporter app and wait until it is ready and then click deliver. Then the IPA file will be transferred to App Store Connect. Update App Store Connect \u00b6 Go to https://appstoreconnect.apple.com and log in with your Apple Developer Account. You might not need to update this, but we had to so check that the gdpr is up-to-date and the same for then it comes to what types of encryption has been used. When making the build ready for review check that under the Sign-In Information that the user is still active in the online database. Because if this is not up-to-date then the review will not be approved by Apple. Lastly make sure to add some text under What's New in This Version so that it is documented what has been added in this version, if it is like new features, bug-fixes, or something else. Then the Release should be ready for review and passable so that it can get on App Store.","title":"Manual Release of Giraf"},{"location":"Legacy/weekplanner/Apps/release_guide/#manual-release-of-giraf","text":"When you have to make a release of Giraf then the pubspec.yaml file must be updated so when building it the bundle gets the correct version number. This means that you have to find the key-word version and update it e.g. version: 1.2.2 to version: 1.3.0 . It is also possible to add a suffix if it is a new build ( +1 , +hotfix ) or a pre-release ( -beta.2 , -rc.1 ) if it should be necessary. In the main.dart file the environment should be changed to assets/environments.prod.json , and the giraf-prod database should be made up-to-date and have at least one working Guardian, Trustee, and Citizen login. If you are unsure what version number your release should have, you can take a look at this page: https://semver.org it is also available in danish https://semver.org/lang/da/ You need to get access to the Giraf project's keepass so that you can use the Apple Developer Account that is on there and also the Android account. It is also possible to add your own personal Apple Developer Account to the Girafs Venner Team.","title":"Manual Release of Giraf"},{"location":"Legacy/weekplanner/Apps/release_guide/#android-release","text":"Here is the official flutter guide to release on Android: https://docs.flutter.dev/deployment/android To build an Android App Bundle, the password for the Keystore has to be added to the android/local.properties file by adding giraf.keystore_password=<password_here> to the file. The command flutter build appbundle is run to generate the Android App Bundle file, which is then uploaded to Google Play Console https://play.google.com/console/ .","title":"Android Release"},{"location":"Legacy/weekplanner/Apps/release_guide/#ios-release","text":"Here is the official flutter guide to release on iOS: https://docs.flutter.dev/deployment/ios Preliminary requirements for creating a build and release a flutter app for iOS is that you need Xcode which means that you must do it on a Mac computer. Remember to have an Apple Developer Account that is connected to the Girafs Venner Team. List over all the different steps for creating a iOS release: Open ios folder in Xcode and check for errors and warnings Click Automatically manage signing Run flutter build ipa Drag and drop the ipa file in Transporter and click deliver Update App Store Connect Update gdpr Update which encryption has been used Ensure that the test user still is active in the database Write text for What's New in This Version","title":"iOS Release"},{"location":"Legacy/weekplanner/Apps/release_guide/#open-ios-folder-in-xcode-and-check-for-errors-and-warnings","text":"The first thing that needs to be done is that in the weekplanner repository find the ios folder and open that in Xcode, then select the Runner target then under the Signing & Capabilities tab add the Apple Developer Account in the Teams section. Also under the Signing & Capabilities tab set the Automatically manage signing to true, this makes it easier to create the IPA file. Lastly check/update what the minimum requirements are for running the app, e.g., the iOS version.","title":"Open ios folder in Xcode and check for errors and warnings"},{"location":"Legacy/weekplanner/Apps/release_guide/#run-flutter-build-ipa","text":"Open a terminal under the weekplanner repository and run the command flutter build ipa and then fix the possible errors or warnings. The command will create an IPA file if it passes with no exceptions. Now proceed to the next step.","title":"Run flutter build ipa"},{"location":"Legacy/weekplanner/Apps/release_guide/#drag-and-drop-the-ipa-file-in-transporter-and-click-deliver","text":"If you have not done it already then go to App Store and download Transporter . Open transporter and drag the ipa file from weekplanner/build/ios/ipa and drop it in to the transporter app and wait until it is ready and then click deliver. Then the IPA file will be transferred to App Store Connect.","title":"Drag and drop the ipa file in Transporter and click deliver"},{"location":"Legacy/weekplanner/Apps/release_guide/#update-app-store-connect","text":"Go to https://appstoreconnect.apple.com and log in with your Apple Developer Account. You might not need to update this, but we had to so check that the gdpr is up-to-date and the same for then it comes to what types of encryption has been used. When making the build ready for review check that under the Sign-In Information that the user is still active in the online database. Because if this is not up-to-date then the review will not be approved by Apple. Lastly make sure to add some text under What's New in This Version so that it is documented what has been added in this version, if it is like new features, bug-fixes, or something else. Then the Release should be ready for review and passable so that it can get on App Store.","title":"Update App Store Connect"},{"location":"Legacy/weekplanner/Apps/test/","text":"Testing \u00b6 This guide describes how you test in Flutter, and what the test criteria is in both blocs and screens/widgets. Test location \u00b6 All tests have to be located in the \"test\" folder. All Weekplanner files should have a corresponding file in the test folder where all tests related to the file are located. The test files should be in the folder that corresponds to the content of the file. For example, screen testes should be in the screen folder. Test basics \u00b6 A test file always contain a main function. The main function is placed directly into the file, and not in any type of class, and all tests are written in the main function. Setup \u00b6 When several tests are run, they often share the same setup. You can automate the setup by creating a setUp() function in the main function. The setup function is then called every time a new test is run. The main function could look like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 void main () { SomeClass a ; SomeOtherClass b ; BlocType bloc ; setUp ((){ a = < Some value > ; b = < Some other value > ; bloc = bloc ( some API ); }); //The tests go here } The variables you use in the setUp() function should be declared in the beginning of the main function, right before the setUp() function. Expect \u00b6 When you run a test you need to compare values to see if the test is successful. In Flutter this is done with an expect function. This is written: 1 expect ( value , expected_value ); There can be several expects inside the same test. Aside from an actual value, you can use a lot of different find predicate. The most commonly used is the following: 1 2 3 4 5 6 7 8 9 10 expect ( < variable > , isTrue ); expect ( < variable > , isFalse ); expect ( < variable > , equals ( < value > )); expect ( < variable > , findsNothing ); expect ( < variable > , findsNWidgets ( < number > )); expect ( < variable > , findsWidgets ); expect ( < variable > , findsOneWidget ); expect ( < variable > , find ); expect ( < variable > , false ); expect ( < variable > , true ); Writing tests \u00b6 There are a significant difference between a widget/screen test and a bloc test. The tests for these two are explained separately. Blocs \u00b6 A test for a bloc looks like this: 1 2 3 test ( 'Test description' ,(){ }); If the code does not run in sequence you should use an async test, that looks this way: 1 2 3 test ( 'Test description' , async (( DoneFn done ){ done (); })); A standard bloc test will finish when all the code are executed while an async bloc test will finish when the done() function is called. If you make an async test, you should always remember to call the done() function in bloc, otherwise the test will continue until it times out, and then fail. The done() function is placed inside the async function. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import 'package:<All the different imports>' ; void main () { SomeClass a ; SomeOtherClass b ; BlocType bloc ; setUp ((){ a = < Some value > ; b = < Some other value > ; bloc = bloc ( some API ); }); test ( 'This is a standard test' ,(){ a . someMethod ( b ); expect ( b . someFunction , 2 ); expect ( a . someFunction , 3 ); }); test ( 'This is an async test' , async (( DoneFn done ){ bloc . steamA . skip ( 1 ). listen (( SomeModel model ){ expect ( model . content , value ); expect ( model . otherContent , otherValue ); done (); }); bloc . setA ( value ); }); } In the async test, the listen function is defined first. In the async test a listen and listen function is set up on a steam in the bloc. When a set function on the behaviour subject of the stream is called, the listen function is called, and in here the expect() and done() functions. Widgets and screens \u00b6 The widget and screen tests separate from the class tests. All tests are still in the main() function and you can still use a setup function, but instead of the norma tests you use test widgets. 1 2 3 testWidgets ( 'Name' ,( WidgetTester tester ) async { await tester . < Something to wait for > ; }); Inside the testWidget() you use a WidgetTester object. This is used to interact with widgets in the test environment. When you use the WidgetTester object for actions, you should use the await keyword before. In blocs you use the done() function when running an async test. This is not done in testWidgets() . However, if you use a listener in your expect, you use something called a completer. 1 2 3 4 5 6 7 8 9 10 testWidgets ( 'Name' ,( WidgetTester tester ) async { final Completer < datatypeA > done = Completer < datatypeA > (); bloc . someBloc . listen (( datatypeA ) async { await tester . pump (); expect ( success , true ); done . complete (); }); await done . future ; }); There are some basic functions we use a lot on the WidgetTester object. The first is pumpWidget() . This acts by setting up a screen in your test environment. This makes it possible to test on the screen, for example if the right elements are present. You can write it like this: 1 await tester . pumpWidget ( MaterialApp ( home: SomeScreen ())); This set up the SomeScreen() screen. Another important function is the pump() function. This is used to execute actions, by rendering the screen. If you want to wait for a duration of time, you can note this as an attribute. If you want to test more actions in sequence, you should use pumpAndSettle() . This is for example if you want to push a button that makes something else happen, and test if that happens. You can also use the tester.enterText() widget. This looks like this: 1 await tester . enterText ( find . byType ( TextField , query )); This also shows the find.byType() function, that searches in the widgets in the tester and finds by a certain type, here TextField , a widget with a name matching the query. The query is a string and could for example be 'cat'. The WidgetTester has many other functions you should explore when needed. The key thing is to understand that a WidgetTester interacts with widgets in the test environment. The environment can be updated with the WidgetTester , and widgets extracted with \"find\". The expect is similar to the bloc expect, but in the actual part you can use \"find\" to choose the widgets you test on. Examples of TestWidget expects are: 1 2 expect(find.byWidgetPredicate((Widget widget) =>widget is WidgetType), <FINDINGS>); expect(find.byType(WidgetType),<FINDINGS>); Mocking \u00b6 When you write tests, you will often need to mock certain objects. This section describes the different mocking often used in GIRAF testing. Mock screen \u00b6 When you test a non screen widget you can use to have a MockScreen as a container. This is written in the test document, outside the main() function and can look like this: 1 2 3 4 5 6 class MockScreen extends StatelessWidget { @override Widget build ( BuildContext context ) { return Scaffold () } } You can fill the MockScreen with all the content you find necessary to do the tests. Dependency injection \u00b6 All dependency injections are automatically set up in the bootStrap.dart file. If you need to change a dependency in for a test, you can do it directly on the dependency injector. This can look like this: 1 2 3 4 5 newDependencyBloc = SomeBloc (); di . clearAll (); di . registerDependency < SomeBloc > (( _ ) => newDependencyBloc ); di . registerDependency < Bloc1 > (( _ ) => StandardBloc1 ()); di . registerDependency < Bloc2 > (( _ ) => StandardBloc2 ()); The newDependencyBloc is the bloc that you want to replace the original dependency with. Before you add a new dependency, you have to clear all dependencies. This means that if you need dependencies that are otherwise there as a standard, you have to add them again. I dependencies are not relevant, you should not add them. Mock API \u00b6 All Weekplanner testing should be independent of the current state of the database. If you need database access for your test, you create a mock API. If we for example wanted to use an API class called SomeAPI , we would write it like: 1 class MockSomeAPI extends Mock implements SomeAPI {} This is done outside the main() function in the test file. If you need to use the mock API in a bloc you can do it like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import 'package:<needed packages>' ; class MockSomeAPI extends Mock implements SomeAPI {} void main () { BlocClass bloc ; API api ; MockSomeAPI someAPI ; setUp ((){ api = API ( 'Write anything' ); someAPI = MockSomeAPI (); api . some = someAPI (); bloc = BlocClass ( API ); }); } Here, the api.some field in the API corresponds to the API of the someAPI type in the class. Now you can use your bloc like normal, you just have to add the needed information to the someAPI . If you need the API to do something when it is updated, you can write a setupAPICalls() function before the setUp() function. This function should be called inside the setUp() function, so it is called every time the API is updated inside the tests. The function could look like the following: 1 2 3 4 5 6 void setupAPICalls () { when ( someAPI . update ()) } setUp ((){ setUpAPICalls }) Override \u00b6 In mock classes you can override elements to change functionality when needed. An example of this is in a mock API, where you can override a function that should give you some specific data from the database. 1 2 3 4 5 6 7 8 9 class MockSomeAPI extends Mock implements SomeAPI @override Observable < ARelevantModel > SomeAPIMethod () { return Observable < AReleventModel > . just ( ARelevantModel ( id: '1' , info1: 'Cat' info2: 3 )); } This is a good way to mock information in the database that you need to test. When you mock a bloc, you can also do it the same way. The difference is that you override Observable objects instead of API calls. An example of this is: 1 2 3 @override Observable < bool > get someStream => observable < bool > . just ( input );","title":"Testing"},{"location":"Legacy/weekplanner/Apps/test/#testing","text":"This guide describes how you test in Flutter, and what the test criteria is in both blocs and screens/widgets.","title":"Testing"},{"location":"Legacy/weekplanner/Apps/test/#test-location","text":"All tests have to be located in the \"test\" folder. All Weekplanner files should have a corresponding file in the test folder where all tests related to the file are located. The test files should be in the folder that corresponds to the content of the file. For example, screen testes should be in the screen folder.","title":"Test location"},{"location":"Legacy/weekplanner/Apps/test/#test-basics","text":"A test file always contain a main function. The main function is placed directly into the file, and not in any type of class, and all tests are written in the main function.","title":"Test basics"},{"location":"Legacy/weekplanner/Apps/test/#setup","text":"When several tests are run, they often share the same setup. You can automate the setup by creating a setUp() function in the main function. The setup function is then called every time a new test is run. The main function could look like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 void main () { SomeClass a ; SomeOtherClass b ; BlocType bloc ; setUp ((){ a = < Some value > ; b = < Some other value > ; bloc = bloc ( some API ); }); //The tests go here } The variables you use in the setUp() function should be declared in the beginning of the main function, right before the setUp() function.","title":"Setup"},{"location":"Legacy/weekplanner/Apps/test/#expect","text":"When you run a test you need to compare values to see if the test is successful. In Flutter this is done with an expect function. This is written: 1 expect ( value , expected_value ); There can be several expects inside the same test. Aside from an actual value, you can use a lot of different find predicate. The most commonly used is the following: 1 2 3 4 5 6 7 8 9 10 expect ( < variable > , isTrue ); expect ( < variable > , isFalse ); expect ( < variable > , equals ( < value > )); expect ( < variable > , findsNothing ); expect ( < variable > , findsNWidgets ( < number > )); expect ( < variable > , findsWidgets ); expect ( < variable > , findsOneWidget ); expect ( < variable > , find ); expect ( < variable > , false ); expect ( < variable > , true );","title":"Expect"},{"location":"Legacy/weekplanner/Apps/test/#writing-tests","text":"There are a significant difference between a widget/screen test and a bloc test. The tests for these two are explained separately.","title":"Writing tests"},{"location":"Legacy/weekplanner/Apps/test/#blocs","text":"A test for a bloc looks like this: 1 2 3 test ( 'Test description' ,(){ }); If the code does not run in sequence you should use an async test, that looks this way: 1 2 3 test ( 'Test description' , async (( DoneFn done ){ done (); })); A standard bloc test will finish when all the code are executed while an async bloc test will finish when the done() function is called. If you make an async test, you should always remember to call the done() function in bloc, otherwise the test will continue until it times out, and then fail. The done() function is placed inside the async function. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import 'package:<All the different imports>' ; void main () { SomeClass a ; SomeOtherClass b ; BlocType bloc ; setUp ((){ a = < Some value > ; b = < Some other value > ; bloc = bloc ( some API ); }); test ( 'This is a standard test' ,(){ a . someMethod ( b ); expect ( b . someFunction , 2 ); expect ( a . someFunction , 3 ); }); test ( 'This is an async test' , async (( DoneFn done ){ bloc . steamA . skip ( 1 ). listen (( SomeModel model ){ expect ( model . content , value ); expect ( model . otherContent , otherValue ); done (); }); bloc . setA ( value ); }); } In the async test, the listen function is defined first. In the async test a listen and listen function is set up on a steam in the bloc. When a set function on the behaviour subject of the stream is called, the listen function is called, and in here the expect() and done() functions.","title":"Blocs"},{"location":"Legacy/weekplanner/Apps/test/#widgets-and-screens","text":"The widget and screen tests separate from the class tests. All tests are still in the main() function and you can still use a setup function, but instead of the norma tests you use test widgets. 1 2 3 testWidgets ( 'Name' ,( WidgetTester tester ) async { await tester . < Something to wait for > ; }); Inside the testWidget() you use a WidgetTester object. This is used to interact with widgets in the test environment. When you use the WidgetTester object for actions, you should use the await keyword before. In blocs you use the done() function when running an async test. This is not done in testWidgets() . However, if you use a listener in your expect, you use something called a completer. 1 2 3 4 5 6 7 8 9 10 testWidgets ( 'Name' ,( WidgetTester tester ) async { final Completer < datatypeA > done = Completer < datatypeA > (); bloc . someBloc . listen (( datatypeA ) async { await tester . pump (); expect ( success , true ); done . complete (); }); await done . future ; }); There are some basic functions we use a lot on the WidgetTester object. The first is pumpWidget() . This acts by setting up a screen in your test environment. This makes it possible to test on the screen, for example if the right elements are present. You can write it like this: 1 await tester . pumpWidget ( MaterialApp ( home: SomeScreen ())); This set up the SomeScreen() screen. Another important function is the pump() function. This is used to execute actions, by rendering the screen. If you want to wait for a duration of time, you can note this as an attribute. If you want to test more actions in sequence, you should use pumpAndSettle() . This is for example if you want to push a button that makes something else happen, and test if that happens. You can also use the tester.enterText() widget. This looks like this: 1 await tester . enterText ( find . byType ( TextField , query )); This also shows the find.byType() function, that searches in the widgets in the tester and finds by a certain type, here TextField , a widget with a name matching the query. The query is a string and could for example be 'cat'. The WidgetTester has many other functions you should explore when needed. The key thing is to understand that a WidgetTester interacts with widgets in the test environment. The environment can be updated with the WidgetTester , and widgets extracted with \"find\". The expect is similar to the bloc expect, but in the actual part you can use \"find\" to choose the widgets you test on. Examples of TestWidget expects are: 1 2 expect(find.byWidgetPredicate((Widget widget) =>widget is WidgetType), <FINDINGS>); expect(find.byType(WidgetType),<FINDINGS>);","title":"Widgets and screens"},{"location":"Legacy/weekplanner/Apps/test/#mocking","text":"When you write tests, you will often need to mock certain objects. This section describes the different mocking often used in GIRAF testing.","title":"Mocking"},{"location":"Legacy/weekplanner/Apps/test/#mock-screen","text":"When you test a non screen widget you can use to have a MockScreen as a container. This is written in the test document, outside the main() function and can look like this: 1 2 3 4 5 6 class MockScreen extends StatelessWidget { @override Widget build ( BuildContext context ) { return Scaffold () } } You can fill the MockScreen with all the content you find necessary to do the tests.","title":"Mock screen"},{"location":"Legacy/weekplanner/Apps/test/#dependency-injection","text":"All dependency injections are automatically set up in the bootStrap.dart file. If you need to change a dependency in for a test, you can do it directly on the dependency injector. This can look like this: 1 2 3 4 5 newDependencyBloc = SomeBloc (); di . clearAll (); di . registerDependency < SomeBloc > (( _ ) => newDependencyBloc ); di . registerDependency < Bloc1 > (( _ ) => StandardBloc1 ()); di . registerDependency < Bloc2 > (( _ ) => StandardBloc2 ()); The newDependencyBloc is the bloc that you want to replace the original dependency with. Before you add a new dependency, you have to clear all dependencies. This means that if you need dependencies that are otherwise there as a standard, you have to add them again. I dependencies are not relevant, you should not add them.","title":"Dependency injection"},{"location":"Legacy/weekplanner/Apps/test/#mock-api","text":"All Weekplanner testing should be independent of the current state of the database. If you need database access for your test, you create a mock API. If we for example wanted to use an API class called SomeAPI , we would write it like: 1 class MockSomeAPI extends Mock implements SomeAPI {} This is done outside the main() function in the test file. If you need to use the mock API in a bloc you can do it like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import 'package:<needed packages>' ; class MockSomeAPI extends Mock implements SomeAPI {} void main () { BlocClass bloc ; API api ; MockSomeAPI someAPI ; setUp ((){ api = API ( 'Write anything' ); someAPI = MockSomeAPI (); api . some = someAPI (); bloc = BlocClass ( API ); }); } Here, the api.some field in the API corresponds to the API of the someAPI type in the class. Now you can use your bloc like normal, you just have to add the needed information to the someAPI . If you need the API to do something when it is updated, you can write a setupAPICalls() function before the setUp() function. This function should be called inside the setUp() function, so it is called every time the API is updated inside the tests. The function could look like the following: 1 2 3 4 5 6 void setupAPICalls () { when ( someAPI . update ()) } setUp ((){ setUpAPICalls })","title":"Mock API"},{"location":"Legacy/weekplanner/Apps/test/#override","text":"In mock classes you can override elements to change functionality when needed. An example of this is in a mock API, where you can override a function that should give you some specific data from the database. 1 2 3 4 5 6 7 8 9 class MockSomeAPI extends Mock implements SomeAPI @override Observable < ARelevantModel > SomeAPIMethod () { return Observable < AReleventModel > . just ( ARelevantModel ( id: '1' , info1: 'Cat' info2: 3 )); } This is a good way to mock information in the database that you need to test. When you mock a bloc, you can also do it the same way. The difference is that you override Observable objects instead of API calls. An example of this is: 1 2 3 @override Observable < bool > get someStream => observable < bool > . just ( input );","title":"Override"},{"location":"Legacy/weekplanner/Apps/Guidelines/","text":"Overview \u00b6 This sections covers guidelines for different implementations in the GIRAF apps. Handle exceptions from the Web API","title":"Overview"},{"location":"Legacy/weekplanner/Apps/Guidelines/#overview","text":"This sections covers guidelines for different implementations in the GIRAF apps. Handle exceptions from the Web API","title":"Overview"},{"location":"Legacy/weekplanner/Apps/Guidelines/web_api_error_handling/","text":"Handle Exceptions from the Web API \u00b6 This section will cover guidelines on how to handle exceptions from Web API in the GIRAF apps. Guidelines \u00b6 Use OnError function when listening to a stream: Listen().OnError to catch the error. Add a case with the requested ErrorCode in the switch statement within getErrorMessage method How to Use a Stream \u00b6 In order to be able to get values from a stream you need to do something referred to as subscribing or listening. When you subscribe to a stream you will only get the values that are emitted (put onto the stream) after the subscription. You subscribe to the stream by calling the listen function and supplying it with a method to call back to when there's a new value available, commonly referred to as a callback method, or just a callback. Catching an API Error from a Stream in the Apps \u00b6 The listen call returns a StreamSubscription with the type of the stream. With a stream subscription it is possible to use the onError function, which takes and error object and the stack trace as parameters. An example of onError usage on a listen call from new_citizen_screen.dart on the weekplanner can be seen below. 1 2 3 4 5 6 7 8 9 onPressed: () { _bloc.createCitizen().listen((GirafUserModel response) { if (response != null) { Routes.pop<GirafUserModel>(context, response); _bloc.resetBloc(); } }).onError((Object error) => _translator.catchApiError(error, context)); } _translator is the referenced ApiErrorTranslater class that holds the catchApiError method. This class is found in errorcode_translater.dart. Referring to this class is done by instantiating the class: final ApiErrorTranslater _translator = ApiErrorTranslater(); The class ApiErrorTranslater holds two methods: catchApiError and getErrorMessage. Catching API errors method \u00b6 The catchApiError method builds a pop up GirafNotifyDialog. The string for the error message is received from the other method getErrorMessage. 1 2 3 4 5 6 7 8 9 10 11 12 13 void catchApiError(Object error, BuildContext context) { showDialog<Center>( /// exception handler to handle web_api exceptions barrierDismissible: false, context: context, builder: (BuildContext context) { return GirafNotifyDialog( title: 'Fejl', description: getErrorMessage(error), key: const Key('ErrorMessageDialog')); }); } Creating new error message method \u00b6 In this method you will need to add a new case for a specific ErrorKey with a return call on an error message string. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ///Get apropriate error message based on error code String getErrorMessage(ApiException error) { switch (error.errorKey) { case ErrorKey.UserAlreadyExists: return 'Brugernavnet eksisterer allerede'; case ErrorKey.Error: // Undefined errors, the message is in english // as we cant predict why it was cast return 'message: ' + error.errorMessage + '\\nDetails: ' + error.errorDetails; default: return 'Ukendt fejl'; } }","title":"Handle Exceptions from the Web API"},{"location":"Legacy/weekplanner/Apps/Guidelines/web_api_error_handling/#handle-exceptions-from-the-web-api","text":"This section will cover guidelines on how to handle exceptions from Web API in the GIRAF apps.","title":"Handle Exceptions from the Web API"},{"location":"Legacy/weekplanner/Apps/Guidelines/web_api_error_handling/#guidelines","text":"Use OnError function when listening to a stream: Listen().OnError to catch the error. Add a case with the requested ErrorCode in the switch statement within getErrorMessage method","title":"Guidelines"},{"location":"Legacy/weekplanner/Apps/Guidelines/web_api_error_handling/#how-to-use-a-stream","text":"In order to be able to get values from a stream you need to do something referred to as subscribing or listening. When you subscribe to a stream you will only get the values that are emitted (put onto the stream) after the subscription. You subscribe to the stream by calling the listen function and supplying it with a method to call back to when there's a new value available, commonly referred to as a callback method, or just a callback.","title":"How to Use a Stream"},{"location":"Legacy/weekplanner/Apps/Guidelines/web_api_error_handling/#catching-an-api-error-from-a-stream-in-the-apps","text":"The listen call returns a StreamSubscription with the type of the stream. With a stream subscription it is possible to use the onError function, which takes and error object and the stack trace as parameters. An example of onError usage on a listen call from new_citizen_screen.dart on the weekplanner can be seen below. 1 2 3 4 5 6 7 8 9 onPressed: () { _bloc.createCitizen().listen((GirafUserModel response) { if (response != null) { Routes.pop<GirafUserModel>(context, response); _bloc.resetBloc(); } }).onError((Object error) => _translator.catchApiError(error, context)); } _translator is the referenced ApiErrorTranslater class that holds the catchApiError method. This class is found in errorcode_translater.dart. Referring to this class is done by instantiating the class: final ApiErrorTranslater _translator = ApiErrorTranslater(); The class ApiErrorTranslater holds two methods: catchApiError and getErrorMessage.","title":"Catching an API Error from a Stream in the Apps"},{"location":"Legacy/weekplanner/Apps/Guidelines/web_api_error_handling/#catching-api-errors-method","text":"The catchApiError method builds a pop up GirafNotifyDialog. The string for the error message is received from the other method getErrorMessage. 1 2 3 4 5 6 7 8 9 10 11 12 13 void catchApiError(Object error, BuildContext context) { showDialog<Center>( /// exception handler to handle web_api exceptions barrierDismissible: false, context: context, builder: (BuildContext context) { return GirafNotifyDialog( title: 'Fejl', description: getErrorMessage(error), key: const Key('ErrorMessageDialog')); }); }","title":"Catching API errors method"},{"location":"Legacy/weekplanner/Apps/Guidelines/web_api_error_handling/#creating-new-error-message-method","text":"In this method you will need to add a new case for a specific ErrorKey with a return call on an error message string. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ///Get apropriate error message based on error code String getErrorMessage(ApiException error) { switch (error.errorKey) { case ErrorKey.UserAlreadyExists: return 'Brugernavnet eksisterer allerede'; case ErrorKey.Error: // Undefined errors, the message is in english // as we cant predict why it was cast return 'message: ' + error.errorMessage + '\\nDetails: ' + error.errorDetails; default: return 'Ukendt fejl'; } }","title":"Creating new error message method"},{"location":"Legacy/weekplanner/GitHub/","text":"Overview \u00b6 Here is an overview of the things to know when using the GIRAF GitHub. Repository Descriptions \u00b6 A list of descriptions for each of the repositories in GIRAF organization can be found here . GitHub Actions \u00b6 A description of how GitHub actions/workflows are used in the GIRAF repositories can be found here . Using GitHub \u00b6 A description on how to create issues, open pull requests and review code can be found here .","title":"Overview"},{"location":"Legacy/weekplanner/GitHub/#overview","text":"Here is an overview of the things to know when using the GIRAF GitHub.","title":"Overview"},{"location":"Legacy/weekplanner/GitHub/#repository-descriptions","text":"A list of descriptions for each of the repositories in GIRAF organization can be found here .","title":"Repository Descriptions"},{"location":"Legacy/weekplanner/GitHub/#github-actions","text":"A description of how GitHub actions/workflows are used in the GIRAF repositories can be found here .","title":"GitHub Actions"},{"location":"Legacy/weekplanner/GitHub/#using-github","text":"A description on how to create issues, open pull requests and review code can be found here .","title":"Using GitHub"},{"location":"Legacy/weekplanner/GitHub/repository_description/","text":"Repositories \u00b6 There are many different repositories in the GIRAF project, but most of them are archived as of 2019. This section describes first all active repositories and then the archived so it is easy to see what the repositories contain. Active Repositories \u00b6 These repositories are those that as of 2019 are active in the GIRAF project. web-api \u00b6 The repository contains the backend for the GIRAF Project, namely a RESTful API using .NET Core and written in C#. The contents of the repository is further detailed in REST API Introduction . wiki \u00b6 The wiki repository containing information about the GIRAF project, including information for people starting out with GIRAF. The wiki should be updated so it always reflect the current status of the other repositories as well as other relevant artifacts. api_client \u00b6 API client for Flutter to communicate with the web-api (2019: Only weekplanner uses flutter). This is a Dart package that can be used to implement communication with the web-api in any flutter project. Earlier the api_client was located in the weekplanner repository, but it is now used as a package so it can be used in other applications. weekplanner \u00b6 The weekplanner repository can be found here . This is a frontend repository. The Weekplanner application is written in Flutter. The Weekplanner is an application to create and maintain a week schedule of a citizen. This schedule shows a plan of the activities due for the day/week of a citizen and is essentially an important tool. It is a digitisation of an already existing week plan that the guardians maintain on a pin-up board at the institutions. The weekplanner repository contains the Flutter Weekplanner application that is used to compile to both iOS and Android devices. The weekplanner repository is dependent on the api_client repository, which it uses to communicate with the backend, hosted in web-api . web-api-dotnetcore-build \u00b6 Docker containers for the development branch of the webAPI (2019). giraf-production-swarm \u00b6 Repository for the production docker swarm for the entire GIRAF project. This repository consists mainly of a configuration file, which is responsible for telling docker how to initialize a docker stack. The stack contains the proxy service, the API's and the database and runs them in docker containers. The stack is deployed and run on the servers inside a docker swarm. When changes need to be made to the stack, the configuration file needs to be changed accordingly. This repository does not interact with any other reposistories, it is merely used to correctly deploy everything onto the servers. More info can be found in README.md in the swarm repository . pictogram-reader \u00b6 The pictogram repository can be found here . The pictogram-reader should be an application that allows citizens to communicate with the use of pictograms. The app should read the pictograms out loud. For inspiration take a look at the archived repository Pictoreader. This should not be implemented before the weekplanner is done, but the PO group could start working on prototypes, widgets and making issues. This application should be written in Flutter. Archived Repositories \u00b6 These repositories are all archived as of 2019. We keep them as a reference point for future development. category-game \u00b6 The Category Game (also known as \u2018Kategorispillet\u2019 in Danish) is a game that asks the player to group up different pictograms into categories either predefined or defined in the Category Manager. The game itself consists of a train which must drop off the pictograms at different stations according to their grouping. (Giraf2016_ServerDevelopment.pdf) launcher \u00b6 The GIRAF Launcher is meant to replace the default launcher of the device and is used to launch the applications of the GIRAF Application Suite, but could also be used to launch other third party applications. It has a home screen like any android OS has, which is where the applications can be launched from. Additionally the options menu and administration panel is opened from here. (Giraf2016_ServerDevelopment.pdf) life-stories \u00b6 Life Story (also known as \u2018Livshistorier\u2019 in Danish) is similar to the Sequence application but is used to remember what actions were taken by a citizen throughout the day, or as a way of communicating since some citizens have a lot of trouble with their verbal communication. It is meant to give the citizens a way to communicate with other people other than verbally. They can create a set of pictograms showing what they did in the weekends and then show others these stories. (Giraf2016_ServerDevelopment.pdf) pictocreator \u00b6 A tool for creating personal pictograms. These can be created from existing pictograms, camera pictures, or drawn by the user. (SW613F15_Pictosearch.pdf) pictoreader \u00b6 The Pictoreader can display and read out-loud a series of pictograms. (Giraf2016_ServerDevelopment.pdf) timer \u00b6 The timer (also known as \u2018Tidstager\u2019 in Danish) is an application which can limit the amount of time another application is allowed to be active for. This is often useful in combination with the Week Schedule, which can feature elements like, e.g., 10 minutes of playing the Category Game, or how long a third party application, e.g. a video game, could be open for. (Giraf2016_ServerDevelopment.pdf) voice-game \u00b6 Voice Game (also known as \u2018Stemmespillet\u2019 in Danish) is a game in which the purpose is to control a car, either avoiding obstacles or gathering stars, using only your voice. This is supposed to help the citizens learn to control the volume of their voice, as some has troubles with either speaking too loudly too silently. (Giraf2016_ServerDevelopment.pdf) showcaseview-lib \u00b6 Unused repository. pictosearch-lib \u00b6 The PictoSearch-Lib is a library containing all classes relating to the search of pictograms. (SW609F17) pictogram-lib \u00b6 This project contain all classes that is used for the pictograms. (sw604f14) giraf-component-lib \u00b6 This repository contains a lot of the essential functionality for the GIRAF project, such as GUI elements and classes for the graphics of applications (SW609F17) weekplanner-test \u00b6 Fork of weekplanner, unused. old Server \u00b6 Old (pre-2019) images for running the server, containes Docker, Kickstart and Kubernetes images. deployment \u00b6 Server deployment tools for Kubernetes (Outdated) server \u00b6 Old server deployment tools using Kubernetes, includes old backup scripts. (Outdated) Category Manager \u00b6 A tool for managing categories of pictograms for each citizen. This include adding, editing and deleting categories. (SW613F15_Pictosearch.pdf) Xamarin Android \u00b6 Image for compiling WeekPlanner Android project. Sequence \u00b6 A tool for creating, and viewing, sequences of pictograms. A sequence is a set of pictograms, defining a sequence of actions for citizens. (SW613F15_Pictosearch.pdf)","title":"Repositories"},{"location":"Legacy/weekplanner/GitHub/repository_description/#repositories","text":"There are many different repositories in the GIRAF project, but most of them are archived as of 2019. This section describes first all active repositories and then the archived so it is easy to see what the repositories contain.","title":"Repositories"},{"location":"Legacy/weekplanner/GitHub/repository_description/#active-repositories","text":"These repositories are those that as of 2019 are active in the GIRAF project.","title":"Active Repositories"},{"location":"Legacy/weekplanner/GitHub/repository_description/#web-api","text":"The repository contains the backend for the GIRAF Project, namely a RESTful API using .NET Core and written in C#. The contents of the repository is further detailed in REST API Introduction .","title":"web-api"},{"location":"Legacy/weekplanner/GitHub/repository_description/#wiki","text":"The wiki repository containing information about the GIRAF project, including information for people starting out with GIRAF. The wiki should be updated so it always reflect the current status of the other repositories as well as other relevant artifacts.","title":"wiki"},{"location":"Legacy/weekplanner/GitHub/repository_description/#api_client","text":"API client for Flutter to communicate with the web-api (2019: Only weekplanner uses flutter). This is a Dart package that can be used to implement communication with the web-api in any flutter project. Earlier the api_client was located in the weekplanner repository, but it is now used as a package so it can be used in other applications.","title":"api_client"},{"location":"Legacy/weekplanner/GitHub/repository_description/#weekplanner","text":"The weekplanner repository can be found here . This is a frontend repository. The Weekplanner application is written in Flutter. The Weekplanner is an application to create and maintain a week schedule of a citizen. This schedule shows a plan of the activities due for the day/week of a citizen and is essentially an important tool. It is a digitisation of an already existing week plan that the guardians maintain on a pin-up board at the institutions. The weekplanner repository contains the Flutter Weekplanner application that is used to compile to both iOS and Android devices. The weekplanner repository is dependent on the api_client repository, which it uses to communicate with the backend, hosted in web-api .","title":"weekplanner"},{"location":"Legacy/weekplanner/GitHub/repository_description/#web-api-dotnetcore-build","text":"Docker containers for the development branch of the webAPI (2019).","title":"web-api-dotnetcore-build"},{"location":"Legacy/weekplanner/GitHub/repository_description/#giraf-production-swarm","text":"Repository for the production docker swarm for the entire GIRAF project. This repository consists mainly of a configuration file, which is responsible for telling docker how to initialize a docker stack. The stack contains the proxy service, the API's and the database and runs them in docker containers. The stack is deployed and run on the servers inside a docker swarm. When changes need to be made to the stack, the configuration file needs to be changed accordingly. This repository does not interact with any other reposistories, it is merely used to correctly deploy everything onto the servers. More info can be found in README.md in the swarm repository .","title":"giraf-production-swarm"},{"location":"Legacy/weekplanner/GitHub/repository_description/#pictogram-reader","text":"The pictogram repository can be found here . The pictogram-reader should be an application that allows citizens to communicate with the use of pictograms. The app should read the pictograms out loud. For inspiration take a look at the archived repository Pictoreader. This should not be implemented before the weekplanner is done, but the PO group could start working on prototypes, widgets and making issues. This application should be written in Flutter.","title":"pictogram-reader"},{"location":"Legacy/weekplanner/GitHub/repository_description/#archived-repositories","text":"These repositories are all archived as of 2019. We keep them as a reference point for future development.","title":"Archived Repositories"},{"location":"Legacy/weekplanner/GitHub/repository_description/#category-game","text":"The Category Game (also known as \u2018Kategorispillet\u2019 in Danish) is a game that asks the player to group up different pictograms into categories either predefined or defined in the Category Manager. The game itself consists of a train which must drop off the pictograms at different stations according to their grouping. (Giraf2016_ServerDevelopment.pdf)","title":"category-game"},{"location":"Legacy/weekplanner/GitHub/repository_description/#launcher","text":"The GIRAF Launcher is meant to replace the default launcher of the device and is used to launch the applications of the GIRAF Application Suite, but could also be used to launch other third party applications. It has a home screen like any android OS has, which is where the applications can be launched from. Additionally the options menu and administration panel is opened from here. (Giraf2016_ServerDevelopment.pdf)","title":"launcher"},{"location":"Legacy/weekplanner/GitHub/repository_description/#life-stories","text":"Life Story (also known as \u2018Livshistorier\u2019 in Danish) is similar to the Sequence application but is used to remember what actions were taken by a citizen throughout the day, or as a way of communicating since some citizens have a lot of trouble with their verbal communication. It is meant to give the citizens a way to communicate with other people other than verbally. They can create a set of pictograms showing what they did in the weekends and then show others these stories. (Giraf2016_ServerDevelopment.pdf)","title":"life-stories"},{"location":"Legacy/weekplanner/GitHub/repository_description/#pictocreator","text":"A tool for creating personal pictograms. These can be created from existing pictograms, camera pictures, or drawn by the user. (SW613F15_Pictosearch.pdf)","title":"pictocreator"},{"location":"Legacy/weekplanner/GitHub/repository_description/#pictoreader","text":"The Pictoreader can display and read out-loud a series of pictograms. (Giraf2016_ServerDevelopment.pdf)","title":"pictoreader"},{"location":"Legacy/weekplanner/GitHub/repository_description/#timer","text":"The timer (also known as \u2018Tidstager\u2019 in Danish) is an application which can limit the amount of time another application is allowed to be active for. This is often useful in combination with the Week Schedule, which can feature elements like, e.g., 10 minutes of playing the Category Game, or how long a third party application, e.g. a video game, could be open for. (Giraf2016_ServerDevelopment.pdf)","title":"timer"},{"location":"Legacy/weekplanner/GitHub/repository_description/#voice-game","text":"Voice Game (also known as \u2018Stemmespillet\u2019 in Danish) is a game in which the purpose is to control a car, either avoiding obstacles or gathering stars, using only your voice. This is supposed to help the citizens learn to control the volume of their voice, as some has troubles with either speaking too loudly too silently. (Giraf2016_ServerDevelopment.pdf)","title":"voice-game"},{"location":"Legacy/weekplanner/GitHub/repository_description/#showcaseview-lib","text":"Unused repository.","title":"showcaseview-lib"},{"location":"Legacy/weekplanner/GitHub/repository_description/#pictosearch-lib","text":"The PictoSearch-Lib is a library containing all classes relating to the search of pictograms. (SW609F17)","title":"pictosearch-lib"},{"location":"Legacy/weekplanner/GitHub/repository_description/#pictogram-lib","text":"This project contain all classes that is used for the pictograms. (sw604f14)","title":"pictogram-lib"},{"location":"Legacy/weekplanner/GitHub/repository_description/#giraf-component-lib","text":"This repository contains a lot of the essential functionality for the GIRAF project, such as GUI elements and classes for the graphics of applications (SW609F17)","title":"giraf-component-lib"},{"location":"Legacy/weekplanner/GitHub/repository_description/#weekplanner-test","text":"Fork of weekplanner, unused.","title":"weekplanner-test"},{"location":"Legacy/weekplanner/GitHub/repository_description/#old-server","text":"Old (pre-2019) images for running the server, containes Docker, Kickstart and Kubernetes images.","title":"old Server"},{"location":"Legacy/weekplanner/GitHub/repository_description/#deployment","text":"Server deployment tools for Kubernetes (Outdated)","title":"deployment"},{"location":"Legacy/weekplanner/GitHub/repository_description/#server","text":"Old server deployment tools using Kubernetes, includes old backup scripts. (Outdated)","title":"server"},{"location":"Legacy/weekplanner/GitHub/repository_description/#category-manager","text":"A tool for managing categories of pictograms for each citizen. This include adding, editing and deleting categories. (SW613F15_Pictosearch.pdf)","title":"Category Manager"},{"location":"Legacy/weekplanner/GitHub/repository_description/#xamarin-android","text":"Image for compiling WeekPlanner Android project.","title":"Xamarin Android"},{"location":"Legacy/weekplanner/GitHub/repository_description/#sequence","text":"A tool for creating, and viewing, sequences of pictograms. A sequence is a set of pictograms, defining a sequence of actions for citizens. (SW613F15_Pictosearch.pdf)","title":"Sequence"},{"location":"Legacy/weekplanner/GitHub/using_github/","text":"Using GitHub in GIRAF \u00b6 This section describes how to use GitHub when you a part of GIRAF. Issues \u00b6 Issues can be created by anyone in the GIRAF team. An issue can be a bug report or a task creation request. The list of issues can be seen at each repository, eg. https://github.com/aau-giraf/weekplanner/issues , or a complete list for the whole organization. Creating an Issue \u00b6 If you find a bug, or have a task creation request you can create an issue: Go the the \"Issues\" tab of the relevant repository (E.g. https://github.com/aau-giraf/weekplanner/issues ). Press the green \"New issue\" button. Choose whether to submit a bug report or task creation request, and press \"Get started\". Create a title and description for the issue. Please follow the template, and do not delete the headers! The title for the Task Creation Request should tell what functionality you would like added using the shown form \"As a developer I would like the docker config file to automatically update so that I don\u00b4t have to manually update the config file\". Instead of the task being for the developer, guardian or user is also frequently used. Label the issue with appropriate labels. It can be a good idea to inform the PO group when you are done, so they can assign and refine the issue. Branches and Pull Requests \u00b6 In GIRAF there is used the branching strategy called GitFlow for all the repositories. A visual representation can be seen underneath. Working on an Issue \u00b6 When you want to work on an issue you need to create a feature branch from the develop branch. The naming convention for feature branches is feature/xx where xx is replaced by an issue number. Release Preparation \u00b6 When a Release Preparation phase begins, a release branch is created from the develop branch. This branch is now used instead of develop until the release is finished. The naming convention for release branches is release/<semester name>s<sprint no.>r<release no.> : <semester name> is name of a semester e.g. 2020E, 2019 etc. <sprint no.> the number of the sprint where the release is created <release no.> the release number. The number starts from 1 and is reset when starting on a new sprint. E.g. release/2020Es1r1 for semester 2020E, sprint 1, release 1. Release fix \u00b6 When you start working on a release fix, you create a branch from a release branch e.g release/2020Es1r1 . The naming convention for the branch is releasefix/xx where xx is an issue number. Creating a Branch \u00b6 From the terminal: 1 2 git checkout develop # The parent branch git checkout -b feature/xx # The new branch Or from GitHub: Make sure the right parent branch is selected (e.g. develop ). Input the name of the new branch (e.g. feature/400 ). Press \"Create branch: from ' '\" Creating a Pull Request \u00b6 When you have finished your issue, it is time to create a pull request. A pull request is a request to merge your branch into another branch. Before making the pull request, make sure that the code complies with the checklist for given repository (e.g. 2020E semester checklists ). From GitHub: Open the \"Pull requests\" tab in the repository (e.g. https://github.com/aau-giraf/weekplanner/pulls ) Press \"New pull request\" Select the appropriate parent branch as base. develop in the Development Phase release/* in the Release Preparation Select your branch. Press \"Create pull request\" Name the pull request Feature xx or Feature xx: A title describing changes Write a description If you write closes #xx or fixes #xx , issue xx will be linked to the PR. The linked issues will then be closed the PR is merged. (All keywords can be seen here ) Code Review \u00b6 You can find the pull requests you have been requested to review here . Checklist \u00b6 While reviewing the changes you should use the checklist for a given repository. It is usually given as a comment on the pull request as seen below. If it is not, you can insert it yourself (e.g. 2020E semester checklists ). Remember to tag yourself with @GitHub username , so everyone can see who the checklist belongs to. Changed Files \u00b6 All the changes associated with a pull request can be found under the Files changed tab as shown underneath. While reviewing the code you can make comments or suggestions for a single line or multiple lines by pressing the blue + icon (move the cursor to a line). The red square marks the selection icon which can be used to suggest code that replaces the line(s). You can view what the author will see by clicking Preview . Give a Review \u00b6 When you are ready to give your review you press the Review changes button. Afterwards you are given 3 choices: Comment If you have comments or questions to the changes in the pull request you select this option. However, comments can also be made directly to a pull request without giving a review. Approve If everything is ok, then select this option. Request changes You select this, if you have made comments to lines in the code or if something should be made different. When you think your review is done select Submit review . Re-review \u00b6 If you have given a review and the author of the pull request makes a new commit, then you have to re-review the code.","title":"Using GitHub in GIRAF"},{"location":"Legacy/weekplanner/GitHub/using_github/#using-github-in-giraf","text":"This section describes how to use GitHub when you a part of GIRAF.","title":"Using GitHub in GIRAF"},{"location":"Legacy/weekplanner/GitHub/using_github/#issues","text":"Issues can be created by anyone in the GIRAF team. An issue can be a bug report or a task creation request. The list of issues can be seen at each repository, eg. https://github.com/aau-giraf/weekplanner/issues , or a complete list for the whole organization.","title":"Issues"},{"location":"Legacy/weekplanner/GitHub/using_github/#creating-an-issue","text":"If you find a bug, or have a task creation request you can create an issue: Go the the \"Issues\" tab of the relevant repository (E.g. https://github.com/aau-giraf/weekplanner/issues ). Press the green \"New issue\" button. Choose whether to submit a bug report or task creation request, and press \"Get started\". Create a title and description for the issue. Please follow the template, and do not delete the headers! The title for the Task Creation Request should tell what functionality you would like added using the shown form \"As a developer I would like the docker config file to automatically update so that I don\u00b4t have to manually update the config file\". Instead of the task being for the developer, guardian or user is also frequently used. Label the issue with appropriate labels. It can be a good idea to inform the PO group when you are done, so they can assign and refine the issue.","title":"Creating an Issue"},{"location":"Legacy/weekplanner/GitHub/using_github/#branches-and-pull-requests","text":"In GIRAF there is used the branching strategy called GitFlow for all the repositories. A visual representation can be seen underneath.","title":"Branches and Pull Requests"},{"location":"Legacy/weekplanner/GitHub/using_github/#working-on-an-issue","text":"When you want to work on an issue you need to create a feature branch from the develop branch. The naming convention for feature branches is feature/xx where xx is replaced by an issue number.","title":"Working on an Issue"},{"location":"Legacy/weekplanner/GitHub/using_github/#release-preparation","text":"When a Release Preparation phase begins, a release branch is created from the develop branch. This branch is now used instead of develop until the release is finished. The naming convention for release branches is release/<semester name>s<sprint no.>r<release no.> : <semester name> is name of a semester e.g. 2020E, 2019 etc. <sprint no.> the number of the sprint where the release is created <release no.> the release number. The number starts from 1 and is reset when starting on a new sprint. E.g. release/2020Es1r1 for semester 2020E, sprint 1, release 1.","title":"Release Preparation"},{"location":"Legacy/weekplanner/GitHub/using_github/#release-fix","text":"When you start working on a release fix, you create a branch from a release branch e.g release/2020Es1r1 . The naming convention for the branch is releasefix/xx where xx is an issue number.","title":"Release fix"},{"location":"Legacy/weekplanner/GitHub/using_github/#creating-a-branch","text":"From the terminal: 1 2 git checkout develop # The parent branch git checkout -b feature/xx # The new branch Or from GitHub: Make sure the right parent branch is selected (e.g. develop ). Input the name of the new branch (e.g. feature/400 ). Press \"Create branch: from ' '\"","title":"Creating a Branch"},{"location":"Legacy/weekplanner/GitHub/using_github/#creating-a-pull-request","text":"When you have finished your issue, it is time to create a pull request. A pull request is a request to merge your branch into another branch. Before making the pull request, make sure that the code complies with the checklist for given repository (e.g. 2020E semester checklists ). From GitHub: Open the \"Pull requests\" tab in the repository (e.g. https://github.com/aau-giraf/weekplanner/pulls ) Press \"New pull request\" Select the appropriate parent branch as base. develop in the Development Phase release/* in the Release Preparation Select your branch. Press \"Create pull request\" Name the pull request Feature xx or Feature xx: A title describing changes Write a description If you write closes #xx or fixes #xx , issue xx will be linked to the PR. The linked issues will then be closed the PR is merged. (All keywords can be seen here )","title":"Creating a Pull Request"},{"location":"Legacy/weekplanner/GitHub/using_github/#code-review","text":"You can find the pull requests you have been requested to review here .","title":"Code Review"},{"location":"Legacy/weekplanner/GitHub/using_github/#checklist","text":"While reviewing the changes you should use the checklist for a given repository. It is usually given as a comment on the pull request as seen below. If it is not, you can insert it yourself (e.g. 2020E semester checklists ). Remember to tag yourself with @GitHub username , so everyone can see who the checklist belongs to.","title":"Checklist"},{"location":"Legacy/weekplanner/GitHub/using_github/#changed-files","text":"All the changes associated with a pull request can be found under the Files changed tab as shown underneath. While reviewing the code you can make comments or suggestions for a single line or multiple lines by pressing the blue + icon (move the cursor to a line). The red square marks the selection icon which can be used to suggest code that replaces the line(s). You can view what the author will see by clicking Preview .","title":"Changed Files"},{"location":"Legacy/weekplanner/GitHub/using_github/#give-a-review","text":"When you are ready to give your review you press the Review changes button. Afterwards you are given 3 choices: Comment If you have comments or questions to the changes in the pull request you select this option. However, comments can also be made directly to a pull request without giving a review. Approve If everything is ok, then select this option. Request changes You select this, if you have made comments to lines in the code or if something should be made different. When you think your review is done select Submit review .","title":"Give a Review"},{"location":"Legacy/weekplanner/GitHub/using_github/#re-review","text":"If you have given a review and the author of the pull request makes a new commit, then you have to re-review the code.","title":"Re-review"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/","text":"Overview \u00b6 This sections gives an overview of the features that are associated with GitHub actions/workflows. Code Coverage \u00b6 A description on how code coverage is used in the repositories can be found here . Workflows in the Repositories \u00b6 A description of the different workflows that has been created for each repository in GIRAF can be found here .","title":"Overview"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/#overview","text":"This sections gives an overview of the features that are associated with GitHub actions/workflows.","title":"Overview"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/#code-coverage","text":"A description on how code coverage is used in the repositories can be found here .","title":"Code Coverage"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/#workflows-in-the-repositories","text":"A description of the different workflows that has been created for each repository in GIRAF can be found here .","title":"Workflows in the Repositories"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/code_coverage/","text":"Code Coverage \u00b6 To make sure that new code is tested, we keep track of the code coverage. To do this we use Codecov.io . The Codecov page for GIRAF is: https://codecov.io/gh/aau-giraf . Usage \u00b6 This section shortly explains how to use Codecov Codecov.io \u00b6 On the Codecov site , the GIRAF repositories using Codecov can be seen. In the different repositories, multiple things can be seen. For example, the code coverage, and changes in code coverage for every commit. Pull Request Comments \u00b6 On every pull request, Codecov will comment on the pull request. This comment is updated when new commits are added to the pull request. Setup \u00b6 GitHub Actions Workflows \u00b6 Flutter Repositories \u00b6 In Flutter repositories, the --coverage argument is added to the flutter test step in the workflow. This generates a coverage file, which is then uploaded by the next step: 1 2 3 4 5 6 ... - run : flutter test --coverage name : Tests - name : Upload coverage to Codecov uses : codecov/codecov-action@v1 ... .NET Repositories \u00b6 In the .NET repositories, the package coverlet.msbuild is used to generate code coverage reports. The package is installed with in GirafRest.Test . Then in the workflow, in the test step, some arguments are added. 1 2 3 4 - name : Test run : dotnet test /p:CollectCoverage=true /p:CoverletOutputFormat=lcov /p:ExcludeByFile=\\\"**/Migrations/**/*\\\" - name : Upload coverage to Codecov uses : codecov/codecov-action@v1.0.6 Beneath is an explanation of the arguments: 1 2 3 - /p:CollectCoverage=true | enables code coverage collection - /p:CoverletOutputFormat=lcov | sets the format of the report to lcov, which can be used by codecov - /p:ExcludeByFile=\\\"**/Migrations/**/*\\\" | excludes the Migrations folder from the report Config \u00b6 The codecov integration is configured in the codecov.yml file in the respective repositories. Documentation for the config file can be seen at https://docs.codecov.io/docs/codecov-yaml . Affected Repositories \u00b6 Right now codecov is used in: weekplanner api_client web-api","title":"Code Coverage"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/code_coverage/#code-coverage","text":"To make sure that new code is tested, we keep track of the code coverage. To do this we use Codecov.io . The Codecov page for GIRAF is: https://codecov.io/gh/aau-giraf .","title":"Code Coverage"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/code_coverage/#usage","text":"This section shortly explains how to use Codecov","title":"Usage"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/code_coverage/#codecovio","text":"On the Codecov site , the GIRAF repositories using Codecov can be seen. In the different repositories, multiple things can be seen. For example, the code coverage, and changes in code coverage for every commit.","title":"Codecov.io"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/code_coverage/#pull-request-comments","text":"On every pull request, Codecov will comment on the pull request. This comment is updated when new commits are added to the pull request.","title":"Pull Request Comments"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/code_coverage/#setup","text":"","title":"Setup"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/code_coverage/#github-actions-workflows","text":"","title":"GitHub Actions Workflows"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/code_coverage/#flutter-repositories","text":"In Flutter repositories, the --coverage argument is added to the flutter test step in the workflow. This generates a coverage file, which is then uploaded by the next step: 1 2 3 4 5 6 ... - run : flutter test --coverage name : Tests - name : Upload coverage to Codecov uses : codecov/codecov-action@v1 ...","title":"Flutter Repositories"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/code_coverage/#net-repositories","text":"In the .NET repositories, the package coverlet.msbuild is used to generate code coverage reports. The package is installed with in GirafRest.Test . Then in the workflow, in the test step, some arguments are added. 1 2 3 4 - name : Test run : dotnet test /p:CollectCoverage=true /p:CoverletOutputFormat=lcov /p:ExcludeByFile=\\\"**/Migrations/**/*\\\" - name : Upload coverage to Codecov uses : codecov/codecov-action@v1.0.6 Beneath is an explanation of the arguments: 1 2 3 - /p:CollectCoverage=true | enables code coverage collection - /p:CoverletOutputFormat=lcov | sets the format of the report to lcov, which can be used by codecov - /p:ExcludeByFile=\\\"**/Migrations/**/*\\\" | excludes the Migrations folder from the report","title":".NET Repositories"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/code_coverage/#config","text":"The codecov integration is configured in the codecov.yml file in the respective repositories. Documentation for the config file can be seen at https://docs.codecov.io/docs/codecov-yaml .","title":"Config"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/code_coverage/#affected-repositories","text":"Right now codecov is used in: weekplanner api_client web-api","title":"Affected Repositories"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/workflows_in_repositories/","text":"Workflows in the Repositories \u00b6 The currently used Continuous Integration Pipeline is GitHub Actions, which provides 2.000 minutes free per month. It can run on either Ubuntu, MacOS, or Windows, allowing us to test multiple platforms from our CI-Platform without cost. web-api \u00b6 This job is built in two tasks: dotnettest.yml and dockerimage.yml Similar to the api_client, the web-api suite runs by building a dotnet system in Release mode, to ensure deployability, and then runs dotnet test , to run the embedded GirafRest.Test project and the unit tests contained. GitHub Packages \u00b6 In addition to running unit tests, when pushed to any branch GitHub Actions starts another job, starts a build of the embedded Dockerfile, and tags this accordingly as either: master : aau-giraf/web-api/web-api:latest develop : aau-giraf/web-api/web-api:develop issue/123 : aau-giraf/web-api/web-api:issue-123 When deploying into the production swarm , these images are pulled into DEV and PROD environments accordingly. wiki \u00b6 This job is built in one task: page-build.yml As the wiki is currently built using MKDocs, this repository is set up with a GitHub Actions script to build the source files, markdown, into a static HTML page, and push this into the GitHub Pages-environment, allowing it to be reachable at http://giraf-aau.github.io/wiki . This is run using an existing GitHub Action, mhausenblas/mkdocs-deploy-gh-pages , thus being the simplest of the builds.","title":"Workflows in the Repositories"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/workflows_in_repositories/#workflows-in-the-repositories","text":"The currently used Continuous Integration Pipeline is GitHub Actions, which provides 2.000 minutes free per month. It can run on either Ubuntu, MacOS, or Windows, allowing us to test multiple platforms from our CI-Platform without cost.","title":"Workflows in the Repositories"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/workflows_in_repositories/#web-api","text":"This job is built in two tasks: dotnettest.yml and dockerimage.yml Similar to the api_client, the web-api suite runs by building a dotnet system in Release mode, to ensure deployability, and then runs dotnet test , to run the embedded GirafRest.Test project and the unit tests contained.","title":"web-api"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/workflows_in_repositories/#github-packages","text":"In addition to running unit tests, when pushed to any branch GitHub Actions starts another job, starts a build of the embedded Dockerfile, and tags this accordingly as either: master : aau-giraf/web-api/web-api:latest develop : aau-giraf/web-api/web-api:develop issue/123 : aau-giraf/web-api/web-api:issue-123 When deploying into the production swarm , these images are pulled into DEV and PROD environments accordingly.","title":"GitHub Packages"},{"location":"Legacy/weekplanner/GitHub/GitHub_Actions/workflows_in_repositories/#wiki","text":"This job is built in one task: page-build.yml As the wiki is currently built using MKDocs, this repository is set up with a GitHub Actions script to build the source files, markdown, into a static HTML page, and push this into the GitHub Pages-environment, allowing it to be reachable at http://giraf-aau.github.io/wiki . This is run using an existing GitHub Action, mhausenblas/mkdocs-deploy-gh-pages , thus being the simplest of the builds.","title":"wiki"},{"location":"Legacy/weekplanner/Server/","text":"Overview \u00b6 This section contains information about and how to work with the servers in the GIRAF project. Architecture An overview of the servers used in the GIRAF project, and their architecture. Access Information about how to get access to the servers. Docker Information about the Docker setup for the GIRAF project.","title":"Overview"},{"location":"Legacy/weekplanner/Server/#overview","text":"This section contains information about and how to work with the servers in the GIRAF project. Architecture An overview of the servers used in the GIRAF project, and their architecture. Access Information about how to get access to the servers. Docker Information about the Docker setup for the GIRAF project.","title":"Overview"},{"location":"Legacy/weekplanner/Server/access/","text":"Access \u00b6 In the following we will describe how to take ownership of the Giraf Servers for the Giraf Project. We will also describe how to access the servers once ownership has been transferred and how to pass on the servers to a new semester of Giraf Developers. Ownership Transfer \u00b6 Once a new semester needs to take ownership of the servers for the Giraf project an email with the new admins student mails must be send to support@its.aau.dk with the Semester Coordinator Ulrik Nyman ( ulrik@cs.aau.dk ) as CC. Once Ulrik has verified the new admins they will be added to the Giraf Server Admins Active Directory group at ITS. System \u00b6 Once added to the group each admin can access the servers via SSH with the following command: 1 ssh -t sshgw.aau.dk -l <USERNAME>@student.aau.dk ssh 172 .19.10.<IP> The servers are on the following IP's: Name Internal IP External IP giraf-master00.srv.aau.dk 172.19.10.29 192.38.56.151 giraf-master01.srv.aau.dk 172.19.10.30 192.38.56.153 giraf-node00.srv.aau.dk 172.19.10.31 N/A giraf-node01.srv.aau.dk 172.19.10.32 N/A giraf-node02.srv.aau.dk 172.19.10.33 N/A giraf-node03.srv.aau.dk 172.19.10.34 N/A The entire project has two public IPs and with has a DNS A record to srv.giraf.cs.aau.dk. Power Management \u00b6 If the servers does not power on after a system reboot the power management can be accessed via the vSphere Client on https://esx-vcsa03.srv.aau.dk/ui/ . To see the servers goto Menu -> VMs and Templates Access Problems \u00b6 The vSphere Client site is only accessible from inside the AAU network and even is not guaranteed to be accessible by ITS. To be sure the user must connect via VPN to AAU. On Linux Mint the package network-manager-openconnect-gnome must be installed to allow for access to the Cisco VPN. Once installed add a new VPN of type CiscoAnyConnect and specify ssl-vpn1.aau.dk as the gateway. When trying to connect the network manager will ask for your username and password, once provided an external passcode must be entered. To get this the user must sign up for MSA passcode at https://mfa.aau.dk/ just use SMS if nothing else is preferred. Pass on the Servers \u00b6 The admins are not removed from the admin list before they leave AAU and their student mail is deleted. Hence the admins are expected to help the new semester doing the project start up and give an short introduction to the work that has been done.","title":"Access"},{"location":"Legacy/weekplanner/Server/access/#access","text":"In the following we will describe how to take ownership of the Giraf Servers for the Giraf Project. We will also describe how to access the servers once ownership has been transferred and how to pass on the servers to a new semester of Giraf Developers.","title":"Access"},{"location":"Legacy/weekplanner/Server/access/#ownership-transfer","text":"Once a new semester needs to take ownership of the servers for the Giraf project an email with the new admins student mails must be send to support@its.aau.dk with the Semester Coordinator Ulrik Nyman ( ulrik@cs.aau.dk ) as CC. Once Ulrik has verified the new admins they will be added to the Giraf Server Admins Active Directory group at ITS.","title":"Ownership Transfer"},{"location":"Legacy/weekplanner/Server/access/#system","text":"Once added to the group each admin can access the servers via SSH with the following command: 1 ssh -t sshgw.aau.dk -l <USERNAME>@student.aau.dk ssh 172 .19.10.<IP> The servers are on the following IP's: Name Internal IP External IP giraf-master00.srv.aau.dk 172.19.10.29 192.38.56.151 giraf-master01.srv.aau.dk 172.19.10.30 192.38.56.153 giraf-node00.srv.aau.dk 172.19.10.31 N/A giraf-node01.srv.aau.dk 172.19.10.32 N/A giraf-node02.srv.aau.dk 172.19.10.33 N/A giraf-node03.srv.aau.dk 172.19.10.34 N/A The entire project has two public IPs and with has a DNS A record to srv.giraf.cs.aau.dk.","title":"System"},{"location":"Legacy/weekplanner/Server/access/#power-management","text":"If the servers does not power on after a system reboot the power management can be accessed via the vSphere Client on https://esx-vcsa03.srv.aau.dk/ui/ . To see the servers goto Menu -> VMs and Templates","title":"Power Management"},{"location":"Legacy/weekplanner/Server/access/#access-problems","text":"The vSphere Client site is only accessible from inside the AAU network and even is not guaranteed to be accessible by ITS. To be sure the user must connect via VPN to AAU. On Linux Mint the package network-manager-openconnect-gnome must be installed to allow for access to the Cisco VPN. Once installed add a new VPN of type CiscoAnyConnect and specify ssl-vpn1.aau.dk as the gateway. When trying to connect the network manager will ask for your username and password, once provided an external passcode must be entered. To get this the user must sign up for MSA passcode at https://mfa.aau.dk/ just use SMS if nothing else is preferred.","title":"Access Problems"},{"location":"Legacy/weekplanner/Server/access/#pass-on-the-servers","text":"The admins are not removed from the admin list before they leave AAU and their student mail is deleted. Hence the admins are expected to help the new semester doing the project start up and give an short introduction to the work that has been done.","title":"Pass on the Servers"},{"location":"Legacy/weekplanner/Server/architecture/","text":"Architecture \u00b6 In the following, the server architecture for the giraf project will be explained. Old Servers (DISCONTINUED) \u00b6 Name IP Specs Master00 192.38.56.37 2 GB RAM 2xCPU Disk: 22 GB OS: CentOS Linux release 7.5.1804 (Core) Node01 172.19.0.244 2 GB RAM 1xCPU Disk: 22 GB OS: CentOS Linux release 7.5.1804 (Core) Node02 172.19.0.245 2 GB RAM 1xCPU Disk: 22 GB OS: CentOS Linux release 7.5.1804 (Core) Node03 192.38.56.36 2 GB RAM 1xCPU Disk: 22 GB OS: CentOS Linux release 7.5.1804 (Core) GitLab 192.38.56.136 4 GB RAM 2xCPU Disk: 46 GB OS: CentOS Linux release 7.5.1804 (Core) web01 192.38.56.38 2 GB RAM 1xCPU Disk: 22 GB OS: CentOS Linux release 7.4.1708 (Core) Backup01 172.19.0.235 4 GB RAM 2xCPU Disk: 10 GB OS: CentOS Linux release 7.2.1511 (Core) The only user on these servers are root, and each server has everything open to the internet and is hence under heavy attack from malicious users trying to brute-force the passwords. New Servers \u00b6 Name Internal IP External IP Specs giraf-master00.srv.aau.dk 172.19.10.29 192.38.56.151 Ram: 4 GB 2xCPU Disk: 24 GB OS: Ubuntu Server 18.04.2 giraf-master01.srv.aau.dk 172.19.10.30 192.38.56.153 Ram: 4 GB 2xCPU Disk: 24 GB OS: Ubuntu Server 18.04.2 giraf-node00.srv.aau.dk 172.19.10.31 N/A Ram: 2 GB 1xCPU Disk: 24 GB OS: Ubuntu Server 18.04.2 giraf-node01.srv.aau.dk 172.19.10.32 N/A Ram: 2 GB 1xCPU Disk: 24 GB OS: Ubuntu Server 18.04.2 giraf-node02.srv.aau.dk 172.19.10.33 N/A Ram: 2 GB 1xCPU Disk: 24 GB OS: Ubuntu Server 18.04.2 giraf-node03.srv.aau.dk 172.19.10.34 N/A Ram: 2 GB 1xCPU Disk: 24 GB OS: Ubuntu Server 18.04.2 The two public IP's for the project only has port 80 and port 443 open. Each node has been configured to use the 10.14.0.0/16 subnet for the local docker daemon. For the swarm overlay network, the 10.10.0.0/16 subnet is used. Web-API \u00b6 The WEB API runs in a docker container, which is routed by NGINX. The WEB API is only available on port 80 and 443 on the URLs shown below. Stage URL Production http://srv.giraf.cs.aau.dk/PROD/API Development http://srv.giraf.cs.aau.dk/DEV/API Test http://srv.giraf.cs.aau.dk/TEST/API Network Drives \u00b6 ITS is responsible for the NFS that is mounted on all the nodes and masters in /swarm-nfs/ . As mentioned above, ITS will attach a network drive at /swarm-nfs/ , which should include the following: api/ appsettings.Develop.json appsettings.Production.json appsettings.Testing.json backup/ cdn/ dev/ pictograms/ test/ pictograms/ prod/ pictograms/ certbot/ mysql/ nginx/ certs/ sites-enabled/ nginx.conf Furthermore, the master00 server, should execute the following cronjob: 1 certbot renew --webroot -w /swarm-nfs/certbot/ -d srv.giraf.cs.aau.dk --post-hook \"cp -RL /etc/letsencrypt/live/srv.giraf.cs.aau.dk/. /swarm-nfs/nginx/certs/\" This is done to ensure a single point of certificate-authority on the first masterserver, that after renewing the certificate, moves it into the /swarm-nfs/nginx/certs. This is done to ensure that the certificate is available on all of the servers, and that the nginx Giraf_PROXY service mounts this folder and the certificate. The Docker Setup \u00b6 More information on Docker can found here . Nodes \u00b6 ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION yny9ky6b6zczqrjzxd7sl71k6 * giraf-master00.srv.aau.dk Ready Active Leader 18.09.3 2n08r588w9p8xazc5cm8r6o9o giraf-master01.srv.aau.dk Ready Active Reachable 18.09.3 wrr68nqt116tk1rszwvdv1nmk giraf-node00.srv.aau.dk Ready Active 18.09.3 bhh5mitvwzdhzbky1cjne9ffg giraf-node01.srv.aau.dk Ready Active 18.09.3 as7n375y2gwcj5vf4h73h9ron giraf-node02.srv.aau.dk Ready Active 18.09.3 koclcs8nxt0y6qu4ho511la0m giraf-node03.srv.aau.dk Ready Active 18.09.3 Network \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 [ { \"Name\" : \"ingress\" , \"Id\" : \"esq3c70wonolez79eifsqeqhc\" , \"Created\" : \"2019-03-15T09:32:18.169968631+01:00\" , \"Scope\" : \"swarm\" , \"Driver\" : \"overlay\" , \"EnableIPv6\" : false, \"IPAM\" : { \"Driver\" : \"default\" , \"Options\" : null, \"Config\" : [ { \"Subnet\" : \"10.255.0.0/16\" , \"Gateway\" : \"10.255.0.1\" } ] } , \"Internal\" : false, \"Attachable\" : false, \"Ingress\" : true, \"ConfigFrom\" : { \"Network\" : \"\" } , \"ConfigOnly\" : false, \"Containers\" : { \"ingress-sbox\" : { \"Name\" : \"ingress-endpoint\" , \"EndpointID\" : \"f8f1417b758bc994a9c77aeda536b0a9e4294226baf29117382913e88bac8702\" , \"MacAddress\" : \"02:42:0a:ff:00:02\" , \"IPv4Address\" : \"10.255.0.2/16\" , \"IPv6Address\" : \"\" } } , \"Options\" : { \"com.docker.network.driver.overlay.vxlanid_list\" : \"4096\" } , \"Labels\" : {} , \"Peers\" : [ { \"Name\" : \"b6a2fb9fd60e\" , \"IP\" : \"172.19.10.29\" } , { \"Name\" : \"e17ba5bebf00\" , \"IP\" : \"172.19.10.31\" } , { \"Name\" : \"8a720eeaec16\" , \"IP\" : \"172.19.10.32\" } , { \"Name\" : \"9a1a09913233\" , \"IP\" : \"172.19.10.34\" } , { \"Name\" : \"b38a3e1ca567\" , \"IP\" : \"172.19.10.33\" } , { \"Name\" : \"943610ab56f5\" , \"IP\" : \"172.19.10.30\" } ] } ]","title":"Architecture"},{"location":"Legacy/weekplanner/Server/architecture/#architecture","text":"In the following, the server architecture for the giraf project will be explained.","title":"Architecture"},{"location":"Legacy/weekplanner/Server/architecture/#old-servers-discontinued","text":"Name IP Specs Master00 192.38.56.37 2 GB RAM 2xCPU Disk: 22 GB OS: CentOS Linux release 7.5.1804 (Core) Node01 172.19.0.244 2 GB RAM 1xCPU Disk: 22 GB OS: CentOS Linux release 7.5.1804 (Core) Node02 172.19.0.245 2 GB RAM 1xCPU Disk: 22 GB OS: CentOS Linux release 7.5.1804 (Core) Node03 192.38.56.36 2 GB RAM 1xCPU Disk: 22 GB OS: CentOS Linux release 7.5.1804 (Core) GitLab 192.38.56.136 4 GB RAM 2xCPU Disk: 46 GB OS: CentOS Linux release 7.5.1804 (Core) web01 192.38.56.38 2 GB RAM 1xCPU Disk: 22 GB OS: CentOS Linux release 7.4.1708 (Core) Backup01 172.19.0.235 4 GB RAM 2xCPU Disk: 10 GB OS: CentOS Linux release 7.2.1511 (Core) The only user on these servers are root, and each server has everything open to the internet and is hence under heavy attack from malicious users trying to brute-force the passwords.","title":"Old Servers (DISCONTINUED)"},{"location":"Legacy/weekplanner/Server/architecture/#new-servers","text":"Name Internal IP External IP Specs giraf-master00.srv.aau.dk 172.19.10.29 192.38.56.151 Ram: 4 GB 2xCPU Disk: 24 GB OS: Ubuntu Server 18.04.2 giraf-master01.srv.aau.dk 172.19.10.30 192.38.56.153 Ram: 4 GB 2xCPU Disk: 24 GB OS: Ubuntu Server 18.04.2 giraf-node00.srv.aau.dk 172.19.10.31 N/A Ram: 2 GB 1xCPU Disk: 24 GB OS: Ubuntu Server 18.04.2 giraf-node01.srv.aau.dk 172.19.10.32 N/A Ram: 2 GB 1xCPU Disk: 24 GB OS: Ubuntu Server 18.04.2 giraf-node02.srv.aau.dk 172.19.10.33 N/A Ram: 2 GB 1xCPU Disk: 24 GB OS: Ubuntu Server 18.04.2 giraf-node03.srv.aau.dk 172.19.10.34 N/A Ram: 2 GB 1xCPU Disk: 24 GB OS: Ubuntu Server 18.04.2 The two public IP's for the project only has port 80 and port 443 open. Each node has been configured to use the 10.14.0.0/16 subnet for the local docker daemon. For the swarm overlay network, the 10.10.0.0/16 subnet is used.","title":"New Servers"},{"location":"Legacy/weekplanner/Server/architecture/#web-api","text":"The WEB API runs in a docker container, which is routed by NGINX. The WEB API is only available on port 80 and 443 on the URLs shown below. Stage URL Production http://srv.giraf.cs.aau.dk/PROD/API Development http://srv.giraf.cs.aau.dk/DEV/API Test http://srv.giraf.cs.aau.dk/TEST/API","title":"Web-API"},{"location":"Legacy/weekplanner/Server/architecture/#network-drives","text":"ITS is responsible for the NFS that is mounted on all the nodes and masters in /swarm-nfs/ . As mentioned above, ITS will attach a network drive at /swarm-nfs/ , which should include the following: api/ appsettings.Develop.json appsettings.Production.json appsettings.Testing.json backup/ cdn/ dev/ pictograms/ test/ pictograms/ prod/ pictograms/ certbot/ mysql/ nginx/ certs/ sites-enabled/ nginx.conf Furthermore, the master00 server, should execute the following cronjob: 1 certbot renew --webroot -w /swarm-nfs/certbot/ -d srv.giraf.cs.aau.dk --post-hook \"cp -RL /etc/letsencrypt/live/srv.giraf.cs.aau.dk/. /swarm-nfs/nginx/certs/\" This is done to ensure a single point of certificate-authority on the first masterserver, that after renewing the certificate, moves it into the /swarm-nfs/nginx/certs. This is done to ensure that the certificate is available on all of the servers, and that the nginx Giraf_PROXY service mounts this folder and the certificate.","title":"Network Drives"},{"location":"Legacy/weekplanner/Server/architecture/#the-docker-setup","text":"More information on Docker can found here .","title":"The Docker Setup"},{"location":"Legacy/weekplanner/Server/architecture/#nodes","text":"ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION yny9ky6b6zczqrjzxd7sl71k6 * giraf-master00.srv.aau.dk Ready Active Leader 18.09.3 2n08r588w9p8xazc5cm8r6o9o giraf-master01.srv.aau.dk Ready Active Reachable 18.09.3 wrr68nqt116tk1rszwvdv1nmk giraf-node00.srv.aau.dk Ready Active 18.09.3 bhh5mitvwzdhzbky1cjne9ffg giraf-node01.srv.aau.dk Ready Active 18.09.3 as7n375y2gwcj5vf4h73h9ron giraf-node02.srv.aau.dk Ready Active 18.09.3 koclcs8nxt0y6qu4ho511la0m giraf-node03.srv.aau.dk Ready Active 18.09.3","title":"Nodes"},{"location":"Legacy/weekplanner/Server/architecture/#network","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 [ { \"Name\" : \"ingress\" , \"Id\" : \"esq3c70wonolez79eifsqeqhc\" , \"Created\" : \"2019-03-15T09:32:18.169968631+01:00\" , \"Scope\" : \"swarm\" , \"Driver\" : \"overlay\" , \"EnableIPv6\" : false, \"IPAM\" : { \"Driver\" : \"default\" , \"Options\" : null, \"Config\" : [ { \"Subnet\" : \"10.255.0.0/16\" , \"Gateway\" : \"10.255.0.1\" } ] } , \"Internal\" : false, \"Attachable\" : false, \"Ingress\" : true, \"ConfigFrom\" : { \"Network\" : \"\" } , \"ConfigOnly\" : false, \"Containers\" : { \"ingress-sbox\" : { \"Name\" : \"ingress-endpoint\" , \"EndpointID\" : \"f8f1417b758bc994a9c77aeda536b0a9e4294226baf29117382913e88bac8702\" , \"MacAddress\" : \"02:42:0a:ff:00:02\" , \"IPv4Address\" : \"10.255.0.2/16\" , \"IPv6Address\" : \"\" } } , \"Options\" : { \"com.docker.network.driver.overlay.vxlanid_list\" : \"4096\" } , \"Labels\" : {} , \"Peers\" : [ { \"Name\" : \"b6a2fb9fd60e\" , \"IP\" : \"172.19.10.29\" } , { \"Name\" : \"e17ba5bebf00\" , \"IP\" : \"172.19.10.31\" } , { \"Name\" : \"8a720eeaec16\" , \"IP\" : \"172.19.10.32\" } , { \"Name\" : \"9a1a09913233\" , \"IP\" : \"172.19.10.34\" } , { \"Name\" : \"b38a3e1ca567\" , \"IP\" : \"172.19.10.33\" } , { \"Name\" : \"943610ab56f5\" , \"IP\" : \"172.19.10.30\" } ] } ]","title":"Network"},{"location":"Legacy/weekplanner/Server/Docker/","text":"Overview \u00b6 This section gives an overview of how Docker is used in the GIRAF project. General information General information about the Docker setup. Docker Swarm Information about how the Docker swarm is configured.","title":"Overview"},{"location":"Legacy/weekplanner/Server/Docker/#overview","text":"This section gives an overview of how Docker is used in the GIRAF project. General information General information about the Docker setup. Docker Swarm Information about how the Docker swarm is configured.","title":"Overview"},{"location":"Legacy/weekplanner/Server/Docker/general_information/","text":"General Information \u00b6 Docker is a tool for managing software containers. A container is used to isolate software from the system it runs on. It encapsulates some software and all necessary dependencies for running it, which guarantees that no matter which environment is used, the application running inside the container will still function the same. The software can be run on any platform that supports the container solution. Bear in mind we don't want to use Docker directly, we use it as a part of Kubernetes, but because of time restrictions we deployed our REST API, MySQL database, Jenkins and Artifactory with Dockerfiles. In addition, we use Dockerfiles as the recipes for running the build notes in the continuous integration pipeline. Each Dockerfile contains all necessary commands that is required to build, test and export the projects, and the resulting image from running the Dockerfile is then discarded. Docker containers are generated from Docker images, which act like a template for the creation of a container. The images can be created from files called Dockerfiles, acting like a recipe for creating the templates. A Dockerfile specifies what software and dependencies a Docker image should contain. Dockerfiles \u00b6 Docker uses its own scripting language for creating Dockerfiles, running commands from top to bottom. Below we show our example of a Dockerfile for deploying Jenkins. (Please note, that as of 2018, the Jenkins CI server has been replaced by the GitLab CI server, and as such the following example is only used in this article.) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 FROM jenkins:latest RUN /usr/local/bin/install-plugins.sh google-oauth-plugin android-emulator android-lint oauth-credentials git-client artifactory cas-plugin checkstyle git google-play-android-publisher postbuild-task gradle javadoc junit repository USER root RUN apt-get update RUN apt-get install -y curl libunwind8 gettext RUN curl -sSL -o dotnet.tar.gz <https://go.microsoft.com/fwlink/?linkid = 847105 > RUN mkdir -p /opt/dotnet RUN tar zxf dotnet.tar.gz -C /opt/dotnet RUN ln -s /opt/dotnet/dotnet /usr/bin/dotnet RUN rm dotnet.tar.gz ENV ANDROID \\_ SDK \\_ URL <https://dl.google.com/android/repository/tools_r25.2.3-linux.zip> RUN cd /opt && curl -o tools.zip $ANDROID \\_ SDK \\_ URL RUN cd /opt && unzip tools.zip COPY intel-android-extra-license /opt/tools/licenses/intel-android-extra-license COPY android-sdk-license /opt/tools/licenses/android-sdk-license COPY jobs /var/jenkins \\_ home/jobs ENV ANDROID \\_ HOME /opt/tools ENV PATH $PATH : $ANDROID \\_ HOME/ RUN echo y | android update sdk --no-ui COPY config.xml /var/jenkins \\_ home/config.xml The Dockerfile commands in our Dockerfile for deploying Jenkins are as follows: FROM indicates which existing Dockerfile we want to use and extend. Our Dockerfile extends the official Dockerfile for Jenkins through this command. ENV sets an enviroment variable. For instance in this case //ANDROID_SDK_URL// is set to the location for the Android SDK. Setting these enviroment variables improves the readability, like using variables in other software. USER sets the user being used to run the commands following it. That user is used until it's changed by another //USER// command. RUN executes shell commands inside the container. For instance in this case it's being used to download the Android SDK, and later execute the Android SDK. COPY can be used to copy external files into the Docker image being created. In this case it's being used to copy a configuration file for Jenkins into the image so we are guaranteed to always have the same configuration when deploying. You may notice that the first command refers to another Dockerfile, which our Dockerfile then extends upon (in this case the official Dockerfile for Jenkins). The Jenkins Dockerfile contains some commands we explain below, along with some other, relevant Dockerfile commands, which we use in other files than the ones shown. Commands are as follows: VOLUME is used to make persistent storage for the Docker containers. It takes one parameter which is the absolute path inside a container that should be persisted even if the container is deleted. This is necessary if data needs to be persisted as Docker doesn't do it by default. EXPOSE is used to expose ports internally in a container, as the application running in the container can only communicate through an exposed port to the outside. This command takes a list of ports that should be exposed. How this can be used is explained further below. ENTRYPOINT is used to specify what command should be used when deploying the Docker container from an image. It takes an array of strings where each string is a part of the complete shell command being executed when deployed. The entrypoint commands can never be overwritten when deploying, so it is used for executing the minimum required commands to deploy. CMD is also used to specify what command or parameters that should be used when deploying. But contrary to the entrypoint, this command can be overwritten. So this command should be used for default parameters that should be possible to overwrite. Deploying Images \u00b6 When a Dockerfile has been made, you can build an image from it by running docker build ./ . This assumes the Dockerfile is called Dockerfile . For instance our Jenkins Dockerfile was built by using docker build -t jenkins\\_giraf ./ . The -t parameter is used for tagging an image, so in this case we are giving the image the tag jenkins\\_giraf . When an image has been created, it's possible to deploy an instance of it with the docker run command. The docker run command needs atleast one parameter which is the tag of the image that should be run. For example if we wanted to deploy our Jenkins image, we would execute docker run jenkins\\_giraf . This only creates a Jenkins container, it cannot communicate with the outside system yet. It's possible to pass a -p parameter that takes a port route, which means the command could look like docker run -p 80:8080 jenkins\\_giraf . Here the port the container listens on, the exposed port, is port 8080. By using the parameter -p 80:8080 we tell Docker that the host should be redirected from port 80 to the exposed port 1. The picture below illustrates how this works: we illustrate how the -v parameter works. This parameter is used to mount a host folder into a folder in the container. For instance this could be used when we want to populate a folder in our container with pre-existing data. The parameter is used as -p /hostfolder:/persistent/folder where hostfolder is the folder on the host system and /persistent/folder is an arbitrary path to an existing folder inside a container. The picture below illustrates how this works:","title":"General Information"},{"location":"Legacy/weekplanner/Server/Docker/general_information/#general-information","text":"Docker is a tool for managing software containers. A container is used to isolate software from the system it runs on. It encapsulates some software and all necessary dependencies for running it, which guarantees that no matter which environment is used, the application running inside the container will still function the same. The software can be run on any platform that supports the container solution. Bear in mind we don't want to use Docker directly, we use it as a part of Kubernetes, but because of time restrictions we deployed our REST API, MySQL database, Jenkins and Artifactory with Dockerfiles. In addition, we use Dockerfiles as the recipes for running the build notes in the continuous integration pipeline. Each Dockerfile contains all necessary commands that is required to build, test and export the projects, and the resulting image from running the Dockerfile is then discarded. Docker containers are generated from Docker images, which act like a template for the creation of a container. The images can be created from files called Dockerfiles, acting like a recipe for creating the templates. A Dockerfile specifies what software and dependencies a Docker image should contain.","title":"General Information"},{"location":"Legacy/weekplanner/Server/Docker/general_information/#dockerfiles","text":"Docker uses its own scripting language for creating Dockerfiles, running commands from top to bottom. Below we show our example of a Dockerfile for deploying Jenkins. (Please note, that as of 2018, the Jenkins CI server has been replaced by the GitLab CI server, and as such the following example is only used in this article.) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 FROM jenkins:latest RUN /usr/local/bin/install-plugins.sh google-oauth-plugin android-emulator android-lint oauth-credentials git-client artifactory cas-plugin checkstyle git google-play-android-publisher postbuild-task gradle javadoc junit repository USER root RUN apt-get update RUN apt-get install -y curl libunwind8 gettext RUN curl -sSL -o dotnet.tar.gz <https://go.microsoft.com/fwlink/?linkid = 847105 > RUN mkdir -p /opt/dotnet RUN tar zxf dotnet.tar.gz -C /opt/dotnet RUN ln -s /opt/dotnet/dotnet /usr/bin/dotnet RUN rm dotnet.tar.gz ENV ANDROID \\_ SDK \\_ URL <https://dl.google.com/android/repository/tools_r25.2.3-linux.zip> RUN cd /opt && curl -o tools.zip $ANDROID \\_ SDK \\_ URL RUN cd /opt && unzip tools.zip COPY intel-android-extra-license /opt/tools/licenses/intel-android-extra-license COPY android-sdk-license /opt/tools/licenses/android-sdk-license COPY jobs /var/jenkins \\_ home/jobs ENV ANDROID \\_ HOME /opt/tools ENV PATH $PATH : $ANDROID \\_ HOME/ RUN echo y | android update sdk --no-ui COPY config.xml /var/jenkins \\_ home/config.xml The Dockerfile commands in our Dockerfile for deploying Jenkins are as follows: FROM indicates which existing Dockerfile we want to use and extend. Our Dockerfile extends the official Dockerfile for Jenkins through this command. ENV sets an enviroment variable. For instance in this case //ANDROID_SDK_URL// is set to the location for the Android SDK. Setting these enviroment variables improves the readability, like using variables in other software. USER sets the user being used to run the commands following it. That user is used until it's changed by another //USER// command. RUN executes shell commands inside the container. For instance in this case it's being used to download the Android SDK, and later execute the Android SDK. COPY can be used to copy external files into the Docker image being created. In this case it's being used to copy a configuration file for Jenkins into the image so we are guaranteed to always have the same configuration when deploying. You may notice that the first command refers to another Dockerfile, which our Dockerfile then extends upon (in this case the official Dockerfile for Jenkins). The Jenkins Dockerfile contains some commands we explain below, along with some other, relevant Dockerfile commands, which we use in other files than the ones shown. Commands are as follows: VOLUME is used to make persistent storage for the Docker containers. It takes one parameter which is the absolute path inside a container that should be persisted even if the container is deleted. This is necessary if data needs to be persisted as Docker doesn't do it by default. EXPOSE is used to expose ports internally in a container, as the application running in the container can only communicate through an exposed port to the outside. This command takes a list of ports that should be exposed. How this can be used is explained further below. ENTRYPOINT is used to specify what command should be used when deploying the Docker container from an image. It takes an array of strings where each string is a part of the complete shell command being executed when deployed. The entrypoint commands can never be overwritten when deploying, so it is used for executing the minimum required commands to deploy. CMD is also used to specify what command or parameters that should be used when deploying. But contrary to the entrypoint, this command can be overwritten. So this command should be used for default parameters that should be possible to overwrite.","title":"Dockerfiles"},{"location":"Legacy/weekplanner/Server/Docker/general_information/#deploying-images","text":"When a Dockerfile has been made, you can build an image from it by running docker build ./ . This assumes the Dockerfile is called Dockerfile . For instance our Jenkins Dockerfile was built by using docker build -t jenkins\\_giraf ./ . The -t parameter is used for tagging an image, so in this case we are giving the image the tag jenkins\\_giraf . When an image has been created, it's possible to deploy an instance of it with the docker run command. The docker run command needs atleast one parameter which is the tag of the image that should be run. For example if we wanted to deploy our Jenkins image, we would execute docker run jenkins\\_giraf . This only creates a Jenkins container, it cannot communicate with the outside system yet. It's possible to pass a -p parameter that takes a port route, which means the command could look like docker run -p 80:8080 jenkins\\_giraf . Here the port the container listens on, the exposed port, is port 8080. By using the parameter -p 80:8080 we tell Docker that the host should be redirected from port 80 to the exposed port 1. The picture below illustrates how this works: we illustrate how the -v parameter works. This parameter is used to mount a host folder into a folder in the container. For instance this could be used when we want to populate a folder in our container with pre-existing data. The parameter is used as -p /hostfolder:/persistent/folder where hostfolder is the folder on the host system and /persistent/folder is an arbitrary path to an existing folder inside a container. The picture below illustrates how this works:","title":"Deploying Images"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/","text":"Overview \u00b6 This section gives an overview of the Docker swarm used in the GIRAF project. Practical information Configuration Network Proxy Web API Database","title":"Overview"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/#overview","text":"This section gives an overview of the Docker swarm used in the GIRAF project. Practical information Configuration Network Proxy Web API Database","title":"Overview"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/configuration/","text":"Configuration \u00b6 The Docker Swarm is manged via the docker-compose.yml file that can be found in GitHub . The docker-compose.yml file consists of the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 version : '3.7' services : PROXY : # NGINX proxy service for the swarm image : nginx:1.15 # This will use the latest version of 1.15.x ports : # Exposes for 80 and 443 from the service to the swarm network(public) - '80:80' - '443:443' volumes : # Mounts the nginx config folder inside the container as read only - /swarm-nfs/nginx/:/etc/nginx/:r - /swarm-nfs/certbot/:/var/www/html/ networks : # Attaches the network - frontend deploy : # Deploy options replicas : 3 # Number er services healthcheck : # Docker healthcheck restarts of the nginx service is not running three times in a row, service might be out for a maximum of 30 secunds test : [ \"CMD-SHELL\" , \"service nginx status || exit 1\" ] interval : 10s timeout : 10s retries : 3 API_PROD : # API service used for production image : giraf/web-api:1 # This will use version 1 of the API hosted on hub.docker.com networks : # Attaches the network - frontend - backend environment : # Sets local envionment for dotnet ASPNETCORE_ENVIRONMENT : Production deploy : # Deploy options replicas : 3 # Number of services volumes : # Mounts the two NFS file shares into the container - /swarm-nfs/cdn/pictograms/:/pictograms - /swarm-nfs/api/appsettings.Develop.json:/srv/appsettings.json healthcheck : # Docker healthcheck restart of it fails three times in a row, service may be out for a maximum of 30 seconds test : [ \"CMD-SHELL\" , \"curl --fail http://localhost:5000/v1/Status || exit 1\" ] interval : 10s timeout : 10s retries : 3 API_DEV : # API service used for development image : giraf/web-api:1 # This will use version 1 of the API hosted on hub.docker.com networks : # Attaches the network - frontend - backend environment : # Sets local envionment for dotnet ASPNETCORE_ENVIRONMENT : Production deploy : # Deploy options replicas : 3 # Number of services volumes : # Mounts the two NFS file shares into the container - /swarm-nfs/cdn/pictograms/:/pictograms - /swarm-nfs/api/appsettings.Develop.json:/srv/appsettings.json healthcheck : # Docker healthcheck restart of it fails three times in a row, service may be out for a maximum of 30 seconds test : [ \"CMD-SHELL\" , \"curl --fail http://localhost:5000/v1/Status || exit 1\" ] interval : 10s timeout : 10s retries : 3 API_TEST : # API service for testing new versions of the API before deploying to the other services. image : giraf/web-api:1 # This will use version 1 of the API hosted on hub.docker.com networks : # Attaches the network - frontend - backend environment : # Sets local envionment for dotnet ASPNETCORE_ENVIRONMENT : Production deploy : # Deploy options replicas : 1 # Number of services volumes : # Mounts the two NFS file shares into the container - /swarm-nfs/cdn/pictograms/:/pictograms - /swarm-nfs/api/appsettings.Develop.json:/srv/appsettings.json healthcheck : # Docker healthcheck restart of it fails three times in a row, service may be out for a maximum of 30 seconds test : [ \"CMD-SHELL\" , \"curl --fail http://localhost:5000/v1/Status || exit 1\" ] interval : 10s timeout : 10s retries : 3 DB : # The DB service will be changed to use the production database later once it has been migrated. The httpd image is only for testing. image : mysql:8.0.19 command : --default-authentication-plugin=mysql_native_password volumes : # Mounts the mysql files from the NFS to the container - /swarm-nfs/mysql/:/var/lib/mysql/ environment : # MySQL root password MYSQL_ROOT_PASSWORD : <password> networks : # Attaches the network - backend deploy : # Deploy options restart_policy : # On failure restart the service condition : on-failure networks : # Defines networks in the swarm frontend : # Creates a new network for frontend traffic backend : # Creates a new network for backend traffic Updating the Docker Swarm \u00b6 In order to update the running Docker Stack ssh to one of the master servers and execute do the following steps: ssh to a master server, see her for more information. Clone the GitHub repository onto the server or pull any new changes to the existing. Verify the changes in the docker-compose.yml file are as expected. Run docker stack deploy -c docker-compose.yml Giraf Use the docker service ls command to verify that all services has started as expected. Run the following command to logout of the server, exit .","title":"Configuration"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/configuration/#configuration","text":"The Docker Swarm is manged via the docker-compose.yml file that can be found in GitHub . The docker-compose.yml file consists of the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 version : '3.7' services : PROXY : # NGINX proxy service for the swarm image : nginx:1.15 # This will use the latest version of 1.15.x ports : # Exposes for 80 and 443 from the service to the swarm network(public) - '80:80' - '443:443' volumes : # Mounts the nginx config folder inside the container as read only - /swarm-nfs/nginx/:/etc/nginx/:r - /swarm-nfs/certbot/:/var/www/html/ networks : # Attaches the network - frontend deploy : # Deploy options replicas : 3 # Number er services healthcheck : # Docker healthcheck restarts of the nginx service is not running three times in a row, service might be out for a maximum of 30 secunds test : [ \"CMD-SHELL\" , \"service nginx status || exit 1\" ] interval : 10s timeout : 10s retries : 3 API_PROD : # API service used for production image : giraf/web-api:1 # This will use version 1 of the API hosted on hub.docker.com networks : # Attaches the network - frontend - backend environment : # Sets local envionment for dotnet ASPNETCORE_ENVIRONMENT : Production deploy : # Deploy options replicas : 3 # Number of services volumes : # Mounts the two NFS file shares into the container - /swarm-nfs/cdn/pictograms/:/pictograms - /swarm-nfs/api/appsettings.Develop.json:/srv/appsettings.json healthcheck : # Docker healthcheck restart of it fails three times in a row, service may be out for a maximum of 30 seconds test : [ \"CMD-SHELL\" , \"curl --fail http://localhost:5000/v1/Status || exit 1\" ] interval : 10s timeout : 10s retries : 3 API_DEV : # API service used for development image : giraf/web-api:1 # This will use version 1 of the API hosted on hub.docker.com networks : # Attaches the network - frontend - backend environment : # Sets local envionment for dotnet ASPNETCORE_ENVIRONMENT : Production deploy : # Deploy options replicas : 3 # Number of services volumes : # Mounts the two NFS file shares into the container - /swarm-nfs/cdn/pictograms/:/pictograms - /swarm-nfs/api/appsettings.Develop.json:/srv/appsettings.json healthcheck : # Docker healthcheck restart of it fails three times in a row, service may be out for a maximum of 30 seconds test : [ \"CMD-SHELL\" , \"curl --fail http://localhost:5000/v1/Status || exit 1\" ] interval : 10s timeout : 10s retries : 3 API_TEST : # API service for testing new versions of the API before deploying to the other services. image : giraf/web-api:1 # This will use version 1 of the API hosted on hub.docker.com networks : # Attaches the network - frontend - backend environment : # Sets local envionment for dotnet ASPNETCORE_ENVIRONMENT : Production deploy : # Deploy options replicas : 1 # Number of services volumes : # Mounts the two NFS file shares into the container - /swarm-nfs/cdn/pictograms/:/pictograms - /swarm-nfs/api/appsettings.Develop.json:/srv/appsettings.json healthcheck : # Docker healthcheck restart of it fails three times in a row, service may be out for a maximum of 30 seconds test : [ \"CMD-SHELL\" , \"curl --fail http://localhost:5000/v1/Status || exit 1\" ] interval : 10s timeout : 10s retries : 3 DB : # The DB service will be changed to use the production database later once it has been migrated. The httpd image is only for testing. image : mysql:8.0.19 command : --default-authentication-plugin=mysql_native_password volumes : # Mounts the mysql files from the NFS to the container - /swarm-nfs/mysql/:/var/lib/mysql/ environment : # MySQL root password MYSQL_ROOT_PASSWORD : <password> networks : # Attaches the network - backend deploy : # Deploy options restart_policy : # On failure restart the service condition : on-failure networks : # Defines networks in the swarm frontend : # Creates a new network for frontend traffic backend : # Creates a new network for backend traffic","title":"Configuration"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/configuration/#updating-the-docker-swarm","text":"In order to update the running Docker Stack ssh to one of the master servers and execute do the following steps: ssh to a master server, see her for more information. Clone the GitHub repository onto the server or pull any new changes to the existing. Verify the changes in the docker-compose.yml file are as expected. Run docker stack deploy -c docker-compose.yml Giraf Use the docker service ls command to verify that all services has started as expected. Run the following command to logout of the server, exit .","title":"Updating the Docker Swarm"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/database/","text":"Database \u00b6 In order for the new Docker Swarm to serve the Giraf project, the database from the old servers had to be migrated on to the new ones. In the following the old setup will be elaborated and the new setup will be explained. Old Setup \u00b6 The old server was configured to use MySQL 5.7.11 as can be seen on the picture below. In order to migrate the database without changing the database schema the same version of MySql. Using standard Mysql commands the database was dumped into a sql file using the following command: 1 mysqldump -u root -p giraf-dev > giraf-dev-dump.sql This command will dump the database giraf-dev to a local file giraf-dev-dump which can be exported to the new server and imported as to a new database. New Setup \u00b6 The Docker Compose files was updated with the following: 1 2 3 4 5 6 7 8 9 10 11 12 DB : # Database service image : mysql:8.0.19 # Database version command : --default-authentication-plugin=mysql_native_password volumes : # Mount the mysql folder from NFS to container - /swarm-nfs/mysql/:/var/lib/mysql/ environment : # Set local root password MYSQL_ROOT_PASSWORD : <Some password> networks : # Attached network - backend deploy : # Settings for deployment restart_policy : condition : on-failure In order to make the MySQL service work inside a Docker Swarm the MySQL database files must be synchronized across all servers inside the Swarm. In order to facilitate this ITS at AAU created an NFS file share that could be mounted in all the servers. The NFS is mounted at /swarm-nfs/ on all the servers hence the mountpoint is the same across all servers. Once the service is created an empty database will be served by default. The sql dump from the old setup was imported into the database using the following command: 1 mysql -u root -p giraf-dev < giraf-dev-dump.sql By executing this command the data stored in the sql dump will be migrated into the database. This new setup will enable the database to be served from all the different servers inside the Swarm meaning that any of the servers can be terminated without effecting the overall status of the Giraf project.","title":"Database"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/database/#database","text":"In order for the new Docker Swarm to serve the Giraf project, the database from the old servers had to be migrated on to the new ones. In the following the old setup will be elaborated and the new setup will be explained.","title":"Database"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/database/#old-setup","text":"The old server was configured to use MySQL 5.7.11 as can be seen on the picture below. In order to migrate the database without changing the database schema the same version of MySql. Using standard Mysql commands the database was dumped into a sql file using the following command: 1 mysqldump -u root -p giraf-dev > giraf-dev-dump.sql This command will dump the database giraf-dev to a local file giraf-dev-dump which can be exported to the new server and imported as to a new database.","title":"Old Setup"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/database/#new-setup","text":"The Docker Compose files was updated with the following: 1 2 3 4 5 6 7 8 9 10 11 12 DB : # Database service image : mysql:8.0.19 # Database version command : --default-authentication-plugin=mysql_native_password volumes : # Mount the mysql folder from NFS to container - /swarm-nfs/mysql/:/var/lib/mysql/ environment : # Set local root password MYSQL_ROOT_PASSWORD : <Some password> networks : # Attached network - backend deploy : # Settings for deployment restart_policy : condition : on-failure In order to make the MySQL service work inside a Docker Swarm the MySQL database files must be synchronized across all servers inside the Swarm. In order to facilitate this ITS at AAU created an NFS file share that could be mounted in all the servers. The NFS is mounted at /swarm-nfs/ on all the servers hence the mountpoint is the same across all servers. Once the service is created an empty database will be served by default. The sql dump from the old setup was imported into the database using the following command: 1 mysql -u root -p giraf-dev < giraf-dev-dump.sql By executing this command the data stored in the sql dump will be migrated into the database. This new setup will enable the database to be served from all the different servers inside the Swarm meaning that any of the servers can be terminated without effecting the overall status of the Giraf project.","title":"New Setup"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/network/","text":"Networking Docker Swarm \u00b6 Networking in a Docker Swarm is fairly easy. In the docker-compose.yml file simply add the name of the networks and specify which services are connected to which network. A service can be connected to 0 or more networks and a network not connected to anything does not make any sense. The Docker Swarm has two networks to separate different services in case one of them is compromised by an intruder. The frontend network is used to exchange information between the NGINX Proxy and the API. The backend network is used to exchange information between the API and database. Since the database has the most sensitive information that we want to protect, it makes sense to isolate it from the open internet.","title":"Networking Docker Swarm"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/network/#networking-docker-swarm","text":"Networking in a Docker Swarm is fairly easy. In the docker-compose.yml file simply add the name of the networks and specify which services are connected to which network. A service can be connected to 0 or more networks and a network not connected to anything does not make any sense. The Docker Swarm has two networks to separate different services in case one of them is compromised by an intruder. The frontend network is used to exchange information between the NGINX Proxy and the API. The backend network is used to exchange information between the API and database. Since the database has the most sensitive information that we want to protect, it makes sense to isolate it from the open internet.","title":"Networking Docker Swarm"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/practical_information/","text":"Practical Information \u00b6 In the following, the installation and setup process of DockerCE and Docker Swarm is described, and at the end, some examples on how to use Docker Services is given. Installation of Docker \u00b6 To install DockerCE on a node in the Swarm the following command can be used to set up the local Docker environment: 1 curl -fsSL https://get.docker.com -o get-docker.sh | sh get-docker.sh Once the Docker engine has been installed, it is recommended that the user is added to the Docker group so that every Docker command is not run as root. The following command ensures that your user is added to the Docker group: 1 sudo usermod -aG docker <USERNAME>@student.aau.dk If the Docker installation is going to be used in a Docker Swarm at AAU, the following changes are needed since the AAU network is running on the default Docker subnet. For local installations use the following. 1 nano /etc/docker/daemon.json and insert the following: 1 2 3 4 { \"bip\" : \"10.14.0.1/16\" , \"ipv6\" : false } For the Swarm overlay network first create the network manualy with the following command: 1 docker network create --subnet 10 .10.0.0/16 -o com.docker.network.bridge.enable_icc = false -o com.docker.network.bridge.name = docker_gwbridge Initializing the Swarm \u00b6 REMEMBER TO CREATE THE NETWORK FIRST On the first master server run the following command: 1 docker swarm init Adding a Manager to the Swarm \u00b6 REMEMBER TO CREATE THE NETWORK FIRST Once the Docker Swarm is initialized used the following command on the first master server to get the join token for the swarm: 1 docker swarm join-token --manager The output is copied to the new master server. Adding a Worker to the Swarm \u00b6 REMEMBER TO CREATE THE NETWORK FIRST To add a worker to the Swarm first run the following command on one of the managers: 1 docker swarm join-token worker The output should be run on the servers intended to be workers in the Swarm. Now that the Docker Swarm has been created and is ready to serve. Use the following command to verify the Swarm setup: 1 docker swarm ls The output should look like this: ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION yny9ky6b6zczqrjzxd7sl71k6 * giraf-master00.srv.aau.dk Ready Active Leader 18.09.3 2n08r588w9p8xazc5cm8r6o9o giraf-master01.srv.aau.dk Ready Active Reachable 18.09.3 wrr68nqt116tk1rszwvdv1nmk giraf-node00.srv.aau.dk Ready Active 18.09.3 bhh5mitvwzdhzbky1cjne9ffg giraf-node01.srv.aau.dk Ready Active 18.09.3 as7n375y2gwcj5vf4h73h9ron giraf-node02.srv.aau.dk Ready Active 18.09.3 koclcs8nxt0y6qu4ho511la0m giraf-node03.srv.aau.dk Ready Active 18.09.3 Examples of Using a Service \u00b6 Once the Docker Swarm has been set up, it can be used to serve the different parts of the project. In the following, we will give some examples of how to work with the Swarm. Example nginx \u00b6 To create a new service, use the following: 1 docker service create --name nginx-giraf-proxy --replicas 2 -p 80 :80 -p 443 :443 nginx:1.15 By running this command, a service with two containers will be stated and the containers will have port 80 and 443 exposed to the internet and can be accessed on the Swarms public IP's. The two containers will be running the nginx version 1.15. To upgrade the version that a service uses, run the following to downgrade or upgrade the service: 1 docker service update --image nginx:1.15 nginx-giraf-proxy To increase the number of containers a service creates, use the following command: 1 docker service scale nginx-giraf-proxy = 5 To verify the service use the following command: 1 docker service ps nginx-giraf-proxy The outputs should look like this: ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS iksklc50ttxt nginx-giraf-proxy.1 nginx:latest giraf-master01.srv.aau.dk Running Running 29 minutes ago Note that the ports exposed are not listed in this view since it is served through the service and can be seen then running the following command: 1 docker service ls ID NAME MODE REPLICAS IMAGE PORTS k6nuoecmam6m proxy replicated 5/5 nginx:latest :80->80/tcp, :443->443/tcp","title":"Practical Information"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/practical_information/#practical-information","text":"In the following, the installation and setup process of DockerCE and Docker Swarm is described, and at the end, some examples on how to use Docker Services is given.","title":"Practical Information"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/practical_information/#installation-of-docker","text":"To install DockerCE on a node in the Swarm the following command can be used to set up the local Docker environment: 1 curl -fsSL https://get.docker.com -o get-docker.sh | sh get-docker.sh Once the Docker engine has been installed, it is recommended that the user is added to the Docker group so that every Docker command is not run as root. The following command ensures that your user is added to the Docker group: 1 sudo usermod -aG docker <USERNAME>@student.aau.dk If the Docker installation is going to be used in a Docker Swarm at AAU, the following changes are needed since the AAU network is running on the default Docker subnet. For local installations use the following. 1 nano /etc/docker/daemon.json and insert the following: 1 2 3 4 { \"bip\" : \"10.14.0.1/16\" , \"ipv6\" : false } For the Swarm overlay network first create the network manualy with the following command: 1 docker network create --subnet 10 .10.0.0/16 -o com.docker.network.bridge.enable_icc = false -o com.docker.network.bridge.name = docker_gwbridge","title":"Installation of Docker"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/practical_information/#initializing-the-swarm","text":"REMEMBER TO CREATE THE NETWORK FIRST On the first master server run the following command: 1 docker swarm init","title":"Initializing the Swarm"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/practical_information/#adding-a-manager-to-the-swarm","text":"REMEMBER TO CREATE THE NETWORK FIRST Once the Docker Swarm is initialized used the following command on the first master server to get the join token for the swarm: 1 docker swarm join-token --manager The output is copied to the new master server.","title":"Adding a Manager to the Swarm"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/practical_information/#adding-a-worker-to-the-swarm","text":"REMEMBER TO CREATE THE NETWORK FIRST To add a worker to the Swarm first run the following command on one of the managers: 1 docker swarm join-token worker The output should be run on the servers intended to be workers in the Swarm. Now that the Docker Swarm has been created and is ready to serve. Use the following command to verify the Swarm setup: 1 docker swarm ls The output should look like this: ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION yny9ky6b6zczqrjzxd7sl71k6 * giraf-master00.srv.aau.dk Ready Active Leader 18.09.3 2n08r588w9p8xazc5cm8r6o9o giraf-master01.srv.aau.dk Ready Active Reachable 18.09.3 wrr68nqt116tk1rszwvdv1nmk giraf-node00.srv.aau.dk Ready Active 18.09.3 bhh5mitvwzdhzbky1cjne9ffg giraf-node01.srv.aau.dk Ready Active 18.09.3 as7n375y2gwcj5vf4h73h9ron giraf-node02.srv.aau.dk Ready Active 18.09.3 koclcs8nxt0y6qu4ho511la0m giraf-node03.srv.aau.dk Ready Active 18.09.3","title":"Adding a Worker to the Swarm"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/practical_information/#examples-of-using-a-service","text":"Once the Docker Swarm has been set up, it can be used to serve the different parts of the project. In the following, we will give some examples of how to work with the Swarm.","title":"Examples of Using a Service"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/practical_information/#example-nginx","text":"To create a new service, use the following: 1 docker service create --name nginx-giraf-proxy --replicas 2 -p 80 :80 -p 443 :443 nginx:1.15 By running this command, a service with two containers will be stated and the containers will have port 80 and 443 exposed to the internet and can be accessed on the Swarms public IP's. The two containers will be running the nginx version 1.15. To upgrade the version that a service uses, run the following to downgrade or upgrade the service: 1 docker service update --image nginx:1.15 nginx-giraf-proxy To increase the number of containers a service creates, use the following command: 1 docker service scale nginx-giraf-proxy = 5 To verify the service use the following command: 1 docker service ps nginx-giraf-proxy The outputs should look like this: ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS iksklc50ttxt nginx-giraf-proxy.1 nginx:latest giraf-master01.srv.aau.dk Running Running 29 minutes ago Note that the ports exposed are not listed in this view since it is served through the service and can be seen then running the following command: 1 docker service ls ID NAME MODE REPLICAS IMAGE PORTS k6nuoecmam6m proxy replicated 5/5 nginx:latest :80->80/tcp, :443->443/tcp","title":"Example nginx"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/proxy/","text":"Proxy \u00b6 For the Giraf project we only have two public IPs that both have a DNS name to srv.giraf.cs.aau.dk. The firewall settings for those IPs are that only port 80 and port 443 are allowed through and since the different parts of the system uses other ports, such as port 5000 and 3306 , we need a reverse proxy to pass the traffic intended for those ports into the Docker Swarm. In the following, the configuration of the proxy will be elaborated upon. docker-compose.yml \u00b6 Using the Docker Stack command for deploying to a production environment, a .yml file has to be passed to the command. The file contains at least the following: 1 2 version : '3' services : and can be elaborated with many more options. The file for the proxy is as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 version : '3' services : PROXY : image : nginx:1.15 # this will use the latest version of 1.15.x ports : - '80:80' # expose 80 on the host and sent it to 80 in the container - '443:443' # expose 443 on the host and sent it to 443 in the container volumes : # mounts the nginx config folder inside the container - ./nginx/:/etc/nginx/ networks : # uses the frontend network to pass traffic into the containers - frontend networks : frontend : # creates a new network for frontend traffic backend : # creates a new network for backend traffic The code specifies a service called proxy that uses the nginx version 1.15.x and that exposes the port 80 and 443 to the network. It has a volume attached where the nginx config folder is mapped into the container. The networks are elaborated in the section about network . nginx.conf \u00b6 The NGINX can be used for many different purposes and one of them is as a proxy. The following config is a standard config for a reverse proxy that will enable all sites in the /etc/nginx/sites-enabled/ folder. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 worker_processes 1 ; events { worker_connections 1024 ; } http { upstream srv.giraf.cs.aau.dk { server proxy ; } default_type application/octet-stream ; sendfile on ; keepalive_timeout 65 ; include /etc/nginx/sites-enabled/* ; gzip on ; client_max_body_size 20m ; } API \u00b6 The API is file /etc/nginx/sites-enabled/API specifies where the traffic for the API is supposed to go once the NGINX server receives it. We can use the proxy_pass http://API/ inside Docker because it uses static DNS names inside its network, all traffic for the API will be directed into the API service. This means that the API can be accessed through http://srv.giraf.cs.aau.dk/API/ on both port 80 and port 443 . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 server { listen 80 ; listen 443 ; server_name srv.giraf.cs.aau.dk ; location /API/ { proxy_buffer_size 128k ; proxy_buffers 4 256k ; proxy_busy_buffers_size 256k ; proxy_pass http://API/ ; proxy_set_header X-Real-IP $remote_addr ; proxy_set_header Host $host ; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for ; } }","title":"Proxy"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/proxy/#proxy","text":"For the Giraf project we only have two public IPs that both have a DNS name to srv.giraf.cs.aau.dk. The firewall settings for those IPs are that only port 80 and port 443 are allowed through and since the different parts of the system uses other ports, such as port 5000 and 3306 , we need a reverse proxy to pass the traffic intended for those ports into the Docker Swarm. In the following, the configuration of the proxy will be elaborated upon.","title":"Proxy"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/proxy/#docker-composeyml","text":"Using the Docker Stack command for deploying to a production environment, a .yml file has to be passed to the command. The file contains at least the following: 1 2 version : '3' services : and can be elaborated with many more options. The file for the proxy is as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 version : '3' services : PROXY : image : nginx:1.15 # this will use the latest version of 1.15.x ports : - '80:80' # expose 80 on the host and sent it to 80 in the container - '443:443' # expose 443 on the host and sent it to 443 in the container volumes : # mounts the nginx config folder inside the container - ./nginx/:/etc/nginx/ networks : # uses the frontend network to pass traffic into the containers - frontend networks : frontend : # creates a new network for frontend traffic backend : # creates a new network for backend traffic The code specifies a service called proxy that uses the nginx version 1.15.x and that exposes the port 80 and 443 to the network. It has a volume attached where the nginx config folder is mapped into the container. The networks are elaborated in the section about network .","title":"docker-compose.yml"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/proxy/#nginxconf","text":"The NGINX can be used for many different purposes and one of them is as a proxy. The following config is a standard config for a reverse proxy that will enable all sites in the /etc/nginx/sites-enabled/ folder. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 worker_processes 1 ; events { worker_connections 1024 ; } http { upstream srv.giraf.cs.aau.dk { server proxy ; } default_type application/octet-stream ; sendfile on ; keepalive_timeout 65 ; include /etc/nginx/sites-enabled/* ; gzip on ; client_max_body_size 20m ; }","title":"nginx.conf"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/proxy/#api","text":"The API is file /etc/nginx/sites-enabled/API specifies where the traffic for the API is supposed to go once the NGINX server receives it. We can use the proxy_pass http://API/ inside Docker because it uses static DNS names inside its network, all traffic for the API will be directed into the API service. This means that the API can be accessed through http://srv.giraf.cs.aau.dk/API/ on both port 80 and port 443 . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 server { listen 80 ; listen 443 ; server_name srv.giraf.cs.aau.dk ; location /API/ { proxy_buffer_size 128k ; proxy_buffers 4 256k ; proxy_busy_buffers_size 256k ; proxy_pass http://API/ ; proxy_set_header X-Real-IP $remote_addr ; proxy_set_header Host $host ; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for ; } }","title":"API"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/web_api/","text":"Web API \u00b6 The WEB API is a DotNet Core 2.2 application built as a REST API which serves all request to and from the Giraf Project. The WEB API has previously been served via the software development kit (SDK) via Docker. This resulted in a huge Docker Image consuming 2.24 GB . Along with enormous file size, it also included the appsettings.json file which contains the database's username and password in plaintext. Before the WEB API could be migrated from the old servers to the new one, the Docker Imaged needed to be made smaller in size, do to the limited disk space in the new servers. And the appsettings.json file needed to be removed from the finished Docker Image. To make these changes, only the Dockerfile needed to be changed. The two different versions of the file can be seen below. Old Dockerfile \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # base image from dockerhub FROM microsoft/aspnetcore-build:2 # execute command to setup environment # RUN apt-get update && apt-get -y install sqlite3 && rm -rf /var/lib/apt/lists/* # copy local files to container COPY . /srv/ # copy appsettings from script_stuff WORKDIR /srv/GirafRest ENV ASPNETCORE_ENVIRONMENT = Production #RUN dotnet add package Microsoft.EntityFrameworkCore.Design RUN dotnet restore # list available dbcontext (sometimes usefull) # RUN dotnet ef dbcontext list RUN dotnet ef database update RUN dotnet build EXPOSE 5000 ENTRYPOINT [ \"dotnet\" , \"run\" , \"--list\" ] New Dockerfile \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # Using microsoft dotnet software development kit as # the build envionment. FROM mcr.microsoft.com/dotnet/core/sdk:2.2 AS build-env WORKDIR /app # Copy csproj and restore as distinct layers COPY GirafRest/*.csproj ./ RUN dotnet restore # Copy everything else and build COPY ./GirafRest/ ./ # Build the app for production RUN dotnet publish -c Release -o out #------------------------------------------# # Using microsoft aps net core 2.2 as hosting envionment FROM mcr.microsoft.com/dotnet/core/aspnet:2.2 AS runtime-env WORKDIR /srv # COPY from build envionment into local container. COPY --from = build-env /app/out . # Remove the appsettings files from the container # so no passwords are pushed to docker hub RUN rm appsettings* # Expose the port intented for communications EXPOSE 5000 # Start running the app. ENTRYPOINT [ \"dotnet\" , \"GirafRest.dll\" , \"--list\" ] The changes resulted in a Docker Image that has decreased in size form 2.24Gb to 339Mb and where the appsettings file is only needed during compilation and is later removed before the image is pushed to Docker Hub. The repository can be found her: https://hub.docker.com/r/giraf/web-api","title":"Web API"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/web_api/#web-api","text":"The WEB API is a DotNet Core 2.2 application built as a REST API which serves all request to and from the Giraf Project. The WEB API has previously been served via the software development kit (SDK) via Docker. This resulted in a huge Docker Image consuming 2.24 GB . Along with enormous file size, it also included the appsettings.json file which contains the database's username and password in plaintext. Before the WEB API could be migrated from the old servers to the new one, the Docker Imaged needed to be made smaller in size, do to the limited disk space in the new servers. And the appsettings.json file needed to be removed from the finished Docker Image. To make these changes, only the Dockerfile needed to be changed. The two different versions of the file can be seen below.","title":"Web API"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/web_api/#old-dockerfile","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # base image from dockerhub FROM microsoft/aspnetcore-build:2 # execute command to setup environment # RUN apt-get update && apt-get -y install sqlite3 && rm -rf /var/lib/apt/lists/* # copy local files to container COPY . /srv/ # copy appsettings from script_stuff WORKDIR /srv/GirafRest ENV ASPNETCORE_ENVIRONMENT = Production #RUN dotnet add package Microsoft.EntityFrameworkCore.Design RUN dotnet restore # list available dbcontext (sometimes usefull) # RUN dotnet ef dbcontext list RUN dotnet ef database update RUN dotnet build EXPOSE 5000 ENTRYPOINT [ \"dotnet\" , \"run\" , \"--list\" ]","title":"Old Dockerfile"},{"location":"Legacy/weekplanner/Server/Docker/Docker_Swarm/web_api/#new-dockerfile","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # Using microsoft dotnet software development kit as # the build envionment. FROM mcr.microsoft.com/dotnet/core/sdk:2.2 AS build-env WORKDIR /app # Copy csproj and restore as distinct layers COPY GirafRest/*.csproj ./ RUN dotnet restore # Copy everything else and build COPY ./GirafRest/ ./ # Build the app for production RUN dotnet publish -c Release -o out #------------------------------------------# # Using microsoft aps net core 2.2 as hosting envionment FROM mcr.microsoft.com/dotnet/core/aspnet:2.2 AS runtime-env WORKDIR /srv # COPY from build envionment into local container. COPY --from = build-env /app/out . # Remove the appsettings files from the container # so no passwords are pushed to docker hub RUN rm appsettings* # Expose the port intented for communications EXPOSE 5000 # Start running the app. ENTRYPOINT [ \"dotnet\" , \"GirafRest.dll\" , \"--list\" ] The changes resulted in a Docker Image that has decreased in size form 2.24Gb to 339Mb and where the appsettings file is only needed during compilation and is later removed before the image is pushed to Docker Hub. The repository can be found her: https://hub.docker.com/r/giraf/web-api","title":"New Dockerfile"},{"location":"Legacy/weekplanner/Setup/Docker/","text":"Optional: Dockerisation \u00b6 Running the web-API with Docker \u00b6 Start by downloading the Docker, this can be either the Docker Engine itself (this requires more knowledge of Docker), or the easier alternative Docker Desktop (Linux, MacOS and Windows) Download Docker Desktop . Ensure that you cloned the web-API repository from this section Clone the web-API . In your terminal navigate to the cloned web-API folder on your local machine. In the terminal you can run one of 2 commands: docker compose up docker compose up -d The first command runs the docker compose from the terminal and effectively occupying the terminal. This can be avoided using the second command as this runs the docker compose in detached mode, such that the terminal can still be used for other things. Open Docker Desktop and confirm the application is running: You are now able to use the API, which is located on http://localhost:5000. Local Development with Docker(Optional) \u00b6 First follow the steps in Running the web-API with Docker . Next shutdown the web-api container within the application, in docker desktop this can be done, by clicking the stop icon. Shown here: After doing that they api should have stopped and you should see the following: Last step is to open your IDE, locate the LocalDocker.AppSettings.json file in the GirafAPI project. Copy the connection string and paste it into the Development.AppSettings.json connection string, if you copy the connection string make sure to change the server to localhost, and change the port to 5100. Another approach would be to change the launchsettings, specifically the ASPNETCORE_ENVIRONMENT to LocalDocker Now you should be able to develop in your IDE, run the API directly in the IDE, and utilize the Docker MySQL container.","title":"Optional: Dockerisation"},{"location":"Legacy/weekplanner/Setup/Docker/#optional-dockerisation","text":"","title":"Optional: Dockerisation"},{"location":"Legacy/weekplanner/Setup/Docker/#running-the-web-api-with-docker","text":"Start by downloading the Docker, this can be either the Docker Engine itself (this requires more knowledge of Docker), or the easier alternative Docker Desktop (Linux, MacOS and Windows) Download Docker Desktop . Ensure that you cloned the web-API repository from this section Clone the web-API . In your terminal navigate to the cloned web-API folder on your local machine. In the terminal you can run one of 2 commands: docker compose up docker compose up -d The first command runs the docker compose from the terminal and effectively occupying the terminal. This can be avoided using the second command as this runs the docker compose in detached mode, such that the terminal can still be used for other things. Open Docker Desktop and confirm the application is running: You are now able to use the API, which is located on http://localhost:5000.","title":"Running the web-API with Docker"},{"location":"Legacy/weekplanner/Setup/Docker/#local-development-with-dockeroptional","text":"First follow the steps in Running the web-API with Docker . Next shutdown the web-api container within the application, in docker desktop this can be done, by clicking the stop icon. Shown here: After doing that they api should have stopped and you should see the following: Last step is to open your IDE, locate the LocalDocker.AppSettings.json file in the GirafAPI project. Copy the connection string and paste it into the Development.AppSettings.json connection string, if you copy the connection string make sure to change the server to localhost, and change the port to 5100. Another approach would be to change the launchsettings, specifically the ASPNETCORE_ENVIRONMENT to LocalDocker Now you should be able to develop in your IDE, run the API directly in the IDE, and utilize the Docker MySQL container.","title":"Local Development with Docker(Optional)"},{"location":"Legacy/weekplanner/Setup/Index/","text":"Environment Setup \u00b6 Cloning the Repositories \u00b6 The weekplanner setup depends on you having the two main Git repositories: weekplanner and web-api \u2013 in order to get these repositories, you need to have installed Git locally. See this guide if you don't have it already (if you're on a Mac, Git is installed during the installation guide for GIRAF). To clone each of these repositories, create a directory, where you want your files to be kept. Be mindful of not storing the project in Dropbox, Google Drive or similar, as it might introduce unwanted behaviour, especially with file permissions etc., which isn't fun to debug. Then open the terminal of your choice (e.g. Terminal, iTerm, PowerShell etc.), and clone the repositories into the directory by running: git clone https://github.com/aau-giraf/weekplanner.git and git clone https://github.com/aau-giraf/web-api.git when you have done this, proceed to the setup guide for your operating system, which you can find in the sidebar. Pushing to GitHub \u00b6 To be able to contribute to GIRAF, you need to setup SSH keys for your GitHub account: Adding SSH Keys to GitHub (Mac and Windows) \u00b6 1. Generate an SSH key \u00b6 Use Terminal on Mac, or Git Bash on Windows 1 ssh-keygen -t ed25519 -C \"your_email@example.com\" Note: If you're using Windows and haven't installed Git Bash, download and install it from the official Git website. 2. Copy the SSH key \u00b6 For Mac \u00b6 1 pbcopy < ~/.ssh/id_ed25519.pub For Windows (Git Bash) \u00b6 1 cat ~/.ssh/id_ed25519.pub | clip Alternatively, you can manually copy the output of: 1 cat ~/.ssh/id_ed25519.pub 3. Add the key to GitHub \u00b6 Go to GitHub and log in Click your profile photo in the top right, then click \"Settings\" In the left sidebar, click \"SSH and GPG keys\" Click \"New SSH key\" or \"Add SSH key\" In the \"Title\" field, add a descriptive label for the new key Paste your key into the \"Key\" field Click \"Add SSH key\" 4. Test the connection \u00b6 For Mac (Terminal) and Windows (Git Bash) \u00b6 1 ssh -T git@github.com You should see a message like: \"Hi username! You've successfully authenticated, but GitHub does not provide shell access.\" Troubleshooting \u00b6 If you're on Windows and Git Bash isn't working, you can use the Command Prompt or PowerShell, but some commands may differ. Ensure your SSH agent is running. On Mac or Git Bash, use: bash eval \"$(ssh-agent -s)\" ssh-add ~/.ssh/id_ed25519 If you're still having issues, check GitHub's official documentation or support resources.","title":"Environment Setup"},{"location":"Legacy/weekplanner/Setup/Index/#environment-setup","text":"","title":"Environment Setup"},{"location":"Legacy/weekplanner/Setup/Index/#cloning-the-repositories","text":"The weekplanner setup depends on you having the two main Git repositories: weekplanner and web-api \u2013 in order to get these repositories, you need to have installed Git locally. See this guide if you don't have it already (if you're on a Mac, Git is installed during the installation guide for GIRAF). To clone each of these repositories, create a directory, where you want your files to be kept. Be mindful of not storing the project in Dropbox, Google Drive or similar, as it might introduce unwanted behaviour, especially with file permissions etc., which isn't fun to debug. Then open the terminal of your choice (e.g. Terminal, iTerm, PowerShell etc.), and clone the repositories into the directory by running: git clone https://github.com/aau-giraf/weekplanner.git and git clone https://github.com/aau-giraf/web-api.git when you have done this, proceed to the setup guide for your operating system, which you can find in the sidebar.","title":"Cloning the Repositories"},{"location":"Legacy/weekplanner/Setup/Index/#pushing-to-github","text":"To be able to contribute to GIRAF, you need to setup SSH keys for your GitHub account:","title":"Pushing to GitHub"},{"location":"Legacy/weekplanner/Setup/Index/#adding-ssh-keys-to-github-mac-and-windows","text":"","title":"Adding SSH Keys to GitHub (Mac and Windows)"},{"location":"Legacy/weekplanner/Setup/Index/#1-generate-an-ssh-key","text":"Use Terminal on Mac, or Git Bash on Windows 1 ssh-keygen -t ed25519 -C \"your_email@example.com\" Note: If you're using Windows and haven't installed Git Bash, download and install it from the official Git website.","title":"1. Generate an SSH key"},{"location":"Legacy/weekplanner/Setup/Index/#2-copy-the-ssh-key","text":"","title":"2. Copy the SSH key"},{"location":"Legacy/weekplanner/Setup/Index/#for-mac","text":"1 pbcopy < ~/.ssh/id_ed25519.pub","title":"For Mac"},{"location":"Legacy/weekplanner/Setup/Index/#for-windows-git-bash","text":"1 cat ~/.ssh/id_ed25519.pub | clip Alternatively, you can manually copy the output of: 1 cat ~/.ssh/id_ed25519.pub","title":"For Windows (Git Bash)"},{"location":"Legacy/weekplanner/Setup/Index/#3-add-the-key-to-github","text":"Go to GitHub and log in Click your profile photo in the top right, then click \"Settings\" In the left sidebar, click \"SSH and GPG keys\" Click \"New SSH key\" or \"Add SSH key\" In the \"Title\" field, add a descriptive label for the new key Paste your key into the \"Key\" field Click \"Add SSH key\"","title":"3. Add the key to GitHub"},{"location":"Legacy/weekplanner/Setup/Index/#4-test-the-connection","text":"","title":"4. Test the connection"},{"location":"Legacy/weekplanner/Setup/Index/#for-mac-terminal-and-windows-git-bash","text":"1 ssh -T git@github.com You should see a message like: \"Hi username! You've successfully authenticated, but GitHub does not provide shell access.\"","title":"For Mac (Terminal) and Windows (Git Bash)"},{"location":"Legacy/weekplanner/Setup/Index/#troubleshooting","text":"If you're on Windows and Git Bash isn't working, you can use the Command Prompt or PowerShell, but some commands may differ. Ensure your SSH agent is running. On Mac or Git Bash, use: bash eval \"$(ssh-agent -s)\" ssh-add ~/.ssh/id_ed25519 If you're still having issues, check GitHub's official documentation or support resources.","title":"Troubleshooting"},{"location":"Legacy/weekplanner/Setup/Linux/","text":"Linux Setup Guide \u00b6 Step 1: Update and Upgrade Your System \u00b6 Before installing new software, it's a good practice to update your system: 1 sudo apt update && sudo apt upgrade -y Step 2: Install OpenJDK \u00b6 Install OpenJDK using your distribution's package manager: 1 sudo apt install default-jdk Verify the installation: 1 java --version Step 3: Install Flutter \u00b6 Please refer to the Flutter installation guide if you face issues Prerequisites \u00b6 Install required dependencies \u00b6 1 sudo apt install clang cmake ninja-build pkg-config libgtk-3-dev liblzma-dev Installation \u00b6 Download Flutter: 1 wget https://storage.googleapis.com/flutter_infra_release/releases/stable/linux/flutter_linux_3.24.1-stable.tar.xz Extract the downloaded file: 1 tar xf flutter_linux_3.24.1-stable.tar.xz Add Flutter to your PATH. Add this line to your ~/.bashrc or ~/.zshrc file: 1 export PATH = \" $PATH :[PATH_TO_FLUTTER_GIT_DIRECTORY]/flutter/bin\" Replace [PATH_TO_FLUTTER_GIT_DIRECTORY] with the path where you extracted Flutter. Reload your shell configuration: 1 source ~/.bashrc # or source ~/.zshrc if you're using Zsh Verify the installation: 1 flutter --version Step 4: Install .NET 8.0 \u00b6 Add the Microsoft package signing key and repository: 1 2 3 wget https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb -O packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb rm packages-microsoft-prod.deb Install the .NET 8.0 SDK: 1 sudo apt update && sudo apt install -y dotnet-sdk-8.0 Verify the installation: 1 dotnet --version Step 5: Install MariaDB \u00b6 Install MariaDB: 1 sudo apt install mariadb-server Secure the MariaDB installation: 1 sudo mysql_secure_installation Follow the prompts to set a root password and secure your installation. Running GIRAF \u00b6 Refer to this guide Troubleshooting \u00b6 If you encounter any issues: Ensure your Linux distribution is up to date Run flutter doctor for diagnostics and follow its recommendations Check system logs for any error messages: journalctl -xe Ensure all required dependencies are installed If you're using a different Linux distribution, consult its documentation for equivalent commands Note: This guide assumes you're using a Debian-based distribution like Ubuntu. If you're using a different distribution, you may need to adjust some commands (e.g., use yum instead of apt for Red Hat-based systems).","title":"Linux Setup Guide"},{"location":"Legacy/weekplanner/Setup/Linux/#linux-setup-guide","text":"","title":"Linux Setup Guide"},{"location":"Legacy/weekplanner/Setup/Linux/#step-1-update-and-upgrade-your-system","text":"Before installing new software, it's a good practice to update your system: 1 sudo apt update && sudo apt upgrade -y","title":"Step 1: Update and Upgrade Your System"},{"location":"Legacy/weekplanner/Setup/Linux/#step-2-install-openjdk","text":"Install OpenJDK using your distribution's package manager: 1 sudo apt install default-jdk Verify the installation: 1 java --version","title":"Step 2: Install OpenJDK"},{"location":"Legacy/weekplanner/Setup/Linux/#step-3-install-flutter","text":"Please refer to the Flutter installation guide if you face issues","title":"Step 3: Install Flutter"},{"location":"Legacy/weekplanner/Setup/Linux/#prerequisites","text":"","title":"Prerequisites"},{"location":"Legacy/weekplanner/Setup/Linux/#install-required-dependencies","text":"1 sudo apt install clang cmake ninja-build pkg-config libgtk-3-dev liblzma-dev","title":"Install required dependencies"},{"location":"Legacy/weekplanner/Setup/Linux/#installation","text":"Download Flutter: 1 wget https://storage.googleapis.com/flutter_infra_release/releases/stable/linux/flutter_linux_3.24.1-stable.tar.xz Extract the downloaded file: 1 tar xf flutter_linux_3.24.1-stable.tar.xz Add Flutter to your PATH. Add this line to your ~/.bashrc or ~/.zshrc file: 1 export PATH = \" $PATH :[PATH_TO_FLUTTER_GIT_DIRECTORY]/flutter/bin\" Replace [PATH_TO_FLUTTER_GIT_DIRECTORY] with the path where you extracted Flutter. Reload your shell configuration: 1 source ~/.bashrc # or source ~/.zshrc if you're using Zsh Verify the installation: 1 flutter --version","title":"Installation"},{"location":"Legacy/weekplanner/Setup/Linux/#step-4-install-net-80","text":"Add the Microsoft package signing key and repository: 1 2 3 wget https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb -O packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb rm packages-microsoft-prod.deb Install the .NET 8.0 SDK: 1 sudo apt update && sudo apt install -y dotnet-sdk-8.0 Verify the installation: 1 dotnet --version","title":"Step 4: Install .NET 8.0"},{"location":"Legacy/weekplanner/Setup/Linux/#step-5-install-mariadb","text":"Install MariaDB: 1 sudo apt install mariadb-server Secure the MariaDB installation: 1 sudo mysql_secure_installation Follow the prompts to set a root password and secure your installation.","title":"Step 5: Install MariaDB"},{"location":"Legacy/weekplanner/Setup/Linux/#running-giraf","text":"Refer to this guide","title":"Running GIRAF"},{"location":"Legacy/weekplanner/Setup/Linux/#troubleshooting","text":"If you encounter any issues: Ensure your Linux distribution is up to date Run flutter doctor for diagnostics and follow its recommendations Check system logs for any error messages: journalctl -xe Ensure all required dependencies are installed If you're using a different Linux distribution, consult its documentation for equivalent commands Note: This guide assumes you're using a Debian-based distribution like Ubuntu. If you're using a different distribution, you may need to adjust some commands (e.g., use yum instead of apt for Red Hat-based systems).","title":"Troubleshooting"},{"location":"Legacy/weekplanner/Setup/MacOS/","text":"MacOS Setup Guide \u00b6 Step 1: Install Homebrew \u00b6 If you don't have Homebrew installed, you'll need to install it first: Open Terminal (Applications > Utilities > Terminal). Run the following command: 1 /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh ) \" Follow the prompts to complete the installation. Verify the installation: 1 brew --version Step 2: Install OpenJDK \u00b6 Open your Terminal and run: 1 brew install openjdk Step 3: Installing Xcode \u00b6 If the available version in the App Store isn't available for your device, please update macOS to the latest version Xcode is necessary for iOS development and running the iOS simulator: Install Xcode Open the App Store on your Mac. Search for \"Xcode\" and click \"Get\" or the download icon. Wait for the download and installation to complete (this may take some time as Xcode is a large application). Once Xcode is installed, open it to accept the license agreement. After Xcode is installed, install the Xcode Command Line Tools and the iOS simulator: Command Line Tools: Open Terminal (or iTerm etc.) Run xcode-select --install Accept the prompt to install iOS Simulator: Run xcodebuild -downloadPlatform iOS Run the simulator with open -a Simulator Step 4: Installing Flutter and Dart \u00b6 Prerequisites \u00b6 CocoaPods \u00b6 Install CocoaPods by running sudo gem install cocoapods in your terminal Rosetta \u00b6 Some Flutter components require the Rosetta 2 translation process on Macs running Apple silicon. To run all Flutter components on Apple silicon, install Rosetta 2. Run in your terminal: 1 sudo softwareupdate --install-rosetta --agree-to-license Installation \u00b6 The recommended IDE for Flutter is VS Code with the Flutter extension Open Terminal and run: 1 brew install --cask flutter Wait for installation. Then verify your installation: 1 flutter doctor Step 5. Install .NET \u00b6 Open your Terminal and run: 1 brew install dotnet Step 6. Install MariaDB \u00b6 Open your Terminal and run: 1 brew install mariadb Then start MariaDB as a service: 1 brew services start mariadb Running GIRAF \u00b6 Refer to this guide Troubleshooting \u00b6 If you encounter any issues: Ensure your macOS is up to date Try running brew doctor to check for any Homebrew-related issues Ensure Xcode and Command Line Tools are up to date: softwareupdate --all --install --force Run flutter doctor for diagnostics and follow its recommendations","title":"MacOS Setup Guide"},{"location":"Legacy/weekplanner/Setup/MacOS/#macos-setup-guide","text":"","title":"MacOS Setup Guide"},{"location":"Legacy/weekplanner/Setup/MacOS/#step-1-install-homebrew","text":"If you don't have Homebrew installed, you'll need to install it first: Open Terminal (Applications > Utilities > Terminal). Run the following command: 1 /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh ) \" Follow the prompts to complete the installation. Verify the installation: 1 brew --version","title":"Step 1: Install Homebrew"},{"location":"Legacy/weekplanner/Setup/MacOS/#step-2-install-openjdk","text":"Open your Terminal and run: 1 brew install openjdk","title":"Step 2: Install OpenJDK"},{"location":"Legacy/weekplanner/Setup/MacOS/#step-3-installing-xcode","text":"If the available version in the App Store isn't available for your device, please update macOS to the latest version Xcode is necessary for iOS development and running the iOS simulator: Install Xcode Open the App Store on your Mac. Search for \"Xcode\" and click \"Get\" or the download icon. Wait for the download and installation to complete (this may take some time as Xcode is a large application). Once Xcode is installed, open it to accept the license agreement. After Xcode is installed, install the Xcode Command Line Tools and the iOS simulator: Command Line Tools: Open Terminal (or iTerm etc.) Run xcode-select --install Accept the prompt to install iOS Simulator: Run xcodebuild -downloadPlatform iOS Run the simulator with open -a Simulator","title":"Step 3: Installing Xcode"},{"location":"Legacy/weekplanner/Setup/MacOS/#step-4-installing-flutter-and-dart","text":"","title":"Step 4: Installing Flutter and Dart"},{"location":"Legacy/weekplanner/Setup/MacOS/#prerequisites","text":"","title":"Prerequisites"},{"location":"Legacy/weekplanner/Setup/MacOS/#cocoapods","text":"Install CocoaPods by running sudo gem install cocoapods in your terminal","title":"CocoaPods"},{"location":"Legacy/weekplanner/Setup/MacOS/#rosetta","text":"Some Flutter components require the Rosetta 2 translation process on Macs running Apple silicon. To run all Flutter components on Apple silicon, install Rosetta 2. Run in your terminal: 1 sudo softwareupdate --install-rosetta --agree-to-license","title":"Rosetta"},{"location":"Legacy/weekplanner/Setup/MacOS/#installation","text":"The recommended IDE for Flutter is VS Code with the Flutter extension Open Terminal and run: 1 brew install --cask flutter Wait for installation. Then verify your installation: 1 flutter doctor","title":"Installation"},{"location":"Legacy/weekplanner/Setup/MacOS/#step-5-install-net","text":"Open your Terminal and run: 1 brew install dotnet","title":"Step 5. Install .NET"},{"location":"Legacy/weekplanner/Setup/MacOS/#step-6-install-mariadb","text":"Open your Terminal and run: 1 brew install mariadb Then start MariaDB as a service: 1 brew services start mariadb","title":"Step 6. Install MariaDB"},{"location":"Legacy/weekplanner/Setup/MacOS/#running-giraf","text":"Refer to this guide","title":"Running GIRAF"},{"location":"Legacy/weekplanner/Setup/MacOS/#troubleshooting","text":"If you encounter any issues: Ensure your macOS is up to date Try running brew doctor to check for any Homebrew-related issues Ensure Xcode and Command Line Tools are up to date: softwareupdate --all --install --force Run flutter doctor for diagnostics and follow its recommendations","title":"Troubleshooting"},{"location":"Legacy/weekplanner/Setup/Running_GIRAF/","text":"Running GIRAF \u00b6 Preliminaries \u00b6 Clone both https://github.com:aau-giraf/weekplanner.git and https://github.com:aau-giraf/web-api.git as mentioned in \" Environment Setup \" Step 1. Running the Web-API \u00b6 Remember to be in the GirafAPI directory when running commands! Open appsettings.Development.json in the cloned web-api/GirafAPI directory from earlier and change password to what you selected when installing MariaDB Navigate to the cloned web-api/GirafAPI directory from earlier, either in Terminal (macOS/Linux) or PowerShell (Windows), and run: dotnet restore dotnet ef database update dotnet run --sample-data Step 2. iOS Simulator \u00b6 Open Simulator app (CMD + Space to search for it) Navigate to the weekplanner directory, that you cloned Run cp .env.example .env in your Terminal Run flutter pub get in your Terminal Run flutter run in your Terminal Step 2. Android Simulator \u00b6 If you have an old version of Android Studio, you need to update the SDK toolchain etc. to the newest versions before proceeding Open the weekplanner directory that you cloned Make a copy of .env.example , and rename the copy to .env If using Docker etc., edit the .env file to reflect this Open Android Studio Select the pre-installed Android emulator near the top-right corner, or create a new device in Tools > Device Manager Select the main.dart runtime and press Run","title":"Running GIRAF"},{"location":"Legacy/weekplanner/Setup/Running_GIRAF/#running-giraf","text":"","title":"Running GIRAF"},{"location":"Legacy/weekplanner/Setup/Running_GIRAF/#preliminaries","text":"Clone both https://github.com:aau-giraf/weekplanner.git and https://github.com:aau-giraf/web-api.git as mentioned in \" Environment Setup \"","title":"Preliminaries"},{"location":"Legacy/weekplanner/Setup/Running_GIRAF/#step-1-running-the-web-api","text":"Remember to be in the GirafAPI directory when running commands! Open appsettings.Development.json in the cloned web-api/GirafAPI directory from earlier and change password to what you selected when installing MariaDB Navigate to the cloned web-api/GirafAPI directory from earlier, either in Terminal (macOS/Linux) or PowerShell (Windows), and run: dotnet restore dotnet ef database update dotnet run --sample-data","title":"Step 1. Running the Web-API"},{"location":"Legacy/weekplanner/Setup/Running_GIRAF/#step-2-ios-simulator","text":"Open Simulator app (CMD + Space to search for it) Navigate to the weekplanner directory, that you cloned Run cp .env.example .env in your Terminal Run flutter pub get in your Terminal Run flutter run in your Terminal","title":"Step 2. iOS Simulator"},{"location":"Legacy/weekplanner/Setup/Running_GIRAF/#step-2-android-simulator","text":"If you have an old version of Android Studio, you need to update the SDK toolchain etc. to the newest versions before proceeding Open the weekplanner directory that you cloned Make a copy of .env.example , and rename the copy to .env If using Docker etc., edit the .env file to reflect this Open Android Studio Select the pre-installed Android emulator near the top-right corner, or create a new device in Tools > Device Manager Select the main.dart runtime and press Run","title":"Step 2. Android Simulator"},{"location":"Legacy/weekplanner/Setup/Windows/","text":"Windows Setup Guide \u00b6 Be mindful that your computer needs to support virtualisation in order to run the Android Simulator! Step 1. Install OpenJDK (or similar) \u00b6 Download OpenJDK Run through the installer Step 2: Install Flutter \u00b6 Create a folder where you want to install Flutter (e.g., C:\\dev ). Download the Flutter SDK Extract the downloaded zip file into the folder you created (e.g., C:\\dev\\flutter ). Add Flutter to your PATH: Open the Start menu and search for \"Environment Variables\" Click on \"Edit the system environment variables\" Click the \"Environment Variables\" button Under \"System variables\", find the \"Path\" variable and click \"Edit\" Click \"New\" and add the full path to the flutter\\bin directory (e.g., C:\\dev\\flutter\\bin ) Click \"OK\" to save the changes Open a new PowerShell window and verify the installation: 1 flutter - -version Step 3: Set up Android Studio \u00b6 Download Android Studio Run the installer and follow the installation wizard. During installation, ensure that the \"Android Virtual Device\" option is selected. Once installed, open Android Studio. Go to File > Settings > Plugins (on Windows) Search for and install the Flutter and Dart plugins Restart Android Studio to activate the plugins Go into Settings > Languages & Frameworks > Dart and set the relevant path to the Dart SDK, e.g. C:\\dev\\flutter\\bin\\cache\\dart-sdk , and enable the weekplanner module Step 4. Installing MariaDB \u00b6 If you have MySQL or similar installed, you don't need to follow this step. Postgres doesn't count. Download MariaDB Select the latest stable version for Windows and choose the appropriate package (MSI) for your system architecture (32-bit or 64-bit) Run the wizard and set a root password you'll be able to remember later. Select HeidiSQL and configure other options as needed (default settings are usually fine). Open HeidiSQL and log in with the password you chose earlier Right-click in the left panel, and choose New > Database and make a database called giraf Step 5. Installing .NET with Entity Framework \u00b6 Download the .NET 8 SDK installer for Windows Run the installer and follow the prompts Run the following command: dotnet --version \u2013 you should see the installed .NET version (8.x.x) Run the following command: dotnet tool install --global dotnet-ef You can then run dotnet ef to verify the installation. Running GIRAF \u00b6 Refer to this guide Troubleshooting \u00b6 If you encounter any issues: Ensure your Windows installation is up to date Run flutter doctor for diagnostics and follow its recommendations Check the official Flutter documentation for known issues For permission issues, try running PowerShell as Administrator Ensure Java is in your PATH environment variable","title":"Windows Setup Guide"},{"location":"Legacy/weekplanner/Setup/Windows/#windows-setup-guide","text":"Be mindful that your computer needs to support virtualisation in order to run the Android Simulator!","title":"Windows Setup Guide"},{"location":"Legacy/weekplanner/Setup/Windows/#step-1-install-openjdk-or-similar","text":"Download OpenJDK Run through the installer","title":"Step 1. Install OpenJDK (or similar)"},{"location":"Legacy/weekplanner/Setup/Windows/#step-2-install-flutter","text":"Create a folder where you want to install Flutter (e.g., C:\\dev ). Download the Flutter SDK Extract the downloaded zip file into the folder you created (e.g., C:\\dev\\flutter ). Add Flutter to your PATH: Open the Start menu and search for \"Environment Variables\" Click on \"Edit the system environment variables\" Click the \"Environment Variables\" button Under \"System variables\", find the \"Path\" variable and click \"Edit\" Click \"New\" and add the full path to the flutter\\bin directory (e.g., C:\\dev\\flutter\\bin ) Click \"OK\" to save the changes Open a new PowerShell window and verify the installation: 1 flutter - -version","title":"Step 2: Install Flutter"},{"location":"Legacy/weekplanner/Setup/Windows/#step-3-set-up-android-studio","text":"Download Android Studio Run the installer and follow the installation wizard. During installation, ensure that the \"Android Virtual Device\" option is selected. Once installed, open Android Studio. Go to File > Settings > Plugins (on Windows) Search for and install the Flutter and Dart plugins Restart Android Studio to activate the plugins Go into Settings > Languages & Frameworks > Dart and set the relevant path to the Dart SDK, e.g. C:\\dev\\flutter\\bin\\cache\\dart-sdk , and enable the weekplanner module","title":"Step 3: Set up Android Studio"},{"location":"Legacy/weekplanner/Setup/Windows/#step-4-installing-mariadb","text":"If you have MySQL or similar installed, you don't need to follow this step. Postgres doesn't count. Download MariaDB Select the latest stable version for Windows and choose the appropriate package (MSI) for your system architecture (32-bit or 64-bit) Run the wizard and set a root password you'll be able to remember later. Select HeidiSQL and configure other options as needed (default settings are usually fine). Open HeidiSQL and log in with the password you chose earlier Right-click in the left panel, and choose New > Database and make a database called giraf","title":"Step 4. Installing MariaDB"},{"location":"Legacy/weekplanner/Setup/Windows/#step-5-installing-net-with-entity-framework","text":"Download the .NET 8 SDK installer for Windows Run the installer and follow the prompts Run the following command: dotnet --version \u2013 you should see the installed .NET version (8.x.x) Run the following command: dotnet tool install --global dotnet-ef You can then run dotnet ef to verify the installation.","title":"Step 5. Installing .NET with Entity Framework"},{"location":"Legacy/weekplanner/Setup/Windows/#running-giraf","text":"Refer to this guide","title":"Running GIRAF"},{"location":"Legacy/weekplanner/Setup/Windows/#troubleshooting","text":"If you encounter any issues: Ensure your Windows installation is up to date Run flutter doctor for diagnostics and follow its recommendations Check the official Flutter documentation for known issues For permission issues, try running PowerShell as Administrator Ensure Java is in your PATH environment variable","title":"Troubleshooting"},{"location":"Legacy/weekplanner/UI_Design/","text":"Overview \u00b6 This section contains guidelines and resources for designing UI elements in the GIRAF project. Design Guide An overall guide for how to design common UI elements in the GIRAF project. Prototypes The prototypes for the various features throughout the GIRAF project.","title":"Overview"},{"location":"Legacy/weekplanner/UI_Design/#overview","text":"This section contains guidelines and resources for designing UI elements in the GIRAF project. Design Guide An overall guide for how to design common UI elements in the GIRAF project. Prototypes The prototypes for the various features throughout the GIRAF project.","title":"Overview"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/","text":"Overview \u00b6 This is the design guide of the GIRAF project. Developers in the GIRAF project should conform to this guide when designing the visual elements in the GIRAF project. Table of contents \u00b6 Application structure Buttons Colours Dialog Greyscale Icons Pictogram representation Loading indicators Typography Scroll","title":"Overview"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/#overview","text":"This is the design guide of the GIRAF project. Developers in the GIRAF project should conform to this guide when designing the visual elements in the GIRAF project.","title":"Overview"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/#table-of-contents","text":"Application structure Buttons Colours Dialog Greyscale Icons Pictogram representation Loading indicators Typography Scroll","title":"Table of contents"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/application_structure/","text":"Application structure \u00b6 App bar \u00b6 The app bar should be present in every screen of the app so that the user can easily navigate around in the app. The app bar that should always be used is the GirafAppBar, which is a widget that is implemented in the weekplanner flutter project. This widget includes all the buttons that should be present on the app bar. Bottom bar \u00b6 Bottom bars must be entirely contextual and depend on the content displayed in the current activity. The bottom bar should always be styled similarly to the app bar with the same color. Content \u00b6 The main content of the applications should be focused in the center of the layout and any menu bars should be above, under, and to the sides of the main content. The layout should utilize widgets to achieve the desired layout. Item familiarity \u00b6 It is important that the users are familiar with the items they are manipulating. One should make sure that a given item looks the same when navigating, creating and editing.","title":"Application structure"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/application_structure/#application-structure","text":"","title":"Application structure"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/application_structure/#app-bar","text":"The app bar should be present in every screen of the app so that the user can easily navigate around in the app. The app bar that should always be used is the GirafAppBar, which is a widget that is implemented in the weekplanner flutter project. This widget includes all the buttons that should be present on the app bar.","title":"App bar"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/application_structure/#bottom-bar","text":"Bottom bars must be entirely contextual and depend on the content displayed in the current activity. The bottom bar should always be styled similarly to the app bar with the same color.","title":"Bottom bar"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/application_structure/#content","text":"The main content of the applications should be focused in the center of the layout and any menu bars should be above, under, and to the sides of the main content. The layout should utilize widgets to achieve the desired layout.","title":"Content"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/application_structure/#item-familiarity","text":"It is important that the users are familiar with the items they are manipulating. One should make sure that a given item looks the same when navigating, creating and editing.","title":"Item familiarity"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/buttons/","text":"Buttons \u00b6 This section describes the design guides for using buttons in GIRAF. States \u00b6 A button should have three states: default, pressed and disabled. The background of a default button should have a yellow/orange (see color guide) gradient background with a darker border. When a button is pressed, the background should become darker alongside an even darker border. The background and the border should be identical to the default button when a button is disabled, however this button should have an opacity of 40%. Default button Pressed button Disabled button A widget called GirafButton has been created in the weekplanner repo which should be used for buttons in the weekplanner app. If the button is added within the citizen view, no text should be added to buttons. Content \u00b6 Buttons can either contain text, an icon or both. If the action of the button can be symbolized easily with an icon that the users are familiar with, only an icon should be used for the button. Text should be used to describe it instead if an icon is not sufficient for explaining the action. An icon together with text should be used in cases where the icon helps to understand the text. Contextual ordering \u00b6 When a collection of buttons are shown in the same context, the buttons should be ordered by whether or not the button represents an action to confirm a change. The leftmost buttons should cause the action to not occur, such as cancelling the deletion, and the rightmost buttons should confirm the action, such as saying yes to a deletion. Multiple state selection buttons \u00b6 If multiple states need to be represented it should be done by creating a multiple state button. On this button the current state should have a darker gradient than the other states. This should only be used when only one of the states can be selected at the time.","title":"Buttons"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/buttons/#buttons","text":"This section describes the design guides for using buttons in GIRAF.","title":"Buttons"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/buttons/#states","text":"A button should have three states: default, pressed and disabled. The background of a default button should have a yellow/orange (see color guide) gradient background with a darker border. When a button is pressed, the background should become darker alongside an even darker border. The background and the border should be identical to the default button when a button is disabled, however this button should have an opacity of 40%. Default button Pressed button Disabled button A widget called GirafButton has been created in the weekplanner repo which should be used for buttons in the weekplanner app. If the button is added within the citizen view, no text should be added to buttons.","title":"States"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/buttons/#content","text":"Buttons can either contain text, an icon or both. If the action of the button can be symbolized easily with an icon that the users are familiar with, only an icon should be used for the button. Text should be used to describe it instead if an icon is not sufficient for explaining the action. An icon together with text should be used in cases where the icon helps to understand the text.","title":"Content"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/buttons/#contextual-ordering","text":"When a collection of buttons are shown in the same context, the buttons should be ordered by whether or not the button represents an action to confirm a change. The leftmost buttons should cause the action to not occur, such as cancelling the deletion, and the rightmost buttons should confirm the action, such as saying yes to a deletion.","title":"Contextual ordering"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/buttons/#multiple-state-selection-buttons","text":"If multiple states need to be represented it should be done by creating a multiple state button. On this button the current state should have a darker gradient than the other states. This should only be used when only one of the states can be selected at the time.","title":"Multiple state selection buttons"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/colors/","text":"Colors \u00b6 This chapter shows which colors should be used for the different components in the application. Throughout this a single color in each entry will denote a solid color, while two colors in an entry will denote a gradient between the two colors. Text and background \u00b6 Usage RGB HEX Color Regular text-color 0,0,0 #000000 Text-color used to indicate placeholder or hint-texts 170,170,170 #AAAAAA Background-color for any applications window background 0,0,0 #000000 Background-color for any activity 233,233,233 #E9E9E9 Showcase help text-color 255,255,255 #FFFFFF Buttons \u00b6 Gradients de\ufb01ned below are from top to bottom. Also note that the colors for the disabled button must be transparent (40% opacity). Usage RGB HEX Color Regular button background 255,205,89 255,157,0 #FFCD59 #FF9D00 Regular button stroke/border 138,110,0 #8A6E00 Pressed button background 212,173,47 255,157,0 #D4AD2F #FF9D00 Pressed button stroke/border #493700 73,55,0 Images \u00b6 Usage RGB HEX Color Image background-color 255,255,255 #FFFFFF Image border-color 0,0,0 #000000 Image marking-color 254,215,108 #FED76C Bars \u00b6 Usage RGB HEX Color Background of any bar 253,187,85 254,215,108 #FDBB55 #FED76C Stroke/border of the bar 229,190,83 #E5BE53 Gradient for top bars is top to bottom. Gradient for side bars is left to right. Gradient for bottom bars is bottom to top. Week indicators \u00b6 These colors must be used whenever a certain weekday is referenced. Note that these colors are primarily used to increase the usability for citizens because they are already familiar with this color scheme. Usage RGB HEX Color Monday 8,160,69 #08A045 Tuesday 84,13,110 #540D6E Wednesday 247,127,0 #F77F00 Thursday 0,71,119 #004777 Friday 249,200,14 #F9C80E Saturday 219,43,57 #DB2B39 Sunday 255,255,255 #FFFFFF Page indicator \u00b6 These colors must be used for indicating which page the user is currently on. Usage RGB HEX Color Active page 255,157,0 #FF9D00 Inactive page 255,205,89 #FFCD59","title":"Colors"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/colors/#colors","text":"This chapter shows which colors should be used for the different components in the application. Throughout this a single color in each entry will denote a solid color, while two colors in an entry will denote a gradient between the two colors.","title":"Colors"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/colors/#text-and-background","text":"Usage RGB HEX Color Regular text-color 0,0,0 #000000 Text-color used to indicate placeholder or hint-texts 170,170,170 #AAAAAA Background-color for any applications window background 0,0,0 #000000 Background-color for any activity 233,233,233 #E9E9E9 Showcase help text-color 255,255,255 #FFFFFF","title":"Text and background"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/colors/#buttons","text":"Gradients de\ufb01ned below are from top to bottom. Also note that the colors for the disabled button must be transparent (40% opacity). Usage RGB HEX Color Regular button background 255,205,89 255,157,0 #FFCD59 #FF9D00 Regular button stroke/border 138,110,0 #8A6E00 Pressed button background 212,173,47 255,157,0 #D4AD2F #FF9D00 Pressed button stroke/border #493700 73,55,0","title":"Buttons"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/colors/#images","text":"Usage RGB HEX Color Image background-color 255,255,255 #FFFFFF Image border-color 0,0,0 #000000 Image marking-color 254,215,108 #FED76C","title":"Images"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/colors/#bars","text":"Usage RGB HEX Color Background of any bar 253,187,85 254,215,108 #FDBB55 #FED76C Stroke/border of the bar 229,190,83 #E5BE53 Gradient for top bars is top to bottom. Gradient for side bars is left to right. Gradient for bottom bars is bottom to top.","title":"Bars"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/colors/#week-indicators","text":"These colors must be used whenever a certain weekday is referenced. Note that these colors are primarily used to increase the usability for citizens because they are already familiar with this color scheme. Usage RGB HEX Color Monday 8,160,69 #08A045 Tuesday 84,13,110 #540D6E Wednesday 247,127,0 #F77F00 Thursday 0,71,119 #004777 Friday 249,200,14 #F9C80E Saturday 219,43,57 #DB2B39 Sunday 255,255,255 #FFFFFF","title":"Week indicators"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/colors/#page-indicator","text":"These colors must be used for indicating which page the user is currently on. Usage RGB HEX Color Active page 255,157,0 #FF9D00 Inactive page 255,205,89 #FFCD59","title":"Page indicator"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/dialog/","text":"Dialogue \u00b6 When a user has to respond to an event, dialogs should be used. A dialog consists of a title, a description, some buttons and possibly some additional views. The usage of dialogues should be minimised as much as possible in citizen view. The dialogues should not contain any text if it is used. Confirm dialog \u00b6 When a user needs to confirm that some action is going to happen, the confirm dialog should be used as it looks here A widget called GirafConfirmDialog has been created in the weekplanner repo which should be used for confirm dialogs in the weekplanner app. Notify dialog \u00b6 When a user needs to be notified that some action has happened, the notify dialog should be used as it looks here A widget called GirafNotifyDialog has been created in the weekplanner repo which should be used for notification dialogs in the weekplanner app. Long running tasks should generally not block the GUI. Any task that can potentially take a long time should be done on a background thread and NOT on the main GUI thread. Custom buttons dialog \u00b6 For dialogs that contain more than two buttons, the Custom buttons dialog can be used as it is seen here. The buttons used for it should comply with the design guides for buttons. Customized dialog \u00b6 Some uses of dialogs might be more specialized than the already existing ones. If one wants to add input fields or a custom view one should use the dialog shown here. This example shows the usecase when a user needs to edit a category. It is important that if buttons should be added to this type of dialog it must be placed in the very bottom of the dialog and should be divided as shown in the example. Also note that buttons should be 40dp in height.","title":"Dialogue"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/dialog/#dialogue","text":"When a user has to respond to an event, dialogs should be used. A dialog consists of a title, a description, some buttons and possibly some additional views. The usage of dialogues should be minimised as much as possible in citizen view. The dialogues should not contain any text if it is used.","title":"Dialogue"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/dialog/#confirm-dialog","text":"When a user needs to confirm that some action is going to happen, the confirm dialog should be used as it looks here A widget called GirafConfirmDialog has been created in the weekplanner repo which should be used for confirm dialogs in the weekplanner app.","title":"Confirm dialog"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/dialog/#notify-dialog","text":"When a user needs to be notified that some action has happened, the notify dialog should be used as it looks here A widget called GirafNotifyDialog has been created in the weekplanner repo which should be used for notification dialogs in the weekplanner app. Long running tasks should generally not block the GUI. Any task that can potentially take a long time should be done on a background thread and NOT on the main GUI thread.","title":"Notify dialog"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/dialog/#custom-buttons-dialog","text":"For dialogs that contain more than two buttons, the Custom buttons dialog can be used as it is seen here. The buttons used for it should comply with the design guides for buttons.","title":"Custom buttons dialog"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/dialog/#customized-dialog","text":"Some uses of dialogs might be more specialized than the already existing ones. If one wants to add input fields or a custom view one should use the dialog shown here. This example shows the usecase when a user needs to edit a category. It is important that if buttons should be added to this type of dialog it must be placed in the very bottom of the dialog and should be divided as shown in the example. Also note that buttons should be 40dp in height.","title":"Customized dialog"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/greyscale/","text":"Greyscale \u00b6 A guardian should be able to change a citizen's colour scheme to greyscale if the citizen prefers this or finds too many colours overwhelming. The guardian should have the option to change the whole page in citizen view to greyscale, or just the weekplan such that the top bar colour still remains. These examples are shown on the prototypes below:","title":"Greyscale"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/greyscale/#greyscale","text":"A guardian should be able to change a citizen's colour scheme to greyscale if the citizen prefers this or finds too many colours overwhelming. The guardian should have the option to change the whole page in citizen view to greyscale, or just the weekplan such that the top bar colour still remains. These examples are shown on the prototypes below:","title":"Greyscale"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/icons/","text":"Icon guide \u00b6 These, and only these, icons should be used within the GIRAF project to ensure that no icon is undescribed or has multiple meanings. If a new icon is added, it should be added to this table. Name Icon Description Add Adds a new instance of an object AddFolder Creates a new Folder AddTimer Adds a new timer to object Back Returns the user to the previous page BurgerMenu Opens the side menu Camera Indicates a camera can be used to take pictures or to film a video Cancel Cancels whatever the user is doing ChangeToCitizen Changes from guardian to citizen mode ChangeToGuardian Changes from citizen to guardian mode Checkmark (accept) Can be used for when a task is completed by a citizen Copy Indicates that an instance of some object can be copied Delete Deletes selected instances of an object Edit Edits the given object FallbackImage Shows that a image was not loaded correctly Folder Folder for categorizing (e.g. citizens) Gallery Opens the gallery of device Help Opens a help menu Home Returns user to the overview of the weekplan Information Opens a screen with information like the users legal rights Log in TBD Logs in the given user, if credentials are correct Log out Signs out the current user and sign into another account Pause Pauses the timer NotSynced TBD Indicates that the current object is not syncronized Profile Goes to the users profile ProfileSettings Goes to the users profile settings Redo Reverts an undo to previous state Save Saves changes made Search Indicates that it is possible to search for something Settings Accesses the settings menu Start Starts the timer Stop Stops and resets the timer Pause Pauses the timer Synced TBD Indicates that the current object is syncronized Undo Reverts latest action AddCitizen Adds a new instance of a citizen LoadingSpinner Shown when something needs to be loaded","title":"Icon guide"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/icons/#icon-guide","text":"These, and only these, icons should be used within the GIRAF project to ensure that no icon is undescribed or has multiple meanings. If a new icon is added, it should be added to this table. Name Icon Description Add Adds a new instance of an object AddFolder Creates a new Folder AddTimer Adds a new timer to object Back Returns the user to the previous page BurgerMenu Opens the side menu Camera Indicates a camera can be used to take pictures or to film a video Cancel Cancels whatever the user is doing ChangeToCitizen Changes from guardian to citizen mode ChangeToGuardian Changes from citizen to guardian mode Checkmark (accept) Can be used for when a task is completed by a citizen Copy Indicates that an instance of some object can be copied Delete Deletes selected instances of an object Edit Edits the given object FallbackImage Shows that a image was not loaded correctly Folder Folder for categorizing (e.g. citizens) Gallery Opens the gallery of device Help Opens a help menu Home Returns user to the overview of the weekplan Information Opens a screen with information like the users legal rights Log in TBD Logs in the given user, if credentials are correct Log out Signs out the current user and sign into another account Pause Pauses the timer NotSynced TBD Indicates that the current object is not syncronized Profile Goes to the users profile ProfileSettings Goes to the users profile settings Redo Reverts an undo to previous state Save Saves changes made Search Indicates that it is possible to search for something Settings Accesses the settings menu Start Starts the timer Stop Stops and resets the timer Pause Pauses the timer Synced TBD Indicates that the current object is syncronized Undo Reverts latest action AddCitizen Adds a new instance of a citizen LoadingSpinner Shown when something needs to be loaded","title":"Icon guide"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/loading_indicators/","text":"Loading indicators \u00b6 The system should show loading indicators whenever it is performing an action that might take a while to process. There are two kinds of loading indicators: Progress bar \u00b6 The progress bar should be shown if there is a reliable way of calculating the actual progress of the running task. Loading spinner \u00b6 If there is no reliable way of calculating the actual progress of a running task, a loading spinner should be displayed to ensure that the user does not think that the system is frozen. A method that shows the loading spinner widget has been created in the weekplanner repo which should be used for loading spinners throughout the weekplanner app.","title":"Loading indicators"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/loading_indicators/#loading-indicators","text":"The system should show loading indicators whenever it is performing an action that might take a while to process. There are two kinds of loading indicators:","title":"Loading indicators"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/loading_indicators/#progress-bar","text":"The progress bar should be shown if there is a reliable way of calculating the actual progress of the running task.","title":"Progress bar"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/loading_indicators/#loading-spinner","text":"If there is no reliable way of calculating the actual progress of a running task, a loading spinner should be displayed to ensure that the user does not think that the system is frozen. A method that shows the loading spinner widget has been created in the weekplanner repo which should be used for loading spinners throughout the weekplanner app.","title":"Loading spinner"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/pictogram_representation/","text":"Pictogram representation \u00b6 Pictograms within the application should have the aspect ratio 1:1. Thereby all pictograms will get a uniform design. The image should therefore be cropped upon upload. The cropping should be visualized to the user.","title":"Pictogram representation"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/pictogram_representation/#pictogram-representation","text":"Pictograms within the application should have the aspect ratio 1:1. Thereby all pictograms will get a uniform design. The image should therefore be cropped upon upload. The cropping should be visualized to the user.","title":"Pictogram representation"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/scroll/","text":"Indication of scroll \u00b6 When showing whether the user is capable of scrolling, different visual cues should be used. Citizens \u00b6 When showing scrollability for Citizens a gradient should be used, as arrows or scroll bars might not be understood. Guardians \u00b6 When showing scrollability for guardians however, a scroll bar should be used. This can both be used vertically and horisontaly , depending on the scrolling direction.","title":"Indication of scroll"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/scroll/#indication-of-scroll","text":"When showing whether the user is capable of scrolling, different visual cues should be used.","title":"Indication of scroll"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/scroll/#citizens","text":"When showing scrollability for Citizens a gradient should be used, as arrows or scroll bars might not be understood.","title":"Citizens"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/scroll/#guardians","text":"When showing scrollability for guardians however, a scroll bar should be used. This can both be used vertically and horisontaly , depending on the scrolling direction.","title":"Guardians"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/typography/","text":"Typography \u00b6 All text in the system should be consistent in terms of font, sizes and colors. Text colors \u00b6 All text should by default be black (see colors section of the design guide). In case the text refers to an inactive element, the grey text color should be used. Font family \u00b6 The font should always be the Quicksand font , as the customers prefer that the font is similar to how the citizens learn to write, especially in relation to the letter a. Font size \u00b6 The font size should follow the declared sizes in the file font_sizes. In the file the font sizes for pictograms (150), timers (50) and the buttons for the activity screen (23) are declared. Besides these there are also 3 general font sizes for the rest of the application, these being declared as small (20), medium (24) and large (30). These are used for general text in the app and can be used as the developer sees fit. Bold, italic and underlined text \u00b6 The text should never be bold, italic or underlined in order to maintain a consistent design.","title":"Typography"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/typography/#typography","text":"All text in the system should be consistent in terms of font, sizes and colors.","title":"Typography"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/typography/#text-colors","text":"All text should by default be black (see colors section of the design guide). In case the text refers to an inactive element, the grey text color should be used.","title":"Text colors"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/typography/#font-family","text":"The font should always be the Quicksand font , as the customers prefer that the font is similar to how the citizens learn to write, especially in relation to the letter a.","title":"Font family"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/typography/#font-size","text":"The font size should follow the declared sizes in the file font_sizes. In the file the font sizes for pictograms (150), timers (50) and the buttons for the activity screen (23) are declared. Besides these there are also 3 general font sizes for the rest of the application, these being declared as small (20), medium (24) and large (30). These are used for general text in the app and can be used as the developer sees fit.","title":"Font size"},{"location":"Legacy/weekplanner/UI_Design/Design_Guide/typography/#bold-italic-and-underlined-text","text":"The text should never be bold, italic or underlined in order to maintain a consistent design.","title":"Bold, italic and underlined text"},{"location":"Legacy/weekplanner/UI_Design/Prototypes/","text":"Overview \u00b6 This section contains the prototypes used for GIRAF, and guidelines for how to make new ones. Semester .xd .pdf 2020E Prototypes2020E.xd Prototypes2020E.pdf 2020F Prototypes2020F.xd Prototypes2020F.pdf 2019 prototypes2019.xd prototypes2019.pdf","title":"Overview"},{"location":"Legacy/weekplanner/UI_Design/Prototypes/#overview","text":"This section contains the prototypes used for GIRAF, and guidelines for how to make new ones. Semester .xd .pdf 2020E Prototypes2020E.xd Prototypes2020E.pdf 2020F Prototypes2020F.xd Prototypes2020F.pdf 2019 prototypes2019.xd prototypes2019.pdf","title":"Overview"},{"location":"Legacy/weekplanner/UI_Design/Prototypes/prototype_guidelines/","text":"Guidelines \u00b6 The system described here was defined by the PO group of the spring semester 2020. A more detailed description is available in their report. The prototype administration system was created to keep track of the prototypes' state and its context. Workspace Organisation \u00b6 The workspace in Adobe XD has been divided to categorize prototypes based on their state as shown by the image below: The figure below describes how the prototype transistions between the states: The table below gives a short description of each state: State Description Concept Prototype is in development (either because it has just been created or because changes were requested by the customers). Review Prototype is ready to be reviewed by customers. Approved Prototype has been reviewed and approved by customers. Implemented Prototype has been implemented. Rejected Prototype has been reviewed and rejected by customers. Naming Convention \u00b6 The format of prototype names is defined as: yyyyx_ContextID_CompareID_ViewID_RelateID_GitID: <Short description> The table below describes each part of the name: Part Description yyyyx yyyy is the year and x is the semester ( f represents spring and e represents fall). Prototypes made during the spring semester of 2020 are named 2020f . ContextID An integer that is unique for the semester and represents the context of the prototypes. E.g. if four prototypes have been created to illustrate the feature of deleting pictograms, they should have the same ContextID . CompareID If some prototypes illustrate one way to implement a feature while others illustrate a different way to implement the same feature (they should share ContextID ) and they are being compared at the customer review, they should be given a letter ( A , B , C etc.). ViewID Either GUA or CIT depending whether the prototype illustrates guardians view or citizen's view. Should always be included. RelateID One or more integers (if there are more they should be seperated by hyphens) that describe how prototypes with the same yyyyx , ContextID , CompareID and ViewID are related. If the name is already unique without RelateID it should be exluded GitID The GitHub issue that the prototype is related to. Use abbreviations so WP273 = weekplanner#273 and PR12 = pictogram-reader#12 The naming convention is illustrated by the figure below: Press button indicates that a button, text field or similar on one prototype leads to another. E.g. pressing one button on 2020f_12_GUA_1-1_WP273 leads to 2020f_12_GUA_2_WP273 while pressing another leads to 2020f_12_GUA_1-2_WP273. But there is no button on 2020f_12_GUA_1-2_WP273 that leads to 2020f_12_GUA_2_WP273.","title":"Guidelines"},{"location":"Legacy/weekplanner/UI_Design/Prototypes/prototype_guidelines/#guidelines","text":"The system described here was defined by the PO group of the spring semester 2020. A more detailed description is available in their report. The prototype administration system was created to keep track of the prototypes' state and its context.","title":"Guidelines"},{"location":"Legacy/weekplanner/UI_Design/Prototypes/prototype_guidelines/#workspace-organisation","text":"The workspace in Adobe XD has been divided to categorize prototypes based on their state as shown by the image below: The figure below describes how the prototype transistions between the states: The table below gives a short description of each state: State Description Concept Prototype is in development (either because it has just been created or because changes were requested by the customers). Review Prototype is ready to be reviewed by customers. Approved Prototype has been reviewed and approved by customers. Implemented Prototype has been implemented. Rejected Prototype has been reviewed and rejected by customers.","title":"Workspace Organisation"},{"location":"Legacy/weekplanner/UI_Design/Prototypes/prototype_guidelines/#naming-convention","text":"The format of prototype names is defined as: yyyyx_ContextID_CompareID_ViewID_RelateID_GitID: <Short description> The table below describes each part of the name: Part Description yyyyx yyyy is the year and x is the semester ( f represents spring and e represents fall). Prototypes made during the spring semester of 2020 are named 2020f . ContextID An integer that is unique for the semester and represents the context of the prototypes. E.g. if four prototypes have been created to illustrate the feature of deleting pictograms, they should have the same ContextID . CompareID If some prototypes illustrate one way to implement a feature while others illustrate a different way to implement the same feature (they should share ContextID ) and they are being compared at the customer review, they should be given a letter ( A , B , C etc.). ViewID Either GUA or CIT depending whether the prototype illustrates guardians view or citizen's view. Should always be included. RelateID One or more integers (if there are more they should be seperated by hyphens) that describe how prototypes with the same yyyyx , ContextID , CompareID and ViewID are related. If the name is already unique without RelateID it should be exluded GitID The GitHub issue that the prototype is related to. Use abbreviations so WP273 = weekplanner#273 and PR12 = pictogram-reader#12 The naming convention is illustrated by the figure below: Press button indicates that a button, text field or similar on one prototype leads to another. E.g. pressing one button on 2020f_12_GUA_1-1_WP273 leads to 2020f_12_GUA_2_WP273 while pressing another leads to 2020f_12_GUA_1-2_WP273. But there is no button on 2020f_12_GUA_1-2_WP273 that leads to 2020f_12_GUA_2_WP273.","title":"Naming Convention"},{"location":"Legacy/weekplanner/Web_API/","text":"Overview \u00b6 This section gives an overview of the Web API. Definition of a Web API \u00b6 A Web API is an application programming interface located on a web-server. It manages the retrieval of requests as well as passing along an associated responses through the HTTP/HTTPS protocol. Such requests are usually in the format of JavaScript Object Notation (JSON) or Extensible Markup Language (XML). Purpose \u00b6 The Web API repository handles the server-side in the GIRAF project, and it is programmed in C# and uses .NET Core. More specifically the Web API is responsible for the handling of all communication coming from and to the api_client . If it is a valid request this leads to an endpoint and if not then an error response is returned. Most request would require the Web API to communicate with the database that stores all the data related to the GIRAF Project, which allows several of components to be communicating information to the common database. How is the Web API called \u00b6 The Web API is called through the api_client . The GIRAF project spans several applications, where each may be required to communicate with the Web API. To alleviate the requirement of implementing communication with the GIRAF backend in several applications, the Web API, a common api-client project has been initiated. Formally it is a Dart package that can be used to implement communication with the Web API in any front-end framework Flutter project e.g. the Weekplanner application. Architecture \u00b6 The architecture of the Web API is explained here . Backend \u00b6 The backend of the Web API consists of a database and the Entity Framework. A description of the database can be found here . Endpoints and Controllers \u00b6 Information about the endpoints and controllers in the Web API can be found here . Run the Web API Locally \u00b6 A guide on how to build and run the Web API locally can be found in the Setup Guide . Settings \u00b6 Information on the settings that can be set in the Web API can be found here . Testing the Web API \u00b6 An overview of the Web API testing projects can be found here .","title":"Overview"},{"location":"Legacy/weekplanner/Web_API/#overview","text":"This section gives an overview of the Web API.","title":"Overview"},{"location":"Legacy/weekplanner/Web_API/#definition-of-a-web-api","text":"A Web API is an application programming interface located on a web-server. It manages the retrieval of requests as well as passing along an associated responses through the HTTP/HTTPS protocol. Such requests are usually in the format of JavaScript Object Notation (JSON) or Extensible Markup Language (XML).","title":"Definition of a Web API"},{"location":"Legacy/weekplanner/Web_API/#purpose","text":"The Web API repository handles the server-side in the GIRAF project, and it is programmed in C# and uses .NET Core. More specifically the Web API is responsible for the handling of all communication coming from and to the api_client . If it is a valid request this leads to an endpoint and if not then an error response is returned. Most request would require the Web API to communicate with the database that stores all the data related to the GIRAF Project, which allows several of components to be communicating information to the common database.","title":"Purpose"},{"location":"Legacy/weekplanner/Web_API/#how-is-the-web-api-called","text":"The Web API is called through the api_client . The GIRAF project spans several applications, where each may be required to communicate with the Web API. To alleviate the requirement of implementing communication with the GIRAF backend in several applications, the Web API, a common api-client project has been initiated. Formally it is a Dart package that can be used to implement communication with the Web API in any front-end framework Flutter project e.g. the Weekplanner application.","title":"How is the Web API called"},{"location":"Legacy/weekplanner/Web_API/#architecture","text":"The architecture of the Web API is explained here .","title":"Architecture"},{"location":"Legacy/weekplanner/Web_API/#backend","text":"The backend of the Web API consists of a database and the Entity Framework. A description of the database can be found here .","title":"Backend"},{"location":"Legacy/weekplanner/Web_API/#endpoints-and-controllers","text":"Information about the endpoints and controllers in the Web API can be found here .","title":"Endpoints and Controllers"},{"location":"Legacy/weekplanner/Web_API/#run-the-web-api-locally","text":"A guide on how to build and run the Web API locally can be found in the Setup Guide .","title":"Run the Web API Locally"},{"location":"Legacy/weekplanner/Web_API/#settings","text":"Information on the settings that can be set in the Web API can be found here .","title":"Settings"},{"location":"Legacy/weekplanner/Web_API/#testing-the-web-api","text":"An overview of the Web API testing projects can be found here .","title":"Testing the Web API"},{"location":"Legacy/weekplanner/Web_API/Testing/","text":"Testing \u00b6 Testing gives a higher chance for the code to behave as intended along with ensuring that no regressions occur when further developing the code. Therefore each controller and repository has been tested to see if the methods work as they should. However, we cannot test the effect of attributes for methods, such as [Authorize] . We therefore restrict to test the functionality of the methods. Libraries Used \u00b6 xUnit \u00b6 xUnit is a library specifically designed for testing in .NET. In order to define a test we put [Fact] above the method. This way the xUnit test runner knows it's a testing method and automatically runs it when invoked. In order to run the tests you may run either of the two following CLI commands; dotnet test to run all tests dotnet test --filter DisplayName~department to for instance run tests with the name department in it. Moq \u00b6 Moq . is used for creating mockups of classes which the method being tested is dependent upon. It is especially well suited for mocking interfaces. It fares less well with classes, and if a class needs mocking it is often easier to create a new class that inherits from the given class and override its methods - luckily most of the built in ASP.NET classes contain only virtual methods. The mock data used for the unit tests is found in the large file \u22ef/Giraf.UnitTest/UnitTestExtensions.cs , and must be initialised in the start of every test. Example \u00b6 1 2 3 4 [HttpPost(\"user/{id}\")] public async Task < IActionResult > AddUser ( long ID , [ FromBody ] GirafUser usr ) { /* ... */ } We need to create tests for all possible outcomes; so before testing please consider all possible scenarios that your controller could encounter (examples are missing values in the DTO, the DTO being null or invalid Ids, usernames and so alike). When the test below tests what happens when a user attempts to add another user to a department that does not exist and what happens when removing a user from a department that does exist. Note that these are only examples and that these do not suffice to test the full functionality of the controller. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [Fact] public void Post_NewDepartmentValidDTO_Success () { var dc = initializeTest (); var name = \"dep1\" ; _testContext . MockUserManager . MockLoginAsUser ( _testContext . MockUsers [ ADMIN_DEP_ONE ]); var depDTO = new DepartmentDTO ( new Department () { Name = name }, new List < UserNameDTO > ()); var result = dc . Post ( depDTO ). Result ; Assert . True ( result . Success ); //Check data Assert . Equal ( name , result . Data . Name ); } The instance of the DepartmentController class used for testing is acquired by calling the InitializeTest() method. The context(which user is logged in) and the arguments are prepared. Then the controller method is executed by calling it directly, and the result returned is then checked.","title":"Testing"},{"location":"Legacy/weekplanner/Web_API/Testing/#testing","text":"Testing gives a higher chance for the code to behave as intended along with ensuring that no regressions occur when further developing the code. Therefore each controller and repository has been tested to see if the methods work as they should. However, we cannot test the effect of attributes for methods, such as [Authorize] . We therefore restrict to test the functionality of the methods.","title":"Testing"},{"location":"Legacy/weekplanner/Web_API/Testing/#libraries-used","text":"","title":"Libraries Used"},{"location":"Legacy/weekplanner/Web_API/Testing/#xunit","text":"xUnit is a library specifically designed for testing in .NET. In order to define a test we put [Fact] above the method. This way the xUnit test runner knows it's a testing method and automatically runs it when invoked. In order to run the tests you may run either of the two following CLI commands; dotnet test to run all tests dotnet test --filter DisplayName~department to for instance run tests with the name department in it.","title":"xUnit"},{"location":"Legacy/weekplanner/Web_API/Testing/#moq","text":"Moq . is used for creating mockups of classes which the method being tested is dependent upon. It is especially well suited for mocking interfaces. It fares less well with classes, and if a class needs mocking it is often easier to create a new class that inherits from the given class and override its methods - luckily most of the built in ASP.NET classes contain only virtual methods. The mock data used for the unit tests is found in the large file \u22ef/Giraf.UnitTest/UnitTestExtensions.cs , and must be initialised in the start of every test.","title":"Moq"},{"location":"Legacy/weekplanner/Web_API/Testing/#example","text":"1 2 3 4 [HttpPost(\"user/{id}\")] public async Task < IActionResult > AddUser ( long ID , [ FromBody ] GirafUser usr ) { /* ... */ } We need to create tests for all possible outcomes; so before testing please consider all possible scenarios that your controller could encounter (examples are missing values in the DTO, the DTO being null or invalid Ids, usernames and so alike). When the test below tests what happens when a user attempts to add another user to a department that does not exist and what happens when removing a user from a department that does exist. Note that these are only examples and that these do not suffice to test the full functionality of the controller. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [Fact] public void Post_NewDepartmentValidDTO_Success () { var dc = initializeTest (); var name = \"dep1\" ; _testContext . MockUserManager . MockLoginAsUser ( _testContext . MockUsers [ ADMIN_DEP_ONE ]); var depDTO = new DepartmentDTO ( new Department () { Name = name }, new List < UserNameDTO > ()); var result = dc . Post ( depDTO ). Result ; Assert . True ( result . Success ); //Check data Assert . Equal ( name , result . Data . Name ); } The instance of the DepartmentController class used for testing is acquired by calling the InitializeTest() method. The context(which user is logged in) and the arguments are prepared. Then the controller method is executed by calling it directly, and the result returned is then checked.","title":"Example"},{"location":"Legacy/weekplanner/Web_API/architecture/","text":"Architecture \u00b6 The layered structure presented is implemented with four different projects. GirafAPI GirafServices GirafRepositories GirafEntities Each of these projects are created to ensure separation of concerns, allowing easier identification of where to add future functionalities. The GirafAPI project handles all aspects of setting up a server for the API to receive and handle requests; this consists mainly of controllers and different settings for running and building the server. The GirafServices contains the business logic and any functionality a controller would need to handle a request, such as hashing. The GirafRepositories project is split into two concerns: one handles communication and setup of the database, and the other access and manipulation of the tables in the database. The last project, GirafEntities, is created to contain all entities and other objects of the API. The GirafRest solution setup is illustrated in the figure above. The projects are set up such that a layer references all inner layers , meaning the GirafAPI references all other projects, while GirafEntities references none. Using this reference structure enforces the communication of each layer only to go inward. This setup indicates that GirafEntities is the lowest layer and GirafAPI is the highest layer. There are 11 controllers in the web API which are responsible for handling all requests to the API. The controller uses the DTOs to define the JSON structure for the objects in the HTTP message body, which in turn are a subset of the Model classes. To extract information from the database the controllers uses functionality from services, which use repositories that utilize Entity Framework Core to execute queries to the database. The controllers also use services to make authentication checks and other common functionality. Making a Request \u00b6 This figure shows how a request from the user is translated into a JSON response given the current architecture of the backend. First, the WeekPlanner will use the generated client-API to make an HTTP request to the server. On the server the HTTP request is validated with the purpose of checking whether or not there is a matching URI and HTTP method; if no match is found the request is redirected to the ErrorController which sends an error response as JSON back to the WeekPlanner application to notify the user of this error. If there is a match, the relevant method is called, which uses the information in the HTTP body and header to produce a JSON response, which depending on the request might be a success or error. To see which method is called, consider the request GET http://web.giraf.cs.aau.dk:5000/v1/Pictogram/2/image . All controller classes are in the folder \u22ef/GirafRest/Controllers/ , and in this case the request is route to the PictogramController , because /v1/Pictogram matches its Route attribute: 1 2 3 [Route(\"v1/[controller] \")] [Authorize] public class PictogramController : Controller {...} This controller contains the method 1 2 [HttpGet(\"{id}/image\")] public async Task < Response < byte [] >> ReadPictogramImage ( long id ) {...} whose attribute matches the GET verb and the last part of the URI, /2/image . The Database \u00b6 As the figure illustrates, we use the ASP.NET Identity membership system to manage Users . Each user with the citizen role, has a relation to a Settings entity which defines configurations specific to a User which can be set in the WeekPlanner application. Furthermore a User has a reference to private Pictograms through the UserResources which is the pivot table for describing a many-to-many relationship between Users and Pictograms . A User is also part of a Department and has a list of weekschedules which is modelled with the Weeks entity. A Week has a reference to up to seven Weekdays where each day contains an Activity which has a reference to a Pictogram , an order, and a state. An Activity is thus a Pictogram that is related to a specific WeekDay in a Week and has an order which indicates the index of the Activity in the Week and a state which for instance can take the values \"checked\" and \"current\". A WeekTemplate is a generic form of Week , but unlike Week , the entity does not have a year and week-number and instead of belonging to a User it belongs to a Department . Departments like Users can also own Pictograms which is modelled through the DepartmentResources entity.","title":"Architecture"},{"location":"Legacy/weekplanner/Web_API/architecture/#architecture","text":"The layered structure presented is implemented with four different projects. GirafAPI GirafServices GirafRepositories GirafEntities Each of these projects are created to ensure separation of concerns, allowing easier identification of where to add future functionalities. The GirafAPI project handles all aspects of setting up a server for the API to receive and handle requests; this consists mainly of controllers and different settings for running and building the server. The GirafServices contains the business logic and any functionality a controller would need to handle a request, such as hashing. The GirafRepositories project is split into two concerns: one handles communication and setup of the database, and the other access and manipulation of the tables in the database. The last project, GirafEntities, is created to contain all entities and other objects of the API. The GirafRest solution setup is illustrated in the figure above. The projects are set up such that a layer references all inner layers , meaning the GirafAPI references all other projects, while GirafEntities references none. Using this reference structure enforces the communication of each layer only to go inward. This setup indicates that GirafEntities is the lowest layer and GirafAPI is the highest layer. There are 11 controllers in the web API which are responsible for handling all requests to the API. The controller uses the DTOs to define the JSON structure for the objects in the HTTP message body, which in turn are a subset of the Model classes. To extract information from the database the controllers uses functionality from services, which use repositories that utilize Entity Framework Core to execute queries to the database. The controllers also use services to make authentication checks and other common functionality.","title":"Architecture"},{"location":"Legacy/weekplanner/Web_API/architecture/#making-a-request","text":"This figure shows how a request from the user is translated into a JSON response given the current architecture of the backend. First, the WeekPlanner will use the generated client-API to make an HTTP request to the server. On the server the HTTP request is validated with the purpose of checking whether or not there is a matching URI and HTTP method; if no match is found the request is redirected to the ErrorController which sends an error response as JSON back to the WeekPlanner application to notify the user of this error. If there is a match, the relevant method is called, which uses the information in the HTTP body and header to produce a JSON response, which depending on the request might be a success or error. To see which method is called, consider the request GET http://web.giraf.cs.aau.dk:5000/v1/Pictogram/2/image . All controller classes are in the folder \u22ef/GirafRest/Controllers/ , and in this case the request is route to the PictogramController , because /v1/Pictogram matches its Route attribute: 1 2 3 [Route(\"v1/[controller] \")] [Authorize] public class PictogramController : Controller {...} This controller contains the method 1 2 [HttpGet(\"{id}/image\")] public async Task < Response < byte [] >> ReadPictogramImage ( long id ) {...} whose attribute matches the GET verb and the last part of the URI, /2/image .","title":"Making a Request"},{"location":"Legacy/weekplanner/Web_API/architecture/#the-database","text":"As the figure illustrates, we use the ASP.NET Identity membership system to manage Users . Each user with the citizen role, has a relation to a Settings entity which defines configurations specific to a User which can be set in the WeekPlanner application. Furthermore a User has a reference to private Pictograms through the UserResources which is the pivot table for describing a many-to-many relationship between Users and Pictograms . A User is also part of a Department and has a list of weekschedules which is modelled with the Weeks entity. A Week has a reference to up to seven Weekdays where each day contains an Activity which has a reference to a Pictogram , an order, and a state. An Activity is thus a Pictogram that is related to a specific WeekDay in a Week and has an order which indicates the index of the Activity in the Week and a state which for instance can take the values \"checked\" and \"current\". A WeekTemplate is a generic form of Week , but unlike Week , the entity does not have a year and week-number and instead of belonging to a User it belongs to a Department . Departments like Users can also own Pictograms which is modelled through the DepartmentResources entity.","title":"The Database"},{"location":"Legacy/weekplanner/Web_API/settings/","text":"Settings \u00b6 The backend must have a configuration enviroment set up in order to be able to run. Connection String \u00b6 A ConnectionString is needed, which tells Entity Core Framework how to connect to a database. It must be on the form: 1 2 3 \"ConnectionStrings\" : { \"DefaultConnection\" : \"server=<db-host>;port=<db-port>;userid=<db-user>;password=<db-password>;database=<db-db>;Allow User Variables=True\" } Fill in the following: Field Description Default <db-host> DB server's address. Set this to localhost for local development <db-port> The port to use for communication with the server. 3306, unless you specified otherwise during installation or in your Docker container. <db-user> Name of the DB server's root user, or another user with the right privileges. <db-password> Password of same. <db-db> Name of the schema that will be created. Set this to giraf if you have no reason to do otherwise. JWT information \u00b6 This information is required for creating the JWT keys which will be used for authorisation on the server. It must be on the form: 1 2 3 4 5 \"Jwt\" : { \"JwtKey\" : \"<jwt-key>\" , \"JwtIssuer\" : \"<jwt-issuer>\" , \"JwtExpireDays\" : 30 } Fill in the following: Field Description <jwt-key> This is the skeleton key used to generate other keys. Secrecy only important for production. Use any string longer than 40 characters, e.g. this text field. <jwt-issuer> Name that will be displayed as the issuer of the key. Avoid using anything official-sounding, but perhaps write in a joke.","title":"Settings"},{"location":"Legacy/weekplanner/Web_API/settings/#settings","text":"The backend must have a configuration enviroment set up in order to be able to run.","title":"Settings"},{"location":"Legacy/weekplanner/Web_API/settings/#connection-string","text":"A ConnectionString is needed, which tells Entity Core Framework how to connect to a database. It must be on the form: 1 2 3 \"ConnectionStrings\" : { \"DefaultConnection\" : \"server=<db-host>;port=<db-port>;userid=<db-user>;password=<db-password>;database=<db-db>;Allow User Variables=True\" } Fill in the following: Field Description Default <db-host> DB server's address. Set this to localhost for local development <db-port> The port to use for communication with the server. 3306, unless you specified otherwise during installation or in your Docker container. <db-user> Name of the DB server's root user, or another user with the right privileges. <db-password> Password of same. <db-db> Name of the schema that will be created. Set this to giraf if you have no reason to do otherwise.","title":"Connection String"},{"location":"Legacy/weekplanner/Web_API/settings/#jwt-information","text":"This information is required for creating the JWT keys which will be used for authorisation on the server. It must be on the form: 1 2 3 4 5 \"Jwt\" : { \"JwtKey\" : \"<jwt-key>\" , \"JwtIssuer\" : \"<jwt-issuer>\" , \"JwtExpireDays\" : 30 } Fill in the following: Field Description <jwt-key> This is the skeleton key used to generate other keys. Secrecy only important for production. Use any string longer than 40 characters, e.g. this text field. <jwt-issuer> Name that will be displayed as the issuer of the key. Avoid using anything official-sounding, but perhaps write in a joke.","title":"JWT information"},{"location":"Legacy/weekplanner/Web_API/Backend/database/","text":"Database \u00b6 MySQL \u00b6 We use MySQL on the server, because it works with Entity Framework and supports all migrations, which SQLite for instance does not. Furthermore MySQL is cross-platform and can be set up on both Windows, Mac and Linux distributions. Migrations (Only for developers of the API) \u00b6 If changes has been made to the model classes of the web-api then a new migration should be added to be able to update the database with new entities without losing data: In a shell, navigate to: .../web-api/GirafAPI dotnet ef migrations add NameOfMigrationDescribingTheChange dotnet ef database update If an exception is thrown then adjust the migration Up method in the file Migrations/...NameOfMigrationDescribingTheChange.cs to include the change of the model without triggering any MySQL exceptions. It may be good to inspect the file in any case, to see that it will function as expected. When the database is updated confirm that the migration NameOfMigrationDescribingTheChange is added to the table __efmigrationshistory in the giraf database. Now check that the Down method in the migration is also working properly. Determine the name of the last migration before yours, looking at the date and time prefixes in ls Migrations/MySQL . If it is 20180409123114_PreviousMigration.cs , then you must run dotnet ef database update PreviousMigration . If an exception is thrown then adjust the migration Down method in the file Migrations/...NameOfMigrationDescribingTheChange.cs to include the change of the model without triggering any MySQL exceptions. When the database is updated to the previous migration confirm that NameOfMigrationDescribingTheChange is no longer in the table __efmigrationshistory in the giraf database. Finally update the database to the newly added migration again using dotnet ef database update . If an exception is thrown adjust the Up method of the migration again to fix the issue. Make sure that the entire web-api repository has no syntax errors before migrating, as this can also cause the migrations to fail. Many-to-Many \u00b6 ASP.NET Core does not support many-to-many relations at the current release (v. 2), however a workaround was developed to obtain the same results. The workaround is inspired by a similar approach shown in the DBS course. The relations created can be found in the GirafEntities project. An example is the relation called DepartmentResource in the user folder, that maps between departments and resources. Many-to-Many relations in the Entity project have a suffix with either Resource or Relation. The relations are built in the GirafDbContext , but an example of creating such a relation is shown below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 builder . Entity < DepartmentResource > () //States that one department is involved in each DepartmentResourceRelation . HasOne ( dr => dr . Other ) //And that this one department may own several DepartmentResources . WithMany ( d => d . Resources ) //And that the key of the department in the relation is 'OtherKey' . HasForeignKey ( dr => dr . OtherKey ); //Configures the relation from Resource to DepartmentResource builder . Entity < DepartmentResource > () //States that only one resource is involved in this relation . HasOne ( dr => dr . Resource ) //And that each resource may take part in many DepartmentResource relations . WithMany ( r => r . Departments ) //And that the key of the department in the relation is 'ResoruceKey' . HasForeignKey ( dr => dr . ResourceKey ); Using this syntax will create the defined relation.","title":"Database"},{"location":"Legacy/weekplanner/Web_API/Backend/database/#database","text":"","title":"Database"},{"location":"Legacy/weekplanner/Web_API/Backend/database/#mysql","text":"We use MySQL on the server, because it works with Entity Framework and supports all migrations, which SQLite for instance does not. Furthermore MySQL is cross-platform and can be set up on both Windows, Mac and Linux distributions.","title":"MySQL"},{"location":"Legacy/weekplanner/Web_API/Backend/database/#migrations-only-for-developers-of-the-api","text":"If changes has been made to the model classes of the web-api then a new migration should be added to be able to update the database with new entities without losing data: In a shell, navigate to: .../web-api/GirafAPI dotnet ef migrations add NameOfMigrationDescribingTheChange dotnet ef database update If an exception is thrown then adjust the migration Up method in the file Migrations/...NameOfMigrationDescribingTheChange.cs to include the change of the model without triggering any MySQL exceptions. It may be good to inspect the file in any case, to see that it will function as expected. When the database is updated confirm that the migration NameOfMigrationDescribingTheChange is added to the table __efmigrationshistory in the giraf database. Now check that the Down method in the migration is also working properly. Determine the name of the last migration before yours, looking at the date and time prefixes in ls Migrations/MySQL . If it is 20180409123114_PreviousMigration.cs , then you must run dotnet ef database update PreviousMigration . If an exception is thrown then adjust the migration Down method in the file Migrations/...NameOfMigrationDescribingTheChange.cs to include the change of the model without triggering any MySQL exceptions. When the database is updated to the previous migration confirm that NameOfMigrationDescribingTheChange is no longer in the table __efmigrationshistory in the giraf database. Finally update the database to the newly added migration again using dotnet ef database update . If an exception is thrown adjust the Up method of the migration again to fix the issue. Make sure that the entire web-api repository has no syntax errors before migrating, as this can also cause the migrations to fail.","title":"Migrations (Only for developers of the API)"},{"location":"Legacy/weekplanner/Web_API/Backend/database/#many-to-many","text":"ASP.NET Core does not support many-to-many relations at the current release (v. 2), however a workaround was developed to obtain the same results. The workaround is inspired by a similar approach shown in the DBS course. The relations created can be found in the GirafEntities project. An example is the relation called DepartmentResource in the user folder, that maps between departments and resources. Many-to-Many relations in the Entity project have a suffix with either Resource or Relation. The relations are built in the GirafDbContext , but an example of creating such a relation is shown below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 builder . Entity < DepartmentResource > () //States that one department is involved in each DepartmentResourceRelation . HasOne ( dr => dr . Other ) //And that this one department may own several DepartmentResources . WithMany ( d => d . Resources ) //And that the key of the department in the relation is 'OtherKey' . HasForeignKey ( dr => dr . OtherKey ); //Configures the relation from Resource to DepartmentResource builder . Entity < DepartmentResource > () //States that only one resource is involved in this relation . HasOne ( dr => dr . Resource ) //And that each resource may take part in many DepartmentResource relations . WithMany ( r => r . Departments ) //And that the key of the department in the relation is 'ResoruceKey' . HasForeignKey ( dr => dr . ResourceKey ); Using this syntax will create the defined relation.","title":"Many-to-Many"},{"location":"Legacy/weekplanner/Web_API/Backend/entity_framework/","text":"Entity Framework Core \u00b6 EF Core (Entity Framework Core) is a database framework for .NET. It allows using LINQ (Language Integrated Query) statements to query the database, adding a layer of abstraction from the SQL queries to the database. The link between the REST API and the database is the GirafDbContext class, which declares a number of DbSet s. Each DbSet represents a table in the database, and thus a LINQ on any of these DbSet s is the same as a SQL query with the given table in the FROM-clause (see example below): 1 context . Pictograms . Where ( p => p . Id == 10 ). FirstOrDefaultAsync (); Is equivalent with: 1 2 3 4 SELECT * FROM Pictogram WHERE Id == 10 LIMIT 1 ; Includes \u00b6 When EF Core fetches data for you in the database, it is always as lazy as possible and will not include related data per default. If we consider the same User -class example as presented before, EF Core will not load the user's department when only queried for a user, and the field will thus be null. In order to cause EF to load the data, you must use an Include on the LINQ. In this case we would have to write: 1 context . Users . Where ( u => u . Id == 1 ). Include ( u => u . Department ). FirstOrDefaultAsync (); In order to find the user with id 1 and his department. Object Relational Mapping \u00b6 In order to define model-classes EF Core uses Object Relational Mapping (ORM). Entity has a set of data annotations, that allows you to define constraints and database relevant information about each property on the model-classes. The model-classes of the current system only uses this sparsely and thus the guide will not be as in-depth as you might have desired. In order to find more information on ORM, please visit the subsections of MSDN's ( Creating a Model Overview ) In order to create a simple, but sufficient model-class, you may use the [Key] and [ForeignKey] annotations along with the [DatabaseGenerated] -attribute and optionally the [Column] and [Required] -annotations. The following is a simple User class, that demonstrates the annotations: 1 2 3 4 5 6 7 8 9 10 11 12 13 class User { [Key] [DatabaseGenerated(DatabaseGeneratedOptions.Identity)] public long Id { get ; set ; } [Required] public string Username { get ; set ; } [Column(\"DepKey\")] public long DepartmentId { get ; set ; } [ForeignKey(\"DepartmentId\")] public Department Department { get ; set ; } } The Id field in this example is marked with two annotations, which means that the property is the primary key of the relation and that the database should automatically assign ids to users respectively. The Username property is only marked as Required , which means that a Username must always be present on Users . The Required annotation also should affect the serialization and deserialization of user-objects, however we suspect that there might be a bug in the current version, as this is not the case. The DepartmentId and Department properties demonstrate how to create relationship from a user to a department (note that the department may still have many users). The string in the ForeignKey attribute must be exactly equal to the property that references the primary key of the other relation, in this case [ForeignKey(\"DepartmentId\")] . NOTE: When creating a many relation, i.e. an ICollection-field on a class, it should always be virtual, as this allows us to use Moq to mock each entity.","title":"Entity Framework Core"},{"location":"Legacy/weekplanner/Web_API/Backend/entity_framework/#entity-framework-core","text":"EF Core (Entity Framework Core) is a database framework for .NET. It allows using LINQ (Language Integrated Query) statements to query the database, adding a layer of abstraction from the SQL queries to the database. The link between the REST API and the database is the GirafDbContext class, which declares a number of DbSet s. Each DbSet represents a table in the database, and thus a LINQ on any of these DbSet s is the same as a SQL query with the given table in the FROM-clause (see example below): 1 context . Pictograms . Where ( p => p . Id == 10 ). FirstOrDefaultAsync (); Is equivalent with: 1 2 3 4 SELECT * FROM Pictogram WHERE Id == 10 LIMIT 1 ;","title":"Entity Framework Core"},{"location":"Legacy/weekplanner/Web_API/Backend/entity_framework/#includes","text":"When EF Core fetches data for you in the database, it is always as lazy as possible and will not include related data per default. If we consider the same User -class example as presented before, EF Core will not load the user's department when only queried for a user, and the field will thus be null. In order to cause EF to load the data, you must use an Include on the LINQ. In this case we would have to write: 1 context . Users . Where ( u => u . Id == 1 ). Include ( u => u . Department ). FirstOrDefaultAsync (); In order to find the user with id 1 and his department.","title":"Includes"},{"location":"Legacy/weekplanner/Web_API/Backend/entity_framework/#object-relational-mapping","text":"In order to define model-classes EF Core uses Object Relational Mapping (ORM). Entity has a set of data annotations, that allows you to define constraints and database relevant information about each property on the model-classes. The model-classes of the current system only uses this sparsely and thus the guide will not be as in-depth as you might have desired. In order to find more information on ORM, please visit the subsections of MSDN's ( Creating a Model Overview ) In order to create a simple, but sufficient model-class, you may use the [Key] and [ForeignKey] annotations along with the [DatabaseGenerated] -attribute and optionally the [Column] and [Required] -annotations. The following is a simple User class, that demonstrates the annotations: 1 2 3 4 5 6 7 8 9 10 11 12 13 class User { [Key] [DatabaseGenerated(DatabaseGeneratedOptions.Identity)] public long Id { get ; set ; } [Required] public string Username { get ; set ; } [Column(\"DepKey\")] public long DepartmentId { get ; set ; } [ForeignKey(\"DepartmentId\")] public Department Department { get ; set ; } } The Id field in this example is marked with two annotations, which means that the property is the primary key of the relation and that the database should automatically assign ids to users respectively. The Username property is only marked as Required , which means that a Username must always be present on Users . The Required annotation also should affect the serialization and deserialization of user-objects, however we suspect that there might be a bug in the current version, as this is not the case. The DepartmentId and Department properties demonstrate how to create relationship from a user to a department (note that the department may still have many users). The string in the ForeignKey attribute must be exactly equal to the property that references the primary key of the other relation, in this case [ForeignKey(\"DepartmentId\")] . NOTE: When creating a many relation, i.e. an ICollection-field on a class, it should always be virtual, as this allows us to use Moq to mock each entity.","title":"Object Relational Mapping"},{"location":"Legacy/weekplanner/Web_API/Backend/models/","text":"Models \u00b6 The models in the Web API is a collection of classes, which is used throughout the application. A list of these classes can be found below. Model name Usage AccessLevel Used to define access level for pictograms. It has three values, PUBLIC for pictograms that are accessible to all users, PROTECTED for pictograms that are only accessible to users in a given department and PRIVATE for pictograms that are accessible only for a given user. Department Used to define a department containing users and additional resources the users can access. GirafRole Used to specify a role, contains role/policy string definitions for authorization. Currently (either) Citizen , Guardian , Trustee , Department and SuperUser . GirafUser Used to define relevant data for a user. Pictogram Used for storing the image of a pictogram as well as meta-information such as its name. Setting Used to specify the settings particular to a user. This includes number of days shown at a time, and the colour theme. WeekdayColor Used to represent a field in settings that specifies the background colour to use when displaying a particular week day. Weekday Used to represent a citizen's day with a number of activities each represented as a pictogram. WeekBase The base class from which Week and WeekTemplate inherits. Contains an ID, a name, a thumbnail and up to 7 days. Week Used to define a users schedule for a week at a certain year and week-number. WeekTemplate Used to define a generic template that can be used as a base when a new week is being planned. Activity Used to represent an activity on a weekday, contains pictograms and timers - can be a choiceboard. DepartmentResource Defines a relationship between a Deparments and a Pictograms. GuardianRelation Defines a relationship between Guardians and Citizens. UserResource Defines a relationship between Users and Pictograms. AlternateName Represents an alternate name for a Pictogram. PictogramRelation Defines a relationship between Activities and Pictograms. Timer Represents a timer, with start time, progress and length.","title":"Models"},{"location":"Legacy/weekplanner/Web_API/Backend/models/#models","text":"The models in the Web API is a collection of classes, which is used throughout the application. A list of these classes can be found below. Model name Usage AccessLevel Used to define access level for pictograms. It has three values, PUBLIC for pictograms that are accessible to all users, PROTECTED for pictograms that are only accessible to users in a given department and PRIVATE for pictograms that are accessible only for a given user. Department Used to define a department containing users and additional resources the users can access. GirafRole Used to specify a role, contains role/policy string definitions for authorization. Currently (either) Citizen , Guardian , Trustee , Department and SuperUser . GirafUser Used to define relevant data for a user. Pictogram Used for storing the image of a pictogram as well as meta-information such as its name. Setting Used to specify the settings particular to a user. This includes number of days shown at a time, and the colour theme. WeekdayColor Used to represent a field in settings that specifies the background colour to use when displaying a particular week day. Weekday Used to represent a citizen's day with a number of activities each represented as a pictogram. WeekBase The base class from which Week and WeekTemplate inherits. Contains an ID, a name, a thumbnail and up to 7 days. Week Used to define a users schedule for a week at a certain year and week-number. WeekTemplate Used to define a generic template that can be used as a base when a new week is being planned. Activity Used to represent an activity on a weekday, contains pictograms and timers - can be a choiceboard. DepartmentResource Defines a relationship between a Deparments and a Pictograms. GuardianRelation Defines a relationship between Guardians and Citizens. UserResource Defines a relationship between Users and Pictograms. AlternateName Represents an alternate name for a Pictogram. PictogramRelation Defines a relationship between Activities and Pictograms. Timer Represents a timer, with start time, progress and length.","title":"Models"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/","text":"Overview \u00b6 This section gives an overview and guidelines for the endpoints and controllers in the Web API. Authorization in Endpoints \u00b6 A description on the authorization system used for the endpoints can be found here . Endpoint and Controller Guidelines \u00b6 Guidelines to the endpoints and controllers can be found here . Controller Descriptions \u00b6 Descriptions of the controllers in the Web API can be found here . Endpoint Descriptions \u00b6 Descriptions of the endpoints for each controller can be found here . Data Transfer Objects (DTO) \u00b6 Data Transfer Objects (DTOs) are used to communicate data in the controllers both as input and output for the methods, while containing only the essential from each model. All models have a corresponding DTO with the exception of AccessLevel and GirafRole, as they are not used directly in any controllers currently. In addition to those there are also six DTOs related to account/user management. NOTE: An object should never leave the server unless it has been packed in a corresponding DTO. NOTE: All DTOs must have parameterless constructors, because it is required by Newtonsoft when deserializing incomming requests. Swagger \u00b6 An overview of the endpoints and controllers used by the GIRAF project can be found here . More information on how Swagger is used in the Web API can be found here .","title":"Overview"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/#overview","text":"This section gives an overview and guidelines for the endpoints and controllers in the Web API.","title":"Overview"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/#authorization-in-endpoints","text":"A description on the authorization system used for the endpoints can be found here .","title":"Authorization in Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/#endpoint-and-controller-guidelines","text":"Guidelines to the endpoints and controllers can be found here .","title":"Endpoint and Controller Guidelines"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/#controller-descriptions","text":"Descriptions of the controllers in the Web API can be found here .","title":"Controller Descriptions"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/#endpoint-descriptions","text":"Descriptions of the endpoints for each controller can be found here .","title":"Endpoint Descriptions"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/#data-transfer-objects-dto","text":"Data Transfer Objects (DTOs) are used to communicate data in the controllers both as input and output for the methods, while containing only the essential from each model. All models have a corresponding DTO with the exception of AccessLevel and GirafRole, as they are not used directly in any controllers currently. In addition to those there are also six DTOs related to account/user management. NOTE: An object should never leave the server unless it has been packed in a corresponding DTO. NOTE: All DTOs must have parameterless constructors, because it is required by Newtonsoft when deserializing incomming requests.","title":"Data Transfer Objects (DTO)"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/#swagger","text":"An overview of the endpoints and controllers used by the GIRAF project can be found here . More information on how Swagger is used in the Web API can be found here .","title":"Swagger"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/controller_descriptions/","text":"Description of Controllers \u00b6 This section gives a description for each of the controllers in the Web API. AccountController \u00b6 The account controller contains all the endpoints related to authenticating users e.g. log in, log out, register and reset password. Endpoints can be found here ActivityController \u00b6 The activity controller contains CRUD endpoints related to activities. It has been created to avoid having to update the whole weekplan when changes are made to a single activity. Endpoints can be found here UserController \u00b6 While AccountController concerns itself with authentication, this controller handles CRUD actions and other actions which can be taken on a user, as well as the user's data. Endpoints can be found here DepartmentController \u00b6 This controller contains CRUD endpoints for managing a department, including getting a list of citizens for that department. Endpoints can be found here PictogramController \u00b6 This controller contains CRUD endpoints for managing pictograms, including their access levels. Endpoints can be found here WeekController \u00b6 In the WeekPlanner application, a citizen has different week schedules which contains information about all of the citizen's activities for a given week. This controller contains the CRUD methods for that week entity. Endpoints can be found here WeekTemplateController \u00b6 Week templates may be used by guardians in the WeekPlanner app when creating a new week for a citizen. This controller contains CRUD endpoints for managing week templates. Endpoints can be found here StatusController \u00b6 The status controller purely used by the server groups to check the status of the API, such as whether the API is running and if it has access to the database. Endpoints can be found here ErrorController \u00b6 By convention the ErrorController is linked to whenever there is an illegal request such as a 404 request. The controller will always send back an ErrorResponse . Endpoints can be found here","title":"Description of Controllers"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/controller_descriptions/#description-of-controllers","text":"This section gives a description for each of the controllers in the Web API.","title":"Description of Controllers"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/controller_descriptions/#accountcontroller","text":"The account controller contains all the endpoints related to authenticating users e.g. log in, log out, register and reset password. Endpoints can be found here","title":"AccountController"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/controller_descriptions/#activitycontroller","text":"The activity controller contains CRUD endpoints related to activities. It has been created to avoid having to update the whole weekplan when changes are made to a single activity. Endpoints can be found here","title":"ActivityController"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/controller_descriptions/#usercontroller","text":"While AccountController concerns itself with authentication, this controller handles CRUD actions and other actions which can be taken on a user, as well as the user's data. Endpoints can be found here","title":"UserController"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/controller_descriptions/#departmentcontroller","text":"This controller contains CRUD endpoints for managing a department, including getting a list of citizens for that department. Endpoints can be found here","title":"DepartmentController"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/controller_descriptions/#pictogramcontroller","text":"This controller contains CRUD endpoints for managing pictograms, including their access levels. Endpoints can be found here","title":"PictogramController"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/controller_descriptions/#weekcontroller","text":"In the WeekPlanner application, a citizen has different week schedules which contains information about all of the citizen's activities for a given week. This controller contains the CRUD methods for that week entity. Endpoints can be found here","title":"WeekController"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/controller_descriptions/#weektemplatecontroller","text":"Week templates may be used by guardians in the WeekPlanner app when creating a new week for a citizen. This controller contains CRUD endpoints for managing week templates. Endpoints can be found here","title":"WeekTemplateController"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/controller_descriptions/#statuscontroller","text":"The status controller purely used by the server groups to check the status of the API, such as whether the API is running and if it has access to the database. Endpoints can be found here","title":"StatusController"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/controller_descriptions/#errorcontroller","text":"By convention the ErrorController is linked to whenever there is an illegal request such as a 404 request. The controller will always send back an ErrorResponse . Endpoints can be found here","title":"ErrorController"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/guidelines/","text":"Guidelines \u00b6 This tutorial is created to help developers understand and create new controllers and endpoints in the Giraf project. An endpoint is basically the end of a communication channel. It is this point the API uses to access resources needed to fulfill a request. Any time the API wants to access the database, it happens through endpoints. Each endpoint has its own URL that the API uses when it wants a specific action. A controller is a collection of endpoints. Each controller is responsible for a limited area of the server/API and contains the endpoints related to said area. All communication is done through 'requests' and 'responses'. Controller \u00b6 Naming Convention \u00b6 A controller should always be named after the resource it controls, i.e. UserController if the controller is responsible for handling requests for user data. The reason for this convention is that the client-side of the REST api extracts the name of the class that the request involves and issues a request to host/class_name . ASP.NET has a neat way of enforcing this name convention, which is annotating the controller classes with [Route(\"[controller]\")] as it automatically fetches the resource-type of the controller, i.e. User in UserController. Creating New Controllers \u00b6 When creating a new controller, you start by creating a new class deriving from the Controller class. The Controller class provides useful responses that are used when sending responses to the API. Here is an example of a controller: 1 2 3 4 5 6 [Authorize] [Route(\"v1/[controller] \")] public class ExampleController : Controller { //Endpoints goes here } Before the class declaration are two attributes [Authorize] and [Route(\"v1/[controller]\")] . The [Authorize] attribute dictates who has access to the controller and the endpoints. If there is no [Authorize] , anyone can access the controller and its endpoints, while if there is a [Authorize] you have to be logged in to access the controller and the endpoints. Additionally you can define what roles the logged in user needs to get access like this [Authorize(Roles = GirafRole.Guardian)] or if you want more roles to have access [Authorize(Roles = GirafRole.SuperUser + \",\" + GirafRole.Department + \",\" + GirafRole.Guardian)] . Typically, you would only use [Authorize] for controllers as specifying what roles have access to the controller, like with [Authorize(Roles = GirafRole.SuperUser)] , locks all endpoints to the same access level, meaning you can't make some endpoints accessible only to a guardian while another is accessible only to a superuser. The [Route(\"v1/[controller]\")] attribute modifies the URL for endpoints to include the pre-fix define in the quotation marks. The [controller] uses the name of the controller excluding Controller in this case it would be /v1/Example . Endpoint \u00b6 Endpoints are essentially methods in a controller. Like the controller, an endpoint has some special attributes. Here is an example of an endpoint 1 2 3 4 5 6 7 [HttpPost(\"ExampleEndpoint\")] [Authorize] [ProducesResponseType(StatusCodes.Status200OK)] public Task < ActionResult > ExampleEndpoint () { //Does stuff } The [HttpPost(\"ExampleEndpoint\")] does two thing, it defines the endpoint's URL with \"/ExampleEndpoint\" and describes what type of operation this endpoint executes. In this case, if the endpoint was a part of ExampleController from before, the full URL would be /v1/Example/ExampleEndpoint . If you have an endpoint where you do not want to use the URL defined by the controller, you can start with a / to ignore the URL defined in [Route] , using [HttpPost(\"/NoRoute/ExampleEndpoint\")] would give the URL /NoRoute/ExampleEndpoint . [HttpPost] describes what type of operation the endpoint does. There are 4 types of operation, [HttpPost] , [HttpGet] , [HttpPut] , [HttpDelete] . These operations follows CRUD . For more information on how the api_client and the web-api communicates see Backend Architecture . The [Authorize] option works in the same way as the [Authorize] in controllers. The main difference is that this only affects this single endpoint. So if the controller is set to [Authorize] , but an endpoint needs access from non-authorized users, this option is given the value [AllowAnonymous] as seen in the example below. This overwrites the [Authorize] option provided by the controller. If the opposite is the case, and the endpoint needs restricted authorization, the option can be set to [Authorize] or something more specific like [Authorize(Roles = GirafRole.SuperUser + \",\" + GirafRole.Department + \",\" + GirafRole.Guardian)] . 1 2 3 4 5 6 7 [HttpPost(\"ExampleEndpoint\")] [AllowAnonymous] [ProducesResponseType(StatusCodes.Status200OK)] public Task < ActionResult > ExampleEndpoint () { //Does stuff } The [ProducesResponseType(StatusCodes.Status200OK)] is a response the endpoint can give. This particular response is for success. Typically you would like more than this to accommodate multiple outcomes. For instance when a user is not found like in the example below. For a more detailed list of responses please see this link . Bellow is an example of how to use [ProducesResponseType()] . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 [HttpGet(\"{id}\")] [ProducesResponseType(StatusCodes.Status200OK)] [ProducesResponseType(StatusCodes.Status404NotFound)] public async Task < ActionResult > GetById ( int id ) { // Code for finding a user if ( noIdMatch ) { return NotFound ( new ErrorResponse ( ErrorCode . UserNotFound , \"User not found\" )); } return Ok ( new SuccessResponse < Object > ( result )); } You can also use StatusCode(StatusCodes.Status200OK, objectToReturn) for giving status codes to a response. The first attribute is a static class containing http status codes and the second attribute is the object to return.","title":"Guidelines"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/guidelines/#guidelines","text":"This tutorial is created to help developers understand and create new controllers and endpoints in the Giraf project. An endpoint is basically the end of a communication channel. It is this point the API uses to access resources needed to fulfill a request. Any time the API wants to access the database, it happens through endpoints. Each endpoint has its own URL that the API uses when it wants a specific action. A controller is a collection of endpoints. Each controller is responsible for a limited area of the server/API and contains the endpoints related to said area. All communication is done through 'requests' and 'responses'.","title":"Guidelines"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/guidelines/#controller","text":"","title":"Controller"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/guidelines/#naming-convention","text":"A controller should always be named after the resource it controls, i.e. UserController if the controller is responsible for handling requests for user data. The reason for this convention is that the client-side of the REST api extracts the name of the class that the request involves and issues a request to host/class_name . ASP.NET has a neat way of enforcing this name convention, which is annotating the controller classes with [Route(\"[controller]\")] as it automatically fetches the resource-type of the controller, i.e. User in UserController.","title":"Naming Convention"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/guidelines/#creating-new-controllers","text":"When creating a new controller, you start by creating a new class deriving from the Controller class. The Controller class provides useful responses that are used when sending responses to the API. Here is an example of a controller: 1 2 3 4 5 6 [Authorize] [Route(\"v1/[controller] \")] public class ExampleController : Controller { //Endpoints goes here } Before the class declaration are two attributes [Authorize] and [Route(\"v1/[controller]\")] . The [Authorize] attribute dictates who has access to the controller and the endpoints. If there is no [Authorize] , anyone can access the controller and its endpoints, while if there is a [Authorize] you have to be logged in to access the controller and the endpoints. Additionally you can define what roles the logged in user needs to get access like this [Authorize(Roles = GirafRole.Guardian)] or if you want more roles to have access [Authorize(Roles = GirafRole.SuperUser + \",\" + GirafRole.Department + \",\" + GirafRole.Guardian)] . Typically, you would only use [Authorize] for controllers as specifying what roles have access to the controller, like with [Authorize(Roles = GirafRole.SuperUser)] , locks all endpoints to the same access level, meaning you can't make some endpoints accessible only to a guardian while another is accessible only to a superuser. The [Route(\"v1/[controller]\")] attribute modifies the URL for endpoints to include the pre-fix define in the quotation marks. The [controller] uses the name of the controller excluding Controller in this case it would be /v1/Example .","title":"Creating New Controllers"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/guidelines/#endpoint","text":"Endpoints are essentially methods in a controller. Like the controller, an endpoint has some special attributes. Here is an example of an endpoint 1 2 3 4 5 6 7 [HttpPost(\"ExampleEndpoint\")] [Authorize] [ProducesResponseType(StatusCodes.Status200OK)] public Task < ActionResult > ExampleEndpoint () { //Does stuff } The [HttpPost(\"ExampleEndpoint\")] does two thing, it defines the endpoint's URL with \"/ExampleEndpoint\" and describes what type of operation this endpoint executes. In this case, if the endpoint was a part of ExampleController from before, the full URL would be /v1/Example/ExampleEndpoint . If you have an endpoint where you do not want to use the URL defined by the controller, you can start with a / to ignore the URL defined in [Route] , using [HttpPost(\"/NoRoute/ExampleEndpoint\")] would give the URL /NoRoute/ExampleEndpoint . [HttpPost] describes what type of operation the endpoint does. There are 4 types of operation, [HttpPost] , [HttpGet] , [HttpPut] , [HttpDelete] . These operations follows CRUD . For more information on how the api_client and the web-api communicates see Backend Architecture . The [Authorize] option works in the same way as the [Authorize] in controllers. The main difference is that this only affects this single endpoint. So if the controller is set to [Authorize] , but an endpoint needs access from non-authorized users, this option is given the value [AllowAnonymous] as seen in the example below. This overwrites the [Authorize] option provided by the controller. If the opposite is the case, and the endpoint needs restricted authorization, the option can be set to [Authorize] or something more specific like [Authorize(Roles = GirafRole.SuperUser + \",\" + GirafRole.Department + \",\" + GirafRole.Guardian)] . 1 2 3 4 5 6 7 [HttpPost(\"ExampleEndpoint\")] [AllowAnonymous] [ProducesResponseType(StatusCodes.Status200OK)] public Task < ActionResult > ExampleEndpoint () { //Does stuff } The [ProducesResponseType(StatusCodes.Status200OK)] is a response the endpoint can give. This particular response is for success. Typically you would like more than this to accommodate multiple outcomes. For instance when a user is not found like in the example below. For a more detailed list of responses please see this link . Bellow is an example of how to use [ProducesResponseType()] . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 [HttpGet(\"{id}\")] [ProducesResponseType(StatusCodes.Status200OK)] [ProducesResponseType(StatusCodes.Status404NotFound)] public async Task < ActionResult > GetById ( int id ) { // Code for finding a user if ( noIdMatch ) { return NotFound ( new ErrorResponse ( ErrorCode . UserNotFound , \"User not found\" )); } return Ok ( new SuccessResponse < Object > ( result )); } You can also use StatusCode(StatusCodes.Status200OK, objectToReturn) for giving status codes to a response. The first attribute is a static class containing http status codes and the second attribute is the object to return.","title":"Endpoint"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/swagger/","text":"Swagger \u00b6 Swagger is a framework of API developer tools. Swagger defines a human and computer readable interface for understanding and documenting a given REST-API. As such Swagger can be used to e.i. generate client-side APIs in different languages. Another nice thing about swagger is that all endpoints are documented in the swagger GUI which can be found by navigating to http://web.giraf.cs.aau.dk:5000/swagger/ . At the GUI the different endpoints and DTOs are displayed and examples of how to all the endpoints are displayed as well a option for making requests to the endpoints and get the response back. Generate a client-side API \u00b6 Because the REST-API integrates swagger as middle-ware it is possible to generate a client-side API in your preferred language. In order to do so, follow the steps: Run the web-api locally Navigate to swagger: http://localhost:5000/swagger/ Copy the url to the swagger json file on top of the site Generate a client in C# using the HTTP library RestSharp \u00b6 Download swagger-codegen from swagger Navigate to the swagger-codegen-cli.jar file and execute the following command: 1 2 java -jar modules/swagger-codegen-cli/target/swagger-codegen-cli.jar generate -i http://localhost:5000/swagger/v1/swagger.json -l csharp -o Client/Generated/ This will produce a set of request methods and DTOs.","title":"Swagger"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/swagger/#swagger","text":"Swagger is a framework of API developer tools. Swagger defines a human and computer readable interface for understanding and documenting a given REST-API. As such Swagger can be used to e.i. generate client-side APIs in different languages. Another nice thing about swagger is that all endpoints are documented in the swagger GUI which can be found by navigating to http://web.giraf.cs.aau.dk:5000/swagger/ . At the GUI the different endpoints and DTOs are displayed and examples of how to all the endpoints are displayed as well a option for making requests to the endpoints and get the response back.","title":"Swagger"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/swagger/#generate-a-client-side-api","text":"Because the REST-API integrates swagger as middle-ware it is possible to generate a client-side API in your preferred language. In order to do so, follow the steps: Run the web-api locally Navigate to swagger: http://localhost:5000/swagger/ Copy the url to the swagger json file on top of the site","title":"Generate a client-side API"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/swagger/#generate-a-client-in-c-using-the-http-library-restsharp","text":"Download swagger-codegen from swagger Navigate to the swagger-codegen-cli.jar file and execute the following command: 1 2 java -jar modules/swagger-codegen-cli/target/swagger-codegen-cli.jar generate -i http://localhost:5000/swagger/v1/swagger.json -l csharp -o Client/Generated/ This will produce a set of request methods and DTOs.","title":"Generate a client in C# using the HTTP library RestSharp"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Authorization/","text":"Overview \u00b6 In this section there are information about the authorization system that has been used in the Web API. Authorization \u00b6 Identity is a membership management system present in the ASP.NET Core framework. It can be used to manage all user related actions e.g. login, register, authorization. Identity is used in combination with JSON Web Tokens to authenticate users, as described in the Token authentication page. Users \u00b6 The class GirafUser has been written for the project, which is derived from IdentityUser . Identity itself is set up as a service in the Startup class. Account management(registering, logging in etc.) is done through the AccountController , while actions and data related to a user is handled in the UserController . Resolving Authorization at Endpoints \u00b6 Whether or not a user is authorised to perform a given action is resolved within each individual method. This means that changes to access policies will require editing of every affected endpoint, as well as a lot of the authorisation logic being copy-pasta. To combat this, the class GirafAuthenticationService (derived IAuthenticationService has been created. In time, every endpoint should make a call to a relevant method in this class(e.g. HasDepartmentEditRights(user, department) ), but currently it is only used in UserController and WeekTemplateController .","title":"Overview"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Authorization/#overview","text":"In this section there are information about the authorization system that has been used in the Web API.","title":"Overview"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Authorization/#authorization","text":"Identity is a membership management system present in the ASP.NET Core framework. It can be used to manage all user related actions e.g. login, register, authorization. Identity is used in combination with JSON Web Tokens to authenticate users, as described in the Token authentication page.","title":"Authorization"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Authorization/#users","text":"The class GirafUser has been written for the project, which is derived from IdentityUser . Identity itself is set up as a service in the Startup class. Account management(registering, logging in etc.) is done through the AccountController , while actions and data related to a user is handled in the UserController .","title":"Users"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Authorization/#resolving-authorization-at-endpoints","text":"Whether or not a user is authorised to perform a given action is resolved within each individual method. This means that changes to access policies will require editing of every affected endpoint, as well as a lot of the authorisation logic being copy-pasta. To combat this, the class GirafAuthenticationService (derived IAuthenticationService has been created. In time, every endpoint should make a call to a relevant method in this class(e.g. HasDepartmentEditRights(user, department) ), but currently it is only used in UserController and WeekTemplateController .","title":"Resolving Authorization at Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Authorization/token_authentication/","text":"Token Authentication \u00b6 For authenticating users we use JWT token authentication , meaning that the user of the REST API must send a bearer token in the header whenever making requests to endpoints that require authentication. Example of Use \u00b6 For instance consider a login request: 1 curl -X POST \"http://localhost:5000/v1/Account/login\" -H \"accept: text/plain\" -H \"Content-Type: application/json-patch+json\" -d \"{ \\\"username\\\": \\\"<yourUserName>\\\", \\\"password\\\": \\\"<youPassword>\\\"}\" This login request will return a token. One can then set the token in the authentication header: Authorization: Bearer <token> And thus a request to get all department names would look like this: 1 curl -X GET \"http://localhost:5000/v1/Department\" -H \"accept: text/plain\" -H \"Authorization: Bearer <token>\" For examples of how to do this in swagger see the README file at the web-api repository . How is JWT configured in GIRAF \u00b6 The configuration is loaded in the Startup.cs class: 1 2 3 4 5 6 public void ConfigureServices ( IServiceCollection services ) { //commented out for brevity... services . Configure < JwtConfig > ( Configuration . GetSection ( \"Jwt\" )); //commented out for brevity... } Thus the settings are configured in appsettings.json in the section called Jwt : 1 2 3 4 5 6 7 8 9 { ... \"Jwt\" : { \"JwtKey\" : \"<30 symbol jwtkey>\" , \"JwtIssuer\" : \"AAU\" , \"JwtExpireDays\" : 30 } ... } What is a JWT \u00b6 JSON Web Token is an authentication token, which actually contains user information, as opposed to API keys. Traditionally, API keys can be a GUID or a nonsense string, which is really a primary key in a database, when supplied in the header of an HTTP request, the backend will then make an appropriate lookup across tables or with its authentication service to ascertain the rights of the user in question, if any. A JWT works differently: It actually contains serialized information about the user, and their access rights, this has the advantage of saving the service from database lookups, to obtain the information. It also contains a timestamp, by which the service can determine whether it is still valid. Comparison between classic API Key usage, and JWT \u00b6 API Key Flow JWT Flow Login Request (not necessary for a classic API key) 1. Login request is made by the client 2. The Server retrieves the users permissions from the database 3. The Server generates a JSON object containing the users permissions signs it and serializes it 4. The client receives the token General Requests 1. The client makes a request, supplying the key in its header 1. The client makes a request, supplying the JWT in the header 2. The server looks up the key in the database and checks the permissions 2. The server performs the request 3. The server performs the requested action 3. The client receives the response 4. The client receives the response So while it requires an initial login to use the JWT, we can see that it saves the server a database query every time it generally performs requests, and since database access is one of the more expensive and time consuming things on the server side, this is quite useful. How does JWT work \u00b6 If a JWT contains user information, or information about access rights, it may seem insecure. But this is not the case, in fact, it works much in the same way as RSA certificates, and can in fact use RSA or an algorithm called HMAC. On an abstract level it works like this: Token creation \u00b6 A login is attempted at the server, e.g. with username and password The server verifies the information, and creates a json object containing the relevant information The server then SIGNS the information, and serializes it. Finally the object is returned to the user. From this point onward, there are two possibilities of what happens when a user tries to log back in: Client access via Token \u00b6 The client sends the JWT to the server The server verifies the signature, resulting in one of two things: Allows the client to do whatever the JWT says he or she is allowed to do Verification fails, and the JWT is discarded, the client is then typically presented with HTTP 403 or similar, if the client has a gui, the user can redirected to a login page, or the credentials are cached, and the token is refreshed. What happens when user rights, etc. are altered \u00b6 One of the weaknesses of the JWT is that since it is signed by an authority, the information in it is valid until the signature (timestamp etc.) determines it is no longer so, or at least until it is checked against the database next time. This means that rights may be changed serverside by an administrator, but it may take until the whatever the refresh rate set by the server is, to deny the client the right to some action which has now been denied them.","title":"Token Authentication"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Authorization/token_authentication/#token-authentication","text":"For authenticating users we use JWT token authentication , meaning that the user of the REST API must send a bearer token in the header whenever making requests to endpoints that require authentication.","title":"Token Authentication"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Authorization/token_authentication/#example-of-use","text":"For instance consider a login request: 1 curl -X POST \"http://localhost:5000/v1/Account/login\" -H \"accept: text/plain\" -H \"Content-Type: application/json-patch+json\" -d \"{ \\\"username\\\": \\\"<yourUserName>\\\", \\\"password\\\": \\\"<youPassword>\\\"}\" This login request will return a token. One can then set the token in the authentication header: Authorization: Bearer <token> And thus a request to get all department names would look like this: 1 curl -X GET \"http://localhost:5000/v1/Department\" -H \"accept: text/plain\" -H \"Authorization: Bearer <token>\" For examples of how to do this in swagger see the README file at the web-api repository .","title":"Example of Use"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Authorization/token_authentication/#how-is-jwt-configured-in-giraf","text":"The configuration is loaded in the Startup.cs class: 1 2 3 4 5 6 public void ConfigureServices ( IServiceCollection services ) { //commented out for brevity... services . Configure < JwtConfig > ( Configuration . GetSection ( \"Jwt\" )); //commented out for brevity... } Thus the settings are configured in appsettings.json in the section called Jwt : 1 2 3 4 5 6 7 8 9 { ... \"Jwt\" : { \"JwtKey\" : \"<30 symbol jwtkey>\" , \"JwtIssuer\" : \"AAU\" , \"JwtExpireDays\" : 30 } ... }","title":"How is JWT configured in GIRAF"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Authorization/token_authentication/#what-is-a-jwt","text":"JSON Web Token is an authentication token, which actually contains user information, as opposed to API keys. Traditionally, API keys can be a GUID or a nonsense string, which is really a primary key in a database, when supplied in the header of an HTTP request, the backend will then make an appropriate lookup across tables or with its authentication service to ascertain the rights of the user in question, if any. A JWT works differently: It actually contains serialized information about the user, and their access rights, this has the advantage of saving the service from database lookups, to obtain the information. It also contains a timestamp, by which the service can determine whether it is still valid.","title":"What is a JWT"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Authorization/token_authentication/#comparison-between-classic-api-key-usage-and-jwt","text":"API Key Flow JWT Flow Login Request (not necessary for a classic API key) 1. Login request is made by the client 2. The Server retrieves the users permissions from the database 3. The Server generates a JSON object containing the users permissions signs it and serializes it 4. The client receives the token General Requests 1. The client makes a request, supplying the key in its header 1. The client makes a request, supplying the JWT in the header 2. The server looks up the key in the database and checks the permissions 2. The server performs the request 3. The server performs the requested action 3. The client receives the response 4. The client receives the response So while it requires an initial login to use the JWT, we can see that it saves the server a database query every time it generally performs requests, and since database access is one of the more expensive and time consuming things on the server side, this is quite useful.","title":"Comparison between classic API Key usage, and JWT"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Authorization/token_authentication/#how-does-jwt-work","text":"If a JWT contains user information, or information about access rights, it may seem insecure. But this is not the case, in fact, it works much in the same way as RSA certificates, and can in fact use RSA or an algorithm called HMAC. On an abstract level it works like this:","title":"How does JWT work"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Authorization/token_authentication/#token-creation","text":"A login is attempted at the server, e.g. with username and password The server verifies the information, and creates a json object containing the relevant information The server then SIGNS the information, and serializes it. Finally the object is returned to the user. From this point onward, there are two possibilities of what happens when a user tries to log back in:","title":"Token creation"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Authorization/token_authentication/#client-access-via-token","text":"The client sends the JWT to the server The server verifies the signature, resulting in one of two things: Allows the client to do whatever the JWT says he or she is allowed to do Verification fails, and the JWT is discarded, the client is then typically presented with HTTP 403 or similar, if the client has a gui, the user can redirected to a login page, or the credentials are cached, and the token is refreshed.","title":"Client access via Token"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Authorization/token_authentication/#what-happens-when-user-rights-etc-are-altered","text":"One of the weaknesses of the JWT is that since it is signed by an authority, the information in it is valid until the signature (timestamp etc.) determines it is no longer so, or at least until it is checked against the database next time. This means that rights may be changed serverside by an administrator, but it may take until the whatever the refresh rate set by the server is, to deny the client the right to some action which has now been denied them.","title":"What happens when user rights, etc. are altered"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/","text":"Overview \u00b6 This section gives a description for each of the endpoints in the Web API. Account User Department Activity Pictogram Week Week template Status Error","title":"Overview"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/#overview","text":"This section gives a description for each of the endpoints in the Web API. Account User Department Activity Pictogram Week Week template Status Error","title":"Overview"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/account_endpoints/","text":"Account Endpoints \u00b6 Login \u00b6 1 /v1/Account/login This endpoint allows the user to sign in to his/her account by providing a valid username and password. The request body for the endpoint is a json object that takes a string for a username and a password: 1 2 3 4 { \"username\": \"string\", \"password\": \"string\" } Possible responses are a 200 Success code, 400 Bad Request or 401 Unauthorized. Register \u00b6 1 /v1/Account/register Registers a new user in the REST-API. The json object expects a username, password, display name, department id, and role. 1 2 3 4 5 6 7 { \"username\": \"string\", \"password\": \"string\", \"displayName\": \"string\", \"departmentId\": 0, \"role\": 1 } Possible responses are 200 Success, 400 Bad Request, 403 Forbidden, 409 Conflict or a 500 server error. Update Password \u00b6 1 /v1/User/{id}/Account/password Allows the user to update his password with a PUT request, if they know their old password. An example URL for updating password with a user ID http://localhost:5000/v1/User/fbfd2be6-414a-4c34-897b-49c3fad64d21/Account/password and the request body requires the old password and a new password: 1 2 3 4 { \"oldPassword\": \"password\", \"newPassword\": \"password\" } The possible responses are 200 Success, 400 Bad Request, 403 Forbidden, 404 Not Found or 500 a server error. Set a New Password \u00b6 1 /v1/User/{id}/Account/password Allows a user to set a new password with a POST request, if they forgot theirs. The request then needs a new password as well as the given user password-reset-token: 1 2 3 4 { \"password\": \"string\", \"token\": \"string\" } Possible responses are 200 Success, 400 Bad Request, 401 Unauthorized and 404 Not Found. Requesting a Password Reset Token \u00b6 1 /v1/User/{id}/Account/password-reset-token Allows the user to get a password reset token for a given user. This GET request outputs a password reset token for a given user, e.g. Request URL: 1 http://localhost:5000/v1/User/fbfd2be6-414a-4c34-897b-49c3fad64d21/Account/password-reset-token and response body: 1 2 3 { \"data\": \"'token string'\" } The token string is the input in the POST request 'Set a new password'. Possible response codes are 200 success, 401 Unauthorized and 404 Not Found. Delete User \u00b6 1 /v1/Account/user/{userId} Deletes the user with the given id. The DELETE request takes the user id as input and prompts a response body with either a 200 Success, 400 Bad Request, 403 Forbidden, 409 Conflict or 500 a server error.","title":"Account Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/account_endpoints/#account-endpoints","text":"","title":"Account Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/account_endpoints/#login","text":"1 /v1/Account/login This endpoint allows the user to sign in to his/her account by providing a valid username and password. The request body for the endpoint is a json object that takes a string for a username and a password: 1 2 3 4 { \"username\": \"string\", \"password\": \"string\" } Possible responses are a 200 Success code, 400 Bad Request or 401 Unauthorized.","title":"Login"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/account_endpoints/#register","text":"1 /v1/Account/register Registers a new user in the REST-API. The json object expects a username, password, display name, department id, and role. 1 2 3 4 5 6 7 { \"username\": \"string\", \"password\": \"string\", \"displayName\": \"string\", \"departmentId\": 0, \"role\": 1 } Possible responses are 200 Success, 400 Bad Request, 403 Forbidden, 409 Conflict or a 500 server error.","title":"Register"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/account_endpoints/#update-password","text":"1 /v1/User/{id}/Account/password Allows the user to update his password with a PUT request, if they know their old password. An example URL for updating password with a user ID http://localhost:5000/v1/User/fbfd2be6-414a-4c34-897b-49c3fad64d21/Account/password and the request body requires the old password and a new password: 1 2 3 4 { \"oldPassword\": \"password\", \"newPassword\": \"password\" } The possible responses are 200 Success, 400 Bad Request, 403 Forbidden, 404 Not Found or 500 a server error.","title":"Update Password"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/account_endpoints/#set-a-new-password","text":"1 /v1/User/{id}/Account/password Allows a user to set a new password with a POST request, if they forgot theirs. The request then needs a new password as well as the given user password-reset-token: 1 2 3 4 { \"password\": \"string\", \"token\": \"string\" } Possible responses are 200 Success, 400 Bad Request, 401 Unauthorized and 404 Not Found.","title":"Set a New Password"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/account_endpoints/#requesting-a-password-reset-token","text":"1 /v1/User/{id}/Account/password-reset-token Allows the user to get a password reset token for a given user. This GET request outputs a password reset token for a given user, e.g. Request URL: 1 http://localhost:5000/v1/User/fbfd2be6-414a-4c34-897b-49c3fad64d21/Account/password-reset-token and response body: 1 2 3 { \"data\": \"'token string'\" } The token string is the input in the POST request 'Set a new password'. Possible response codes are 200 success, 401 Unauthorized and 404 Not Found.","title":"Requesting a Password Reset Token"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/account_endpoints/#delete-user","text":"1 /v1/Account/user/{userId} Deletes the user with the given id. The DELETE request takes the user id as input and prompts a response body with either a 200 Success, 400 Bad Request, 403 Forbidden, 409 Conflict or 500 a server error.","title":"Delete User"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/activity_endpoints/","text":"Activity Endpoints \u00b6 Create Activity \u00b6 1 /v2/Activity/{userId}/{weekplanName}/{weekYear}/{weekNumber}/{weekDayNmb} Add a new activity to a given weekplan on the given day. Bulletpoints of parameters for this request: {userId} (string): id of the user that you want to add the activity for. {weekplanName} (string): name of the weekplan that you want to add the activity on. {weekYear} (integer): year of the weekplan that you want to add the activity on. {weekNumber} (integer): week number of the weekplan that you want to add the activity on. {weekDayNmb} (integer): day of the week that you want to add the activity on (Monday=1, Sunday=7). Example response body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \"pictograms\": [ { \"id\": 0, \"lastEdit\": \"2020-10-20T02:58:02.072Z\", \"title\": \"string\", \"accessLevel\": 1, \"imageHash\": \"string\" } ], \"order\": 0, \"state\": 1, \"id\": 0, \"isChoiceBoard\": true, \"timer\": { \"startTime\": 0, \"progress\": 0, \"fullLength\": 0, \"paused\": true, \"key\": 0 } } Possible status code responses are 201 Success, 400 Bad Request, 403 Forbidden and 404 Not Found. Delete Activity \u00b6 1 /v2/Activity/{userId}/delete/{activityId} Delete an activity with a given id. Bulletpoints of parameters for this request: {userId} (string): id of the user you want to delete an activity for. {activityId} (integer): id of the activity you want to delete. Possible status code responses are 200 Success, 403 Forbidden, 404 Not Found. Get Activity \u00b6 1 /v2/Activity/{userId}/{activityId} Gets a given activity for a given user. Bulletpoints of parameters for this request: {userId} (string): id of the user you want to delete an activity for. {activityId} (integer): id of the activity you want to delete. Example response body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"data\": { \"pictograms\": [ { \"id\": 6, \"lastEdit\": \"2020-10-19T13:02:25.082867Z\", \"title\": \"alting\", \"accessLevel\": 1, \"imageHash\": \"secure hash\", \"imageUrl\": \"/v1/pictogram/6/image/raw\" } ], \"order\": 0, \"state\": 2, \"id\": 1, \"isChoiceBoard\": false, \"timer\": null } } Possible status code response is 200 Success. Update Activity for Given User \u00b6 1 /v2/Activity/{userId}/update Updates an activity with a given id. {userId} (string): id of the user you want to delete an activity for. Request body required for this PATCH request: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \"pictograms\": [ { \"id\": 0, \"lastEdit\": \"2020-10-20T03:09:26.772Z\", \"title\": \"string\", \"accessLevel\": 1, \"imageHash\": \"string\" } ], \"order\": 0, \"state\": 1, \"id\": 0, \"isChoiceBoard\": true, \"timer\": { \"startTime\": 0, \"progress\": 0, \"fullLength\": 0, \"paused\": true, \"key\": 0 } } Possible status code response are 200 Success, 400 Bad Request, 403 Forbidden and 404 Not Found.","title":"Activity Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/activity_endpoints/#activity-endpoints","text":"","title":"Activity Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/activity_endpoints/#create-activity","text":"1 /v2/Activity/{userId}/{weekplanName}/{weekYear}/{weekNumber}/{weekDayNmb} Add a new activity to a given weekplan on the given day. Bulletpoints of parameters for this request: {userId} (string): id of the user that you want to add the activity for. {weekplanName} (string): name of the weekplan that you want to add the activity on. {weekYear} (integer): year of the weekplan that you want to add the activity on. {weekNumber} (integer): week number of the weekplan that you want to add the activity on. {weekDayNmb} (integer): day of the week that you want to add the activity on (Monday=1, Sunday=7). Example response body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \"pictograms\": [ { \"id\": 0, \"lastEdit\": \"2020-10-20T02:58:02.072Z\", \"title\": \"string\", \"accessLevel\": 1, \"imageHash\": \"string\" } ], \"order\": 0, \"state\": 1, \"id\": 0, \"isChoiceBoard\": true, \"timer\": { \"startTime\": 0, \"progress\": 0, \"fullLength\": 0, \"paused\": true, \"key\": 0 } } Possible status code responses are 201 Success, 400 Bad Request, 403 Forbidden and 404 Not Found.","title":"Create Activity"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/activity_endpoints/#delete-activity","text":"1 /v2/Activity/{userId}/delete/{activityId} Delete an activity with a given id. Bulletpoints of parameters for this request: {userId} (string): id of the user you want to delete an activity for. {activityId} (integer): id of the activity you want to delete. Possible status code responses are 200 Success, 403 Forbidden, 404 Not Found.","title":"Delete Activity"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/activity_endpoints/#get-activity","text":"1 /v2/Activity/{userId}/{activityId} Gets a given activity for a given user. Bulletpoints of parameters for this request: {userId} (string): id of the user you want to delete an activity for. {activityId} (integer): id of the activity you want to delete. Example response body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"data\": { \"pictograms\": [ { \"id\": 6, \"lastEdit\": \"2020-10-19T13:02:25.082867Z\", \"title\": \"alting\", \"accessLevel\": 1, \"imageHash\": \"secure hash\", \"imageUrl\": \"/v1/pictogram/6/image/raw\" } ], \"order\": 0, \"state\": 2, \"id\": 1, \"isChoiceBoard\": false, \"timer\": null } } Possible status code response is 200 Success.","title":"Get Activity"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/activity_endpoints/#update-activity-for-given-user","text":"1 /v2/Activity/{userId}/update Updates an activity with a given id. {userId} (string): id of the user you want to delete an activity for. Request body required for this PATCH request: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \"pictograms\": [ { \"id\": 0, \"lastEdit\": \"2020-10-20T03:09:26.772Z\", \"title\": \"string\", \"accessLevel\": 1, \"imageHash\": \"string\" } ], \"order\": 0, \"state\": 1, \"id\": 0, \"isChoiceBoard\": true, \"timer\": { \"startTime\": 0, \"progress\": 0, \"fullLength\": 0, \"paused\": true, \"key\": 0 } } Possible status code response are 200 Success, 400 Bad Request, 403 Forbidden and 404 Not Found.","title":"Update Activity for Given User"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/department_endpoints/","text":"Department Endpoints \u00b6 Get Department Names \u00b6 1 /v1/Department Get request for getting all the department names. Example response body: 1 2 3 4 5 6 7 8 9 10 11 12 { \"data\": [ { \"id\": 1, \"name\": \"Bajer plejen\" }, { \"id\": 2, \"name\": \"Tobias' stue for godt hum\u00f8r\" } ] } Possible status code responses are 200 Success and 404 Not Found. Create a Department \u00b6 1 /v1/Department Create a new department. It is only necessary to supply the departments name. Request body required for this POST request: 1 2 3 4 5 6 7 8 9 10 11 12 { \"name\": \"string\", \"members\": [ { \"displayName\": \"string\", \"userId\": \"string\" } ], \"resources\": [ 0 ] } Possible status code responses are 200 Success, 400 Bad Request, 403 Forbidden, 404 Not Found and 500 Server Error. Get Given Department \u00b6 1 /v1/Department/{id} Get the department with the specified id. Example response body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 { \"data\": { \"id\": 2, \"name\": \"Tobias' stue for godt hum\u00f8r\", \"members\": [ { \"displayName\": \"Tobias Tobiassss\", \"userRole\": \"Department\", \"userId\": \"b12a7cb9-de27-44ff-9d91-7b8b06db8269\" }, { \"displayName\": \"Simon Sim\", \"userRole\": \"Citizen\", \"userId\": \"0fc4a29a-ac65-4c2f-a7db-20e6af21e4c9\" }, { \"displayName\": \"Harald Graatand\", \"userRole\": \"Guardian\", \"userId\": \"132b458a-1e84-45c8-a61d-042407faa817\" }, { \"displayName\": \"Kurt Andersen\", \"userRole\": \"Citizen\", \"userId\": \"68ea551d-376d-4da1-8030-e833e2982dc9\" }, { \"displayName\": \"Deck\", \"userRole\": \"Citizen\", \"userId\": \"a4e2f736-757b-4495-8372-e0cd9cc93ef7\" } ], \"resources\": [] } } Possible status code responses are 200 Success, 403 Forbidden and 404 Not Found. Get Citizen Names \u00b6 1 /v1/Department/{id}/citizens Gets the citizen names. Example response body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"data\": [ { \"displayName\": \"Simon Sim\", \"userRole\": \"Citizen\", \"userId\": \"0fc4a29a-ac65-4c2f-a7db-20e6af21e4c9\" }, { \"displayName\": \"Kurt Andersen\", \"userRole\": \"Citizen\", \"userId\": \"68ea551d-376d-4da1-8030-e833e2982dc9\" }, { \"displayName\": \"Deck\", \"userRole\": \"Citizen\", \"userId\": \"a4e2f736-757b-4495-8372-e0cd9cc93ef7\" } ] } Possible status code responses are 200 Success, 403 Forbidden and 404 Not Found. Add User to a Department \u00b6 1 /v1/Department/{departmentId}/user/{userId} Add an existing user, that does not have a department, to the given department. Requires role Department, Guardian or SuperUser. Possible status code responses are 200 Success, 400 Bad Request, 401 Unauthorized, 403 Forbidden and 409 Conflict. Change Department Name \u00b6 1 /v1/Department/{departmentId}/name Handles changing name of a Department. Request body required for this PUT request: 1 2 3 4 { \"id\": 0, \"name\": \"string\" } Possible status code responses are 200 Success, 400 Bad Request, 403 Forbidden and 404 Not Found. Delete Department \u00b6 1 /v1/Department/{departmentId} Endpoint for deleting the GirafRest.Models.Department with the given id. Possible status code responses are 200 Success, 403 Forbidden and 404 Not Found.","title":"Department Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/department_endpoints/#department-endpoints","text":"","title":"Department Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/department_endpoints/#get-department-names","text":"1 /v1/Department Get request for getting all the department names. Example response body: 1 2 3 4 5 6 7 8 9 10 11 12 { \"data\": [ { \"id\": 1, \"name\": \"Bajer plejen\" }, { \"id\": 2, \"name\": \"Tobias' stue for godt hum\u00f8r\" } ] } Possible status code responses are 200 Success and 404 Not Found.","title":"Get Department Names"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/department_endpoints/#create-a-department","text":"1 /v1/Department Create a new department. It is only necessary to supply the departments name. Request body required for this POST request: 1 2 3 4 5 6 7 8 9 10 11 12 { \"name\": \"string\", \"members\": [ { \"displayName\": \"string\", \"userId\": \"string\" } ], \"resources\": [ 0 ] } Possible status code responses are 200 Success, 400 Bad Request, 403 Forbidden, 404 Not Found and 500 Server Error.","title":"Create a Department"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/department_endpoints/#get-given-department","text":"1 /v1/Department/{id} Get the department with the specified id. Example response body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 { \"data\": { \"id\": 2, \"name\": \"Tobias' stue for godt hum\u00f8r\", \"members\": [ { \"displayName\": \"Tobias Tobiassss\", \"userRole\": \"Department\", \"userId\": \"b12a7cb9-de27-44ff-9d91-7b8b06db8269\" }, { \"displayName\": \"Simon Sim\", \"userRole\": \"Citizen\", \"userId\": \"0fc4a29a-ac65-4c2f-a7db-20e6af21e4c9\" }, { \"displayName\": \"Harald Graatand\", \"userRole\": \"Guardian\", \"userId\": \"132b458a-1e84-45c8-a61d-042407faa817\" }, { \"displayName\": \"Kurt Andersen\", \"userRole\": \"Citizen\", \"userId\": \"68ea551d-376d-4da1-8030-e833e2982dc9\" }, { \"displayName\": \"Deck\", \"userRole\": \"Citizen\", \"userId\": \"a4e2f736-757b-4495-8372-e0cd9cc93ef7\" } ], \"resources\": [] } } Possible status code responses are 200 Success, 403 Forbidden and 404 Not Found.","title":"Get Given Department"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/department_endpoints/#get-citizen-names","text":"1 /v1/Department/{id}/citizens Gets the citizen names. Example response body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"data\": [ { \"displayName\": \"Simon Sim\", \"userRole\": \"Citizen\", \"userId\": \"0fc4a29a-ac65-4c2f-a7db-20e6af21e4c9\" }, { \"displayName\": \"Kurt Andersen\", \"userRole\": \"Citizen\", \"userId\": \"68ea551d-376d-4da1-8030-e833e2982dc9\" }, { \"displayName\": \"Deck\", \"userRole\": \"Citizen\", \"userId\": \"a4e2f736-757b-4495-8372-e0cd9cc93ef7\" } ] } Possible status code responses are 200 Success, 403 Forbidden and 404 Not Found.","title":"Get Citizen Names"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/department_endpoints/#add-user-to-a-department","text":"1 /v1/Department/{departmentId}/user/{userId} Add an existing user, that does not have a department, to the given department. Requires role Department, Guardian or SuperUser. Possible status code responses are 200 Success, 400 Bad Request, 401 Unauthorized, 403 Forbidden and 409 Conflict.","title":"Add User to a Department"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/department_endpoints/#change-department-name","text":"1 /v1/Department/{departmentId}/name Handles changing name of a Department. Request body required for this PUT request: 1 2 3 4 { \"id\": 0, \"name\": \"string\" } Possible status code responses are 200 Success, 400 Bad Request, 403 Forbidden and 404 Not Found.","title":"Change Department Name"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/department_endpoints/#delete-department","text":"1 /v1/Department/{departmentId} Endpoint for deleting the GirafRest.Models.Department with the given id. Possible status code responses are 200 Success, 403 Forbidden and 404 Not Found.","title":"Delete Department"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/error_endpoints/","text":"Error Endpoints \u00b6 Post Status Code \u00b6 1 /v1/Error This endpoint is reached when an error happens in the routing. Possible query: statusCode (int): The statuscode gotten when the error happened Example request URL: 1 http://localhost:5000/v1/Error?statusCode=400 Example response body: 1 2 3 4 5 { \"message\": \"Statuscode: 400\", \"details\": \"\", \"errorKey\": \"UnknownError\" } Get Status Code \u00b6 1 /v1/Error This endpoint is reached when an error happens in the routing. Update Status Code \u00b6 1 /v1/Error This endpoint is reached when an error happens in the routing. Delete Status Code \u00b6 1 /v1/Error This endpoint is reached when an error happens in the routing. Patch Status Code \u00b6 1 /v1/Error This endpoint is reached when an error happens in the routing.","title":"Error Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/error_endpoints/#error-endpoints","text":"","title":"Error Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/error_endpoints/#post-status-code","text":"1 /v1/Error This endpoint is reached when an error happens in the routing. Possible query: statusCode (int): The statuscode gotten when the error happened Example request URL: 1 http://localhost:5000/v1/Error?statusCode=400 Example response body: 1 2 3 4 5 { \"message\": \"Statuscode: 400\", \"details\": \"\", \"errorKey\": \"UnknownError\" }","title":"Post Status Code"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/error_endpoints/#get-status-code","text":"1 /v1/Error This endpoint is reached when an error happens in the routing.","title":"Get Status Code"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/error_endpoints/#update-status-code","text":"1 /v1/Error This endpoint is reached when an error happens in the routing.","title":"Update Status Code"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/error_endpoints/#delete-status-code","text":"1 /v1/Error This endpoint is reached when an error happens in the routing.","title":"Delete Status Code"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/error_endpoints/#patch-status-code","text":"1 /v1/Error This endpoint is reached when an error happens in the routing.","title":"Patch Status Code"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/pictogram_endpoints/","text":"Pictogram Endpoints \u00b6 Get All Public Pictograms \u00b6 1 /v1/Pictogram Get all public GirafRest.Models.Pictogram pictograms available to the user (i.e the public pictograms and those owned by the user (PRIVATE) and his department (PROTECTED)). Possible queries: query (string): the query string. pictograms are filtered based on this string if passed. page (integer): Page number. pageSize (integer): Number of pictograms per page. Example request URL with page and pageSize query: 1 http://localhost:5000/v1/Pictogram?page=1&pageSize=1 Response body: 1 2 3 4 5 6 7 8 9 10 11 12 { \"data\": [ { \"id\": 1, \"lastEdit\": \"2020-10-19T13:02:25.080748Z\", \"title\": \"som\", \"accessLevel\": 1, \"imageHash\": \"secure hash\", \"imageUrl\": \"/v1/pictogram/1/image/raw\" } ] } Possible status response codes are 200 Success, 400 Bad Request and 404 Not Found. Create Pictogram \u00b6 1 /v1/Pictogram Create a new GirafRest.Models.Pictogram pictogram. Request body required for this POST request: 1 2 3 4 5 { \"title\": \"string\", \"accessLevel\": 1, \"imageHash\": \"string\" } Status response codes are 200 Success, 400 Bad Request and 404 Not Found. Check Given Pictogram Authorization \u00b6 1 /v1/Pictogram/{id} Read the GirafRest.Models.Pictogram pictogram with the specified id and check if the user is authorized to see it. Status response codes are 200 Success, 401 Unauthorized, 403 Forbidden, 404 Not Found and 500 Server Error. Update Pictogram \u00b6 1 /v1/Pictogram/{id} Update info of a GirafRest.Models.Pictogram pictogram. Request body required for this PUT request: 1 2 3 4 5 { \"title\": \"string\", \"accessLevel\": 1, \"imageHash\": \"string\" } Status response codes are 200 Success, 400 Bad Request, 401 Unauthorized, 403 Forbidden and 404 Not Found. Delete Pictogram \u00b6 1 /v1/Pictogram/{id} Delete the GirafRest.Models.Pictogram pictogram with the specified id. Status response codes are 200 Success, 401 Unauthorized, 403 Forbidden and 404 Not Found. Update Pictogram Image \u00b6 1 /v1/Pictogram/{id}/image Update the image of a GirafRest.Models.Pictogram pictogram with the given Id. Example response body: 1 2 3 4 5 6 7 8 9 10 { \"data\": { \"id\": 1, \"lastEdit\": \"2020-10-19T13:02:25.080748Z\", \"title\": \"som\", \"accessLevel\": 1, \"imageHash\": \"secure hash\", \"imageUrl\": \"/v1/pictogram/1/image/raw\" } } Status response codes are 200 Success, 401 Unauthorized, 403 Forbidden and 404 Not Found. Get Pictogram Image \u00b6 1 /v1/Pictogram/{id}/image Read the image of a given pictogram as a sequence of bytes. Status response codes are 200 Success, 401 Unauthorized, 403 Forbidden and 404 Not Found. Get Raw Pictogram Image \u00b6 1 /v1/Pictogram/{id}/image/raw Reads the raw pictogram image. You are allowed to read all public pictograms aswell as your own pictograms or any pictograms shared within the department. Status response codes are 200 Success, 401 Unauthorized, 403 Forbidden and 404 Not Found.","title":"Pictogram Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/pictogram_endpoints/#pictogram-endpoints","text":"","title":"Pictogram Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/pictogram_endpoints/#get-all-public-pictograms","text":"1 /v1/Pictogram Get all public GirafRest.Models.Pictogram pictograms available to the user (i.e the public pictograms and those owned by the user (PRIVATE) and his department (PROTECTED)). Possible queries: query (string): the query string. pictograms are filtered based on this string if passed. page (integer): Page number. pageSize (integer): Number of pictograms per page. Example request URL with page and pageSize query: 1 http://localhost:5000/v1/Pictogram?page=1&pageSize=1 Response body: 1 2 3 4 5 6 7 8 9 10 11 12 { \"data\": [ { \"id\": 1, \"lastEdit\": \"2020-10-19T13:02:25.080748Z\", \"title\": \"som\", \"accessLevel\": 1, \"imageHash\": \"secure hash\", \"imageUrl\": \"/v1/pictogram/1/image/raw\" } ] } Possible status response codes are 200 Success, 400 Bad Request and 404 Not Found.","title":"Get All Public Pictograms"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/pictogram_endpoints/#create-pictogram","text":"1 /v1/Pictogram Create a new GirafRest.Models.Pictogram pictogram. Request body required for this POST request: 1 2 3 4 5 { \"title\": \"string\", \"accessLevel\": 1, \"imageHash\": \"string\" } Status response codes are 200 Success, 400 Bad Request and 404 Not Found.","title":"Create Pictogram"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/pictogram_endpoints/#check-given-pictogram-authorization","text":"1 /v1/Pictogram/{id} Read the GirafRest.Models.Pictogram pictogram with the specified id and check if the user is authorized to see it. Status response codes are 200 Success, 401 Unauthorized, 403 Forbidden, 404 Not Found and 500 Server Error.","title":"Check Given Pictogram Authorization"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/pictogram_endpoints/#update-pictogram","text":"1 /v1/Pictogram/{id} Update info of a GirafRest.Models.Pictogram pictogram. Request body required for this PUT request: 1 2 3 4 5 { \"title\": \"string\", \"accessLevel\": 1, \"imageHash\": \"string\" } Status response codes are 200 Success, 400 Bad Request, 401 Unauthorized, 403 Forbidden and 404 Not Found.","title":"Update Pictogram"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/pictogram_endpoints/#delete-pictogram","text":"1 /v1/Pictogram/{id} Delete the GirafRest.Models.Pictogram pictogram with the specified id. Status response codes are 200 Success, 401 Unauthorized, 403 Forbidden and 404 Not Found.","title":"Delete Pictogram"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/pictogram_endpoints/#update-pictogram-image","text":"1 /v1/Pictogram/{id}/image Update the image of a GirafRest.Models.Pictogram pictogram with the given Id. Example response body: 1 2 3 4 5 6 7 8 9 10 { \"data\": { \"id\": 1, \"lastEdit\": \"2020-10-19T13:02:25.080748Z\", \"title\": \"som\", \"accessLevel\": 1, \"imageHash\": \"secure hash\", \"imageUrl\": \"/v1/pictogram/1/image/raw\" } } Status response codes are 200 Success, 401 Unauthorized, 403 Forbidden and 404 Not Found.","title":"Update Pictogram Image"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/pictogram_endpoints/#get-pictogram-image","text":"1 /v1/Pictogram/{id}/image Read the image of a given pictogram as a sequence of bytes. Status response codes are 200 Success, 401 Unauthorized, 403 Forbidden and 404 Not Found.","title":"Get Pictogram Image"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/pictogram_endpoints/#get-raw-pictogram-image","text":"1 /v1/Pictogram/{id}/image/raw Reads the raw pictogram image. You are allowed to read all public pictograms aswell as your own pictograms or any pictograms shared within the department. Status response codes are 200 Success, 401 Unauthorized, 403 Forbidden and 404 Not Found.","title":"Get Raw Pictogram Image"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/status_endpoints/","text":"Status Endpoints \u00b6 API Ping \u00b6 1 /v1/Status GET request for checking if the API is running. Example response body: 1 2 3 { \"data\": \"GIRAF API is running!\" } Possible status response code is 200 Success. Database Connection Ping \u00b6 1 /v1/Status/database GET request for checking connection to the database. Example response body: 1 2 3 { \"data\": \"Connection to database\" } Possible status responses code are 200 Success and 503 server error. Get Git Version \u00b6 1 /v1/Status/version-info Endpoint for getting git version info i.e. branch and commithash. Example response body: 1 2 3 { \"data\": \"CommitHash: ref: refs/heads/feature/114\" } Possible status response is 200 Success.","title":"Status Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/status_endpoints/#status-endpoints","text":"","title":"Status Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/status_endpoints/#api-ping","text":"1 /v1/Status GET request for checking if the API is running. Example response body: 1 2 3 { \"data\": \"GIRAF API is running!\" } Possible status response code is 200 Success.","title":"API Ping"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/status_endpoints/#database-connection-ping","text":"1 /v1/Status/database GET request for checking connection to the database. Example response body: 1 2 3 { \"data\": \"Connection to database\" } Possible status responses code are 200 Success and 503 server error.","title":"Database Connection Ping"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/status_endpoints/#get-git-version","text":"1 /v1/Status/version-info Endpoint for getting git version info i.e. branch and commithash. Example response body: 1 2 3 { \"data\": \"CommitHash: ref: refs/heads/feature/114\" } Possible status response is 200 Success.","title":"Get Git Version"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/user_endpoints/","text":"User Endpoints \u00b6 Get User \u00b6 1 /v1/User Find information about the currently authenticated user. Possible status code responses are 200 Success and 404 Not Found. Get Given User \u00b6 1 /v1/User/{id} Find information on the user with the username supplied as a url query parameter or the current user. Status code responses are 200 Success, 400 Bad Request, 403 Forbidden and 404 Not Found. Update Given User \u00b6 1 /v1/User/{id} Updates the user with the information in GirafRest.Models.DTOs.GirafUserDTO Example user PUT request body: 1 2 3 4 5 { \"role\": 1, \"username\": \"string\", \"displayName\": \"string\" } Status code responses are 200 Success, 400 Bad Request, 403 Forbidden, 404 Not Found and 409 Conflict. Get Given User-settings \u00b6 1 /v1/User/{id}/settings Get user-settings for the user with the specified Id Status code responses are 200 Success, 400 Bad Request, 403 Forbidden and 404 Not Found. Update Given User-settings \u00b6 1 /v1/User/{id}/settings Updates the user settings for the user with the provided id. Example user-setting PUT request body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"orientation\": 1, \"completeMark\": 1, \"cancelMark\": 1, \"defaultTimer\": 1, \"timerSeconds\": 0, \"activitiesCount\": 0, \"theme\": 1, \"nrOfDaysToDisplay\": 0, \"greyScale\": true, \"lockTimerControl\": true, \"pictogramText\": true, \"showPopup\": true, \"weekDayColors\": [ { \"hexColor\": \"string\", \"day\": 1 } ] } Status code responses are 200 Success, 400 Bad Request, 403 Forbidden and 404 Not Found. Get Given User Icon \u00b6 1 /v1/User/{id}/icon Endpoint for getting the UserIcon for a specific User- Possible status code responses are 200 Success and 404 Not found. Update Given User Icon \u00b6 1 /v1/User/{id}/icon Sets the user icon of the given user. Status code responses are 200 Success, 400 Bad Request, 403 Forbidden and 404 Not Found. Delete Given User Icon \u00b6 1 /v1/User/{id}/icon Deletes the user icon for a given user. Status code responses are 200 Success, 400 Bad Request, 403 Forbidden and 404 Not Found. Get Given Raw User Icon \u00b6 1 /v1/User/{id}/icon/raw Gets the raw user icon for a given user. Status code responses are 200 Success and 404 Not Found. Get Given User's Citizens \u00b6 1 /v1/User/{id}/citizens Gets the citizens of the user with the provided id. The provided user must be a guardian. Status code responses are 200 Success, 400 Bad Request, 403 Forbidden and 404 Not Found. Get Given User's Guardians \u00b6 1 /v1/User/{id}/guardians Gets the guardians for the specific citizen corresponding to the provided id. Status code responses are 200 Success, 400 Bad Request, 403 Forbidden and 404 Not Found. Create Relation Between Guardian and Citizen \u00b6 1 /v1/User/{id}/citizens/{citizenId} POST request to add a relation between the authenticated user (guardian) and an existing citizen. Status code responses are 200 Success, 403 Forbidden and 404 Not Found.","title":"User Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/user_endpoints/#user-endpoints","text":"","title":"User Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/user_endpoints/#get-user","text":"1 /v1/User Find information about the currently authenticated user. Possible status code responses are 200 Success and 404 Not Found.","title":"Get User"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/user_endpoints/#get-given-user","text":"1 /v1/User/{id} Find information on the user with the username supplied as a url query parameter or the current user. Status code responses are 200 Success, 400 Bad Request, 403 Forbidden and 404 Not Found.","title":"Get Given User"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/user_endpoints/#update-given-user","text":"1 /v1/User/{id} Updates the user with the information in GirafRest.Models.DTOs.GirafUserDTO Example user PUT request body: 1 2 3 4 5 { \"role\": 1, \"username\": \"string\", \"displayName\": \"string\" } Status code responses are 200 Success, 400 Bad Request, 403 Forbidden, 404 Not Found and 409 Conflict.","title":"Update Given User"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/user_endpoints/#get-given-user-settings","text":"1 /v1/User/{id}/settings Get user-settings for the user with the specified Id Status code responses are 200 Success, 400 Bad Request, 403 Forbidden and 404 Not Found.","title":"Get Given User-settings"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/user_endpoints/#update-given-user-settings","text":"1 /v1/User/{id}/settings Updates the user settings for the user with the provided id. Example user-setting PUT request body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"orientation\": 1, \"completeMark\": 1, \"cancelMark\": 1, \"defaultTimer\": 1, \"timerSeconds\": 0, \"activitiesCount\": 0, \"theme\": 1, \"nrOfDaysToDisplay\": 0, \"greyScale\": true, \"lockTimerControl\": true, \"pictogramText\": true, \"showPopup\": true, \"weekDayColors\": [ { \"hexColor\": \"string\", \"day\": 1 } ] } Status code responses are 200 Success, 400 Bad Request, 403 Forbidden and 404 Not Found.","title":"Update Given User-settings"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/user_endpoints/#get-given-user-icon","text":"1 /v1/User/{id}/icon Endpoint for getting the UserIcon for a specific User- Possible status code responses are 200 Success and 404 Not found.","title":"Get Given User Icon"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/user_endpoints/#update-given-user-icon","text":"1 /v1/User/{id}/icon Sets the user icon of the given user. Status code responses are 200 Success, 400 Bad Request, 403 Forbidden and 404 Not Found.","title":"Update Given User Icon"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/user_endpoints/#delete-given-user-icon","text":"1 /v1/User/{id}/icon Deletes the user icon for a given user. Status code responses are 200 Success, 400 Bad Request, 403 Forbidden and 404 Not Found.","title":"Delete Given User Icon"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/user_endpoints/#get-given-raw-user-icon","text":"1 /v1/User/{id}/icon/raw Gets the raw user icon for a given user. Status code responses are 200 Success and 404 Not Found.","title":"Get Given Raw User Icon"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/user_endpoints/#get-given-users-citizens","text":"1 /v1/User/{id}/citizens Gets the citizens of the user with the provided id. The provided user must be a guardian. Status code responses are 200 Success, 400 Bad Request, 403 Forbidden and 404 Not Found.","title":"Get Given User's Citizens"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/user_endpoints/#get-given-users-guardians","text":"1 /v1/User/{id}/guardians Gets the guardians for the specific citizen corresponding to the provided id. Status code responses are 200 Success, 400 Bad Request, 403 Forbidden and 404 Not Found.","title":"Get Given User's Guardians"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/user_endpoints/#create-relation-between-guardian-and-citizen","text":"1 /v1/User/{id}/citizens/{citizenId} POST request to add a relation between the authenticated user (guardian) and an existing citizen. Status code responses are 200 Success, 403 Forbidden and 404 Not Found.","title":"Create Relation Between Guardian and Citizen"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/week_endpoints/","text":"Week Endpoints \u00b6 Get List of Week Schedules Given User ID \u00b6 1 /v2/User/{userId}/week Gets list of GirafRest.Models.DTOs.WeekDTO for all weeks belonging to the user with the provided id, days are not included. A User identifier is needed for the GirafRest.Models.GirafUser to get schedules. 1 /v2/User/{userId}/week Gets list of GirafRest.Models.DTOs.WeekDTO for all weeks belonging to the user with the provided id, days not are included. A User identifier is needed for the GirafRest.Models.GirafUser to get schedules. Possible status code responses are 200 Success, 403 Forbidden and 404 Not Found. Get a List of Week Schedules Given User ID \u00b6 1 /v1/User/{userId}/week Gets list of GirafRest.WeekNameDTO for all schedules belonging to the user with the provided id. A User identifier is needed for the GirafRest.Models.GirafUser to get schedules. Possible status code responses are 200 Success, 403 Forbidden and 404 Not Found. Get List of Week Schedules with Specified User ID and Week and Year \u00b6 1 /v1/User/{userId}/week/{weekYear}/{weekNumber} Gets the GirafRest.Models.DTOs.WeekDTO with the specified week number and year for the user with the given id. userId is the identifier of the GirafRest.Models.GirafUser to request schedule for. weekYear is the year of the week schedule to fetch. weekNumber is the week number of the week schedule to fetch. Possible status code responses are 200 Success, 403, Forbidden and 404 Not Found. Update Week Information \u00b6 1 /v1/User/{userId}/week/{weekYear}/{weekNumber} Updates the entire information of the week with the given year and week number. userId is the identifier of the GirafRest.Models.GirafUser to request schedule for. weekYear is the year of the week schedule to fetch. weekNumber is the week number of the week schedule to fetch. An example of a successful week PUT request: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 { \"thumbnail\": { \"id\": 0, \"lastEdit\": \"2020-10-19T18:49:26.879Z\", \"title\": \"string\", \"accessLevel\": 1, \"imageHash\": \"string\" }, \"name\": \"string\", \"days\": [ { \"day\": 1, \"activities\": [ { \"pictograms\": [ { \"id\": 0, \"lastEdit\": \"2020-10-19T18:49:26.879Z\", \"title\": \"string\", \"accessLevel\": 1, \"imageHash\": \"string\" } ], \"order\": 0, \"state\": 1, \"id\": 0, \"isChoiceBoard\": true, \"timer\": { \"startTime\": 0, \"progress\": 0, \"fullLength\": 0, \"paused\": true, \"key\": 0 } } ] } ] } Possible status code responses are 200 Success, 403, Forbidden and 404 Not Found. Delete a Week \u00b6 1 /v1/User/{userId}/week/{weekYear}/{weekNumber} Deletes all information for the entire week with the given year and week number. {userId} is the identifier of the GirafRest.Models.GirafUser to request a schedule for. {weekYear} is the year of the week schedule to fetch. {weekNumber} is the week number of the week schedule to fetch. Possible status code responses are 200 Success, 403, Forbidden and 404 Not Found.","title":"Week Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/week_endpoints/#week-endpoints","text":"","title":"Week Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/week_endpoints/#get-list-of-week-schedules-given-user-id","text":"1 /v2/User/{userId}/week Gets list of GirafRest.Models.DTOs.WeekDTO for all weeks belonging to the user with the provided id, days are not included. A User identifier is needed for the GirafRest.Models.GirafUser to get schedules. 1 /v2/User/{userId}/week Gets list of GirafRest.Models.DTOs.WeekDTO for all weeks belonging to the user with the provided id, days not are included. A User identifier is needed for the GirafRest.Models.GirafUser to get schedules. Possible status code responses are 200 Success, 403 Forbidden and 404 Not Found.","title":"Get List of Week Schedules Given User ID"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/week_endpoints/#get-a-list-of-week-schedules-given-user-id","text":"1 /v1/User/{userId}/week Gets list of GirafRest.WeekNameDTO for all schedules belonging to the user with the provided id. A User identifier is needed for the GirafRest.Models.GirafUser to get schedules. Possible status code responses are 200 Success, 403 Forbidden and 404 Not Found.","title":"Get a List of Week Schedules Given User ID"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/week_endpoints/#get-list-of-week-schedules-with-specified-user-id-and-week-and-year","text":"1 /v1/User/{userId}/week/{weekYear}/{weekNumber} Gets the GirafRest.Models.DTOs.WeekDTO with the specified week number and year for the user with the given id. userId is the identifier of the GirafRest.Models.GirafUser to request schedule for. weekYear is the year of the week schedule to fetch. weekNumber is the week number of the week schedule to fetch. Possible status code responses are 200 Success, 403, Forbidden and 404 Not Found.","title":"Get List of Week Schedules with Specified User ID and Week and Year"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/week_endpoints/#update-week-information","text":"1 /v1/User/{userId}/week/{weekYear}/{weekNumber} Updates the entire information of the week with the given year and week number. userId is the identifier of the GirafRest.Models.GirafUser to request schedule for. weekYear is the year of the week schedule to fetch. weekNumber is the week number of the week schedule to fetch. An example of a successful week PUT request: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 { \"thumbnail\": { \"id\": 0, \"lastEdit\": \"2020-10-19T18:49:26.879Z\", \"title\": \"string\", \"accessLevel\": 1, \"imageHash\": \"string\" }, \"name\": \"string\", \"days\": [ { \"day\": 1, \"activities\": [ { \"pictograms\": [ { \"id\": 0, \"lastEdit\": \"2020-10-19T18:49:26.879Z\", \"title\": \"string\", \"accessLevel\": 1, \"imageHash\": \"string\" } ], \"order\": 0, \"state\": 1, \"id\": 0, \"isChoiceBoard\": true, \"timer\": { \"startTime\": 0, \"progress\": 0, \"fullLength\": 0, \"paused\": true, \"key\": 0 } } ] } ] } Possible status code responses are 200 Success, 403, Forbidden and 404 Not Found.","title":"Update Week Information"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/week_endpoints/#delete-a-week","text":"1 /v1/User/{userId}/week/{weekYear}/{weekNumber} Deletes all information for the entire week with the given year and week number. {userId} is the identifier of the GirafRest.Models.GirafUser to request a schedule for. {weekYear} is the year of the week schedule to fetch. {weekNumber} is the week number of the week schedule to fetch. Possible status code responses are 200 Success, 403, Forbidden and 404 Not Found.","title":"Delete a Week"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/week_template_endpoints/","text":"WeekTemplate Endpoints \u00b6 Get Week Schedule Templates for the Authenticated User \u00b6 1 /v1/WeekTemplate Gets all schedule templates for the currently authenticated user. Available to all users. Possible status code responses are 200 Success, 403 forbidden and 404 Not Found status codes. Create Week Template \u00b6 1 /v1/WeekTemplate Creates new week template in the department of the given user. Available to Supers, Departments and Guardians. After successful execution, a new week template will be created. An example of a successful week template POST request: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 { \"thumbnail\": { \"id\": 0, \"lastEdit\": \"2020-10-19T16:44:58.607Z\", \"title\": \"string\", \"accessLevel\": 1, \"imageHash\": \"string\" }, \"name\": \"string\", \"days\": [ { \"day\": 1, \"activities\": [ { \"pictograms\": [ { \"id\": 0, \"lastEdit\": \"2020-10-19T16:44:58.607Z\", \"title\": \"string\", \"accessLevel\": 1, \"imageHash\": \"string\" } ], \"order\": 0, \"state\": 1, \"id\": 0, \"isChoiceBoard\": true, \"timer\": { \"startTime\": 0, \"progress\": 0, \"fullLength\": 0, \"paused\": true, \"key\": 0 } } ] } ], \"id\": 0 } Responses are 201 Success, 400 Bad Request, 403 Forbidden and 404 Not Found. Get Week Template for a Given User \u00b6 1 /v1/WeekTemplate/{id} Gets the week template with the specified id. Available to all users. An id integer is needed for the week template to fetch. Possible responses are 200 Success, 403 Forbidden and 404 Not Found. Update Week Template for a Given User \u00b6 1 /v1/WeekTemplate/{id} Overwrite the information of a week template. Available to all Supers, and to Departments and Guardians of the same department as the template. An id integer is needed for week template to overwrite. Possible responses are 200 Success, 400 Bad Request, 401 Unauthorized, 403 Forbidden and 404 Not Found. Delete Week Template for a Given User \u00b6 1 /v1/WeekTemplate/{id} Deletes the template of the given ID. Available to all Supers, and to Departments and Guardians of the same department as the template. An id integer is needed for the week template to delete. Possible responses are 200 Success, 401 Unauthorized, 403 Forbidden and 404 Not Found.","title":"WeekTemplate Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/week_template_endpoints/#weektemplate-endpoints","text":"","title":"WeekTemplate Endpoints"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/week_template_endpoints/#get-week-schedule-templates-for-the-authenticated-user","text":"1 /v1/WeekTemplate Gets all schedule templates for the currently authenticated user. Available to all users. Possible status code responses are 200 Success, 403 forbidden and 404 Not Found status codes.","title":"Get Week Schedule Templates for the Authenticated User"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/week_template_endpoints/#create-week-template","text":"1 /v1/WeekTemplate Creates new week template in the department of the given user. Available to Supers, Departments and Guardians. After successful execution, a new week template will be created. An example of a successful week template POST request: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 { \"thumbnail\": { \"id\": 0, \"lastEdit\": \"2020-10-19T16:44:58.607Z\", \"title\": \"string\", \"accessLevel\": 1, \"imageHash\": \"string\" }, \"name\": \"string\", \"days\": [ { \"day\": 1, \"activities\": [ { \"pictograms\": [ { \"id\": 0, \"lastEdit\": \"2020-10-19T16:44:58.607Z\", \"title\": \"string\", \"accessLevel\": 1, \"imageHash\": \"string\" } ], \"order\": 0, \"state\": 1, \"id\": 0, \"isChoiceBoard\": true, \"timer\": { \"startTime\": 0, \"progress\": 0, \"fullLength\": 0, \"paused\": true, \"key\": 0 } } ] } ], \"id\": 0 } Responses are 201 Success, 400 Bad Request, 403 Forbidden and 404 Not Found.","title":"Create Week Template"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/week_template_endpoints/#get-week-template-for-a-given-user","text":"1 /v1/WeekTemplate/{id} Gets the week template with the specified id. Available to all users. An id integer is needed for the week template to fetch. Possible responses are 200 Success, 403 Forbidden and 404 Not Found.","title":"Get Week Template for a Given User"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/week_template_endpoints/#update-week-template-for-a-given-user","text":"1 /v1/WeekTemplate/{id} Overwrite the information of a week template. Available to all Supers, and to Departments and Guardians of the same department as the template. An id integer is needed for week template to overwrite. Possible responses are 200 Success, 400 Bad Request, 401 Unauthorized, 403 Forbidden and 404 Not Found.","title":"Update Week Template for a Given User"},{"location":"Legacy/weekplanner/Web_API/Endpoints_and_Controllers/Endpoints/week_template_endpoints/#delete-week-template-for-a-given-user","text":"1 /v1/WeekTemplate/{id} Deletes the template of the given ID. Available to all Supers, and to Departments and Guardians of the same department as the template. An id integer is needed for the week template to delete. Possible responses are 200 Success, 401 Unauthorized, 403 Forbidden and 404 Not Found.","title":"Delete Week Template for a Given User"},{"location":"Legal/","text":"Overview \u00b6 This section contains legal, privacy, and security relevant information about the GIRAF project. Privacy Policy The privacy policy for the GIRAF project. GDPR Information regarding GDPR for the GIRAF project. Risk Assessment A risk assessment for the GIRAF project, describing risks and their potential impact. Licences The licenses used to build the applications in the GIRAF platform.","title":"Overview"},{"location":"Legal/#overview","text":"This section contains legal, privacy, and security relevant information about the GIRAF project. Privacy Policy The privacy policy for the GIRAF project. GDPR Information regarding GDPR for the GIRAF project. Risk Assessment A risk assessment for the GIRAF project, describing risks and their potential impact. Licences The licenses used to build the applications in the GIRAF platform.","title":"Overview"},{"location":"Legal/licences/","text":"Licenses \u00b6 In this section of the Wiki you will find the licenses used to build the applications in the GIRAF platform. Pictogram Licenses \u00b6 All pictograms used within the Weekplanner application are from the ARAASAC symbol collection. This collection has been translated to Danish by the communication centre at Hilleroed Kommune. Download from Picto Selector (accessed at 17/04/2020) License available at Creative Common's License (accessed at 17/04/2020) Sound Licenses \u00b6 dingSound.wav file used for the timer in the Weekplanner-application: Download from MediaFire (accessed at 30/04/2020) License available at MediaFire License (accessed at 30/04/2020) Icon Licenses \u00b6 FallBackImage icon used in the Weekplanner-application (available in the Wiki-repository under GIRAF-icons): Download from Font Awesome (accessed at 20/04/2020) License available at Font Awesome Free License (accessed at 20/04/2020) Information icon used in the Weekplanner-application (available in the Wiki-repository under GIRAF-icons): Download from Font Awesome (accessed at 05/05/2020) License available at Font Awesome Free License (accessed at 05/05/2020) Folder icon (folder.svg) and AddFolder icon (addFolder.svg): Developed by a development group in 2020S1 Released under Public Domain LoadingSpinner icon (loadingSpinner.svg): Created by the PO group in 2020e According to the report \"Product Owner of Giraf\" written by group SW610F19 in the GIRAF 2019 project all icons are from Font Awesome with exception of the icons changeToCitizen and changeToGuardian which are developed by themselves. See p. 36 l. 1 in the report Download from Font Awesome by searching the icons file-name with the exceptions of gallery.png (search for \"images\"), cancel.png (\"close\") and addUser (\"user\") (accessed at 20/04/2020) License available at Font Awesome Free License (accessed at 17/04/2020) Other Licenses \u00b6 Quicksand-font used in the Weekplanner-application: Download from Google Fonts (accessed at 17/04/2020) License available at SIL Open Font License (accessed at 17/04/2020)","title":"Licenses"},{"location":"Legal/licences/#licenses","text":"In this section of the Wiki you will find the licenses used to build the applications in the GIRAF platform.","title":"Licenses"},{"location":"Legal/licences/#pictogram-licenses","text":"All pictograms used within the Weekplanner application are from the ARAASAC symbol collection. This collection has been translated to Danish by the communication centre at Hilleroed Kommune. Download from Picto Selector (accessed at 17/04/2020) License available at Creative Common's License (accessed at 17/04/2020)","title":"Pictogram Licenses"},{"location":"Legal/licences/#sound-licenses","text":"dingSound.wav file used for the timer in the Weekplanner-application: Download from MediaFire (accessed at 30/04/2020) License available at MediaFire License (accessed at 30/04/2020)","title":"Sound Licenses"},{"location":"Legal/licences/#icon-licenses","text":"FallBackImage icon used in the Weekplanner-application (available in the Wiki-repository under GIRAF-icons): Download from Font Awesome (accessed at 20/04/2020) License available at Font Awesome Free License (accessed at 20/04/2020) Information icon used in the Weekplanner-application (available in the Wiki-repository under GIRAF-icons): Download from Font Awesome (accessed at 05/05/2020) License available at Font Awesome Free License (accessed at 05/05/2020) Folder icon (folder.svg) and AddFolder icon (addFolder.svg): Developed by a development group in 2020S1 Released under Public Domain LoadingSpinner icon (loadingSpinner.svg): Created by the PO group in 2020e According to the report \"Product Owner of Giraf\" written by group SW610F19 in the GIRAF 2019 project all icons are from Font Awesome with exception of the icons changeToCitizen and changeToGuardian which are developed by themselves. See p. 36 l. 1 in the report Download from Font Awesome by searching the icons file-name with the exceptions of gallery.png (search for \"images\"), cancel.png (\"close\") and addUser (\"user\") (accessed at 20/04/2020) License available at Font Awesome Free License (accessed at 17/04/2020)","title":"Icon Licenses"},{"location":"Legal/licences/#other-licenses","text":"Quicksand-font used in the Weekplanner-application: Download from Google Fonts (accessed at 17/04/2020) License available at SIL Open Font License (accessed at 17/04/2020)","title":"Other Licenses"},{"location":"Legal/privacy_policy/","text":"Privacy Policy \u00b6 Last updated: 08/03/2017 We develop GIRAF and its associated apps. This page informs you of our policies regarding the collection, use and disclosure of Personal Information we receive from users of the GIRAF apps(\u201cthe apps\u201d). We use your Personal Information only for providing and improving the apps. By using the apps, you agree to the collection and use of information in accordance with this policy. Information Collection And Use \u00b6 While using GIRAF, we may ask you to provide us with certain personally identifiable information that can be used to contact or identify you. This personally identifiable information only includes your name. Log Data \u00b6 We use the Google Analytics service to get crash reports and other errors report data sent to us. This data does not include any personal data, apart from the previously mentioned personally identifiable data. Permissions \u00b6 RECORD_AUDIO: This permission is used in \u201cStemmespillet\u201d (Voice Game), as the user\u2019s voice is the main mechanic of the game. CAMERA: This permission is used in \u201cPiktotegner\u201d (Pictocreator), as it is possible to make pictograms with pictures taken through the camera. It is also used in profile creation where it is possible to make a profile picture with a picture taken through the app with the camera. READ_CONTACTS: This permission is outdated and will be removed in the nearest future. Security \u00b6 The security of your Personal Information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your Personal Information, we cannot guarantee its absolute security. Changes To This Privacy Policy \u00b6 This Privacy Policy is effective as of 08/03/2017 and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. Your continued use of the Service after we post any modifications to the Privacy Policy on this page will constitute your acknowledgment of the modifications and your consent to abide and be bound by the modified Privacy Policy. Contact Us \u00b6 If you have any questions about this Privacy Policy, please contact us.","title":"Privacy Policy"},{"location":"Legal/privacy_policy/#privacy-policy","text":"Last updated: 08/03/2017 We develop GIRAF and its associated apps. This page informs you of our policies regarding the collection, use and disclosure of Personal Information we receive from users of the GIRAF apps(\u201cthe apps\u201d). We use your Personal Information only for providing and improving the apps. By using the apps, you agree to the collection and use of information in accordance with this policy.","title":"Privacy Policy"},{"location":"Legal/privacy_policy/#information-collection-and-use","text":"While using GIRAF, we may ask you to provide us with certain personally identifiable information that can be used to contact or identify you. This personally identifiable information only includes your name.","title":"Information Collection And Use"},{"location":"Legal/privacy_policy/#log-data","text":"We use the Google Analytics service to get crash reports and other errors report data sent to us. This data does not include any personal data, apart from the previously mentioned personally identifiable data.","title":"Log Data"},{"location":"Legal/privacy_policy/#permissions","text":"RECORD_AUDIO: This permission is used in \u201cStemmespillet\u201d (Voice Game), as the user\u2019s voice is the main mechanic of the game. CAMERA: This permission is used in \u201cPiktotegner\u201d (Pictocreator), as it is possible to make pictograms with pictures taken through the camera. It is also used in profile creation where it is possible to make a profile picture with a picture taken through the app with the camera. READ_CONTACTS: This permission is outdated and will be removed in the nearest future.","title":"Permissions"},{"location":"Legal/privacy_policy/#security","text":"The security of your Personal Information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your Personal Information, we cannot guarantee its absolute security.","title":"Security"},{"location":"Legal/privacy_policy/#changes-to-this-privacy-policy","text":"This Privacy Policy is effective as of 08/03/2017 and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. Your continued use of the Service after we post any modifications to the Privacy Policy on this page will constitute your acknowledgment of the modifications and your consent to abide and be bound by the modified Privacy Policy.","title":"Changes To This Privacy Policy"},{"location":"Legal/privacy_policy/#contact-us","text":"If you have any questions about this Privacy Policy, please contact us.","title":"Contact Us"},{"location":"Legal/risk_assessment/","text":"Risk assessment \u00b6 From issue 171 . The risk assessment can be found here: PDF The risk assessment has been based on the guidance of Datatilsynet. Specifically these documents: Datatilsynets vejledning Datatilsynets artikel 25 og 32 Template for risk assessment Additionally, as inspiration, ENISA has provided a guide which has also been utilized: ENISA threat taxonomy","title":"Risk assessment"},{"location":"Legal/risk_assessment/#risk-assessment","text":"From issue 171 . The risk assessment can be found here: PDF The risk assessment has been based on the guidance of Datatilsynet. Specifically these documents: Datatilsynets vejledning Datatilsynets artikel 25 og 32 Template for risk assessment Additionally, as inspiration, ENISA has provided a guide which has also been utilized: ENISA threat taxonomy","title":"Risk assessment"},{"location":"Legal/GDPR/","text":"Overview \u00b6 This section contains information regarding GDPR for the GIRAF project. GDPR Regulation Overall information about the GDPR regulation, and how the GIRAF project meets it. Consent Information and Samtykkeerkl\u00e6ring Information regarding consent given to the GIRAF project, and the actual statement for consent used. Information about the Processing Activity Record Information about the contents and use of the Processing Activity Record, and the record itself.","title":"Overview"},{"location":"Legal/GDPR/#overview","text":"This section contains information regarding GDPR for the GIRAF project. GDPR Regulation Overall information about the GDPR regulation, and how the GIRAF project meets it. Consent Information and Samtykkeerkl\u00e6ring Information regarding consent given to the GIRAF project, and the actual statement for consent used. Information about the Processing Activity Record Information about the contents and use of the Processing Activity Record, and the record itself.","title":"Overview"},{"location":"Legal/GDPR/consent/","text":"Samtykkeerkl\u00e6ring \u00b6 Anmodning om samtykke til indsamling af almindelig personlig information. Hvis data subjektet er under 18 \u00e5r, er det vigtigt at samtykkeerkl\u00e6ring indhentes fra den p\u00e5g\u00e6ldendes v\u00e6rge. Oplysninger om vores behandling af dine personoplysninger mv. 1. Vi er den dataansvarlige \u2013 s\u00e5dan kontakter du os \u00b6 Girafs Venner er dataansvarlig for behandlingen af de personoplysninger, som vi har modtaget om dig. Du finder vores kontaktoplysninger nedenfor. Navn: Girafs Venner Adresse: Selma Lagerl\u00f8fs vej 300, 9220 Aalborg \u00d8st CVR-nr.: 40519025 Telefon: 40 89 21 56 Mail: ulrik@cs.aau.dk 2. Form\u00e5lene med og retsgrundlaget for behandlingen af dine personoplysninger \u00b6 Vi behandler dine personoplysninger til f\u00f8lgende form\u00e5l: Form\u00e5let med behandlingen af personlige oplysninger er udelukkende at tilvejebringe et kommunikationsv\u00e6rkt\u00f8j til autistiske b\u00f8rn, og medarbejdere til den udbudte institution. Behandlingen er s\u00e5ledes kun med form\u00e5l at skabe en personaliseret interaktion, mellem systemet og det enkelte barn, ved at have en systembruger. Legitime interesser, der forf\u00f8lges med behandlingen. Som n\u00e6vnt ovenfor sker vores behandling af dine personoplysninger p\u00e5 baggrund af interesseafvejningsreglen i databeskyttelsesforordningens artikel 6, stk. 1, litra f. De legitime interesser, der begrunder behandlingen, er at hj\u00e6lpe autistiske b\u00f8rn med at kommunikere og skabe struktur i hverdagen gennem et kommunikationsv\u00e6rkt\u00f8j. 3. Kategorier af personoplysninger \u00b6 Vi behandler f\u00f8lgende kategorier af personoplysninger om dig: Identifikationsoplysninger: Almindelige personoplysninger 4. Hvor dine personoplysninger stammer fra \u00b6 Personoplysningerne stammer fra registreringen af brugeren i applikationen. 5. Opbevaring af dine personoplysninger \u00b6 Personlige oplysninger slettes 1 \u00e5r efter, at brugeren er erkl\u00e6ret inaktiv, hvilket afh\u00e6ngigt af institutionen enten kan v\u00e6re et aktivt valg foretaget af institutionen eller som resultat af manglende anvendelse af systemet. 6. Retten til at tr\u00e6kke samtykke tilbage \u00b6 Du har til enhver tid ret til at tr\u00e6kke dit samtykke tilbage. Dette kan du g\u00f8re ved at kontakte os p\u00e5 de kontaktoplysninger, der fremg\u00e5r ovenfor i punkt 1. Hvis du v\u00e6lger at tr\u00e6kke dit samtykke tilbage, p\u00e5virker det ikke lovligheden af vores behandling af dine personoplysninger p\u00e5 baggrund af dit tidligere meddelte samtykke og op til tidspunktet for tilbagetr\u00e6kningen. Hvis du tilbagetr\u00e6kker dit samtykke, har det derfor f\u00f8rst virkning fra dette tidspunkt. 7. Dine rettigheder \u00b6 Du har efter databeskyttelsesforordningen en r\u00e6kke rettigheder i forhold til vores behandling af oplysninger om dig. Hvis du vil g\u00f8re brug af dine rettigheder skal du kontakte os. Ret til at se oplysninger (indsigtsret) Du har ret til at f\u00e5 indsigt i de oplysninger, som vi behandler om dig, samt en r\u00e6kke yderligere oplysninger. Ret til berigtigelse (rettelse). Du har ret til at f\u00e5 urigtige oplysninger om dig selv rettet. Ret til sletning. I s\u00e6rlige tilf\u00e6lde har du ret til at f\u00e5 slettet oplysninger om dig, inden tidspunktet for vores almindelige generelle sletning indtr\u00e6ffer. Ret til begr\u00e6nsning af behandling. Du har visse tilf\u00e6lde ret til at f\u00e5 behandlingen af dine personoplysninger begr\u00e6nset. Hvis du har ret til at f\u00e5 begr\u00e6nset behandlingen, m\u00e5 vi fremover kun behandle oplysningerne \u2013 bortset fra opbevaring \u2013 med dit samtykke, eller med henblik p\u00e5 at retskrav kan fastl\u00e6gges, g\u00f8res g\u00e6ldende eller forsvares, eller for at beskytte en person eller vigtige samfundsinteresser. Ret til indsigelse. Du har i visse tilf\u00e6lde ret til at g\u00f8re indsigelse mod vores eller lovlige behandling af dine personoplysninger. Du kan ogs\u00e5 g\u00f8re indsigelse mod behandling af dine oplysninger til direkte markedsf\u00f8ring. Ret til at transmittere oplysninger (dataportabilitet). Du har i visse tilf\u00e6lde ret til at modtage dine personoplysninger i et struktureret, almindeligt anvendt og maskinl\u00e6sbart format samt at f\u00e5 overf\u00f8rt disse personoplysninger fra \u00e9n dataansvarlig til en anden uden hindring. Du kan l\u00e6se mere om dine rettigheder i Datatilsynets vejledning om de registreredes rettigheder, som du finder p\u00e5 www.datatilsynet.dk . 8. Klage til Datatilsynet \u00b6 Du har ret til at indgive en klage til Datatilsynet, hvis du er utilfreds med den m\u00e5de, vi behandler dine personoplysninger p\u00e5. Du finder Datatilsynets kontaktoplysninger p\u00e5 www.datatilsynet.dk .","title":"Samtykkeerkl\u00e6ring"},{"location":"Legal/GDPR/consent/#samtykkeerklring","text":"Anmodning om samtykke til indsamling af almindelig personlig information. Hvis data subjektet er under 18 \u00e5r, er det vigtigt at samtykkeerkl\u00e6ring indhentes fra den p\u00e5g\u00e6ldendes v\u00e6rge. Oplysninger om vores behandling af dine personoplysninger mv.","title":"Samtykkeerkl\u00e6ring"},{"location":"Legal/GDPR/consent/#1-vi-er-den-dataansvarlige-sadan-kontakter-du-os","text":"Girafs Venner er dataansvarlig for behandlingen af de personoplysninger, som vi har modtaget om dig. Du finder vores kontaktoplysninger nedenfor. Navn: Girafs Venner Adresse: Selma Lagerl\u00f8fs vej 300, 9220 Aalborg \u00d8st CVR-nr.: 40519025 Telefon: 40 89 21 56 Mail: ulrik@cs.aau.dk","title":"1. Vi er den dataansvarlige \u2013 s\u00e5dan kontakter du os"},{"location":"Legal/GDPR/consent/#2-formalene-med-og-retsgrundlaget-for-behandlingen-af-dine-personoplysninger","text":"Vi behandler dine personoplysninger til f\u00f8lgende form\u00e5l: Form\u00e5let med behandlingen af personlige oplysninger er udelukkende at tilvejebringe et kommunikationsv\u00e6rkt\u00f8j til autistiske b\u00f8rn, og medarbejdere til den udbudte institution. Behandlingen er s\u00e5ledes kun med form\u00e5l at skabe en personaliseret interaktion, mellem systemet og det enkelte barn, ved at have en systembruger. Legitime interesser, der forf\u00f8lges med behandlingen. Som n\u00e6vnt ovenfor sker vores behandling af dine personoplysninger p\u00e5 baggrund af interesseafvejningsreglen i databeskyttelsesforordningens artikel 6, stk. 1, litra f. De legitime interesser, der begrunder behandlingen, er at hj\u00e6lpe autistiske b\u00f8rn med at kommunikere og skabe struktur i hverdagen gennem et kommunikationsv\u00e6rkt\u00f8j.","title":"2. Form\u00e5lene med og retsgrundlaget for behandlingen af dine personoplysninger"},{"location":"Legal/GDPR/consent/#3-kategorier-af-personoplysninger","text":"Vi behandler f\u00f8lgende kategorier af personoplysninger om dig: Identifikationsoplysninger: Almindelige personoplysninger","title":"3. Kategorier af personoplysninger"},{"location":"Legal/GDPR/consent/#4-hvor-dine-personoplysninger-stammer-fra","text":"Personoplysningerne stammer fra registreringen af brugeren i applikationen.","title":"4. Hvor dine personoplysninger stammer fra"},{"location":"Legal/GDPR/consent/#5-opbevaring-af-dine-personoplysninger","text":"Personlige oplysninger slettes 1 \u00e5r efter, at brugeren er erkl\u00e6ret inaktiv, hvilket afh\u00e6ngigt af institutionen enten kan v\u00e6re et aktivt valg foretaget af institutionen eller som resultat af manglende anvendelse af systemet.","title":"5. Opbevaring af dine personoplysninger"},{"location":"Legal/GDPR/consent/#6-retten-til-at-trkke-samtykke-tilbage","text":"Du har til enhver tid ret til at tr\u00e6kke dit samtykke tilbage. Dette kan du g\u00f8re ved at kontakte os p\u00e5 de kontaktoplysninger, der fremg\u00e5r ovenfor i punkt 1. Hvis du v\u00e6lger at tr\u00e6kke dit samtykke tilbage, p\u00e5virker det ikke lovligheden af vores behandling af dine personoplysninger p\u00e5 baggrund af dit tidligere meddelte samtykke og op til tidspunktet for tilbagetr\u00e6kningen. Hvis du tilbagetr\u00e6kker dit samtykke, har det derfor f\u00f8rst virkning fra dette tidspunkt.","title":"6. Retten til at tr\u00e6kke samtykke tilbage"},{"location":"Legal/GDPR/consent/#7-dine-rettigheder","text":"Du har efter databeskyttelsesforordningen en r\u00e6kke rettigheder i forhold til vores behandling af oplysninger om dig. Hvis du vil g\u00f8re brug af dine rettigheder skal du kontakte os. Ret til at se oplysninger (indsigtsret) Du har ret til at f\u00e5 indsigt i de oplysninger, som vi behandler om dig, samt en r\u00e6kke yderligere oplysninger. Ret til berigtigelse (rettelse). Du har ret til at f\u00e5 urigtige oplysninger om dig selv rettet. Ret til sletning. I s\u00e6rlige tilf\u00e6lde har du ret til at f\u00e5 slettet oplysninger om dig, inden tidspunktet for vores almindelige generelle sletning indtr\u00e6ffer. Ret til begr\u00e6nsning af behandling. Du har visse tilf\u00e6lde ret til at f\u00e5 behandlingen af dine personoplysninger begr\u00e6nset. Hvis du har ret til at f\u00e5 begr\u00e6nset behandlingen, m\u00e5 vi fremover kun behandle oplysningerne \u2013 bortset fra opbevaring \u2013 med dit samtykke, eller med henblik p\u00e5 at retskrav kan fastl\u00e6gges, g\u00f8res g\u00e6ldende eller forsvares, eller for at beskytte en person eller vigtige samfundsinteresser. Ret til indsigelse. Du har i visse tilf\u00e6lde ret til at g\u00f8re indsigelse mod vores eller lovlige behandling af dine personoplysninger. Du kan ogs\u00e5 g\u00f8re indsigelse mod behandling af dine oplysninger til direkte markedsf\u00f8ring. Ret til at transmittere oplysninger (dataportabilitet). Du har i visse tilf\u00e6lde ret til at modtage dine personoplysninger i et struktureret, almindeligt anvendt og maskinl\u00e6sbart format samt at f\u00e5 overf\u00f8rt disse personoplysninger fra \u00e9n dataansvarlig til en anden uden hindring. Du kan l\u00e6se mere om dine rettigheder i Datatilsynets vejledning om de registreredes rettigheder, som du finder p\u00e5 www.datatilsynet.dk .","title":"7. Dine rettigheder"},{"location":"Legal/GDPR/consent/#8-klage-til-datatilsynet","text":"Du har ret til at indgive en klage til Datatilsynet, hvis du er utilfreds med den m\u00e5de, vi behandler dine personoplysninger p\u00e5. Du finder Datatilsynets kontaktoplysninger p\u00e5 www.datatilsynet.dk .","title":"8. Klage til Datatilsynet"},{"location":"Legal/GDPR/consent_information/","text":"Consent Information \u00b6 Overview \u00b6 The purpose of this document, is to detail two subjects mentioned in the GDPR Regulation document, namely the Legal Processing and the Legal Rights of the Data Subject. The essence of these two subject, are that in order to process personal information of the data subject, there has to be lawful basis for doing so, such as the establishment of a consent contract, and also that the data subject has to be informed of its legal rights. Both of these are required to be fulfilled prior to committing personal information to the system, and therefore it is optimal to ensure that both of these are ensured at the same time, such as during sign-up. Requirements and maintenance \u00b6 These requirements can be fulfilled by means of a consent-contract, with information about: Contact information about the data-controller Purpose of processing personal information Categories of personal information How personal information is retrieved How long person information is contained in the system and how it is deleted General rights of the data-subject This document is based on a template provided by Datatilsynet, with intention of being visible to the data-subject at any time. This is also the reason why it is provided in Danish, such that it can be integrated as part of the application. The consent contract should be continuously maintained, which means updating it given any changes to the processing of personal information, which would also require a new acceptance of the consent contract from the data-subject. The changes that would trigger an update, are equivalent to those described in the document Processing Activity Record Information . The consent document \u00b6 The consent-contract is provided as a separate document .","title":"Consent Information"},{"location":"Legal/GDPR/consent_information/#consent-information","text":"","title":"Consent Information"},{"location":"Legal/GDPR/consent_information/#overview","text":"The purpose of this document, is to detail two subjects mentioned in the GDPR Regulation document, namely the Legal Processing and the Legal Rights of the Data Subject. The essence of these two subject, are that in order to process personal information of the data subject, there has to be lawful basis for doing so, such as the establishment of a consent contract, and also that the data subject has to be informed of its legal rights. Both of these are required to be fulfilled prior to committing personal information to the system, and therefore it is optimal to ensure that both of these are ensured at the same time, such as during sign-up.","title":"Overview"},{"location":"Legal/GDPR/consent_information/#requirements-and-maintenance","text":"These requirements can be fulfilled by means of a consent-contract, with information about: Contact information about the data-controller Purpose of processing personal information Categories of personal information How personal information is retrieved How long person information is contained in the system and how it is deleted General rights of the data-subject This document is based on a template provided by Datatilsynet, with intention of being visible to the data-subject at any time. This is also the reason why it is provided in Danish, such that it can be integrated as part of the application. The consent contract should be continuously maintained, which means updating it given any changes to the processing of personal information, which would also require a new acceptance of the consent contract from the data-subject. The changes that would trigger an update, are equivalent to those described in the document Processing Activity Record Information .","title":"Requirements and maintenance"},{"location":"Legal/GDPR/consent_information/#the-consent-document","text":"The consent-contract is provided as a separate document .","title":"The consent document"},{"location":"Legal/GDPR/gdpr_regulation/","text":"GDPR Regulation \u00b6 Sources \u00b6 gdpr.dk datatilsynet.dk en.wikipedia.com What is the GDPR regulation \u00b6 GDPR stands for General Protection Regulation and it is a collection of laws on data protection and privacy introduced in the European Union (EU), that applies to every entity that collects and processes information on any subject within the EU. An entity might be any organization, either public or private, and a subject might be a citizen of any of the member countries of the EU. The law is in place to protect people's fundamental right and freedom, namely their right to protect their personal information. The GDPR laws requires careful consideration with regards to information technology (IT) systems, which oftentimes revolves around user information. The GIRAF project is no exception to this, and it is therefore required to take into account what data is collected, stored and how it is used. The GDPR is concerned with data that is categorised as personal information. Definitions and concepts \u00b6 Personal data: Is information that identifies an individual. Processing: Any one or set of operations which is performed on personal data, whether or not by automated means. Data subject: Is a person based in the EU. Data controller: Is a natural, legal person, organization, public authority, agency or other body, which determines the purposes and means of the processing of personal data, and should be able to prove it. Processor: Natural or legal person (such as an organization), public authority, agency or other body, which processes data on behalf of a data controller. Types of personal information \u00b6 GDPR deals with two types of personal information, ordinary and sensitive. Treatment of sensitive data is more limited in terms of the law. Ordinary personal information Sensitive personal information Name Race and ethnicity Address Political beliefs Identification number Religious or philosophical beliefs Location data Union membership Online identification Genetic data Economic Biometric data for identification of a person Taxes Health information Debt Sexual relationship and sexual orientation Social issues Sick days Family Home Car Exam Application CV Work The essential principles \u00b6 Processing of personal information should be performed in a legal, fair and transparent way. Only sufficient, relevant and limited to what is necessary in relation to the purpose to which they are addressed (\u2018data minimization\u2019). Be correct. There needs to be taken reasonable steps to make sure that the personal information processed is correct information, that is, it is required that the data controller ensures the correctness of the data with regards to the purpose it is processed in relation to and any incorrect information should be corrected or deleted. There needs to be taken reasonable steps to ensure the integrity, confidentiality and security of personal information. For example, it needs to be ensured that correctness is persisted through time, that no unauthorized access to the data should be possible, and that all personal data is processed by adequately secure means. Such steps should be ensured by the data controller, by measures at both the organizational and technical level. The personal information should not be stored longer than what is necessary to fulfill its purpose, and afterwards it should be deleted or anonymized such that it is simply information. Legal processing \u00b6 There has to be legal basis, to process personal information. Lawful purposes for doing so, are as follows: If the data subject has given consent to the processing of his or her personal data To fulfill contractual obligations with a data subject, or for tasks at the request of a data subject who is in the process of entering into a contract To comply with a data controller's legal obligations To protect the vital interests of a data subject or another individual To perform a task in the public interest or in official authority For the legitimate interests of a data controller or a third party, unless these interests are overridden by interests of the data subject or her or his rights according to the Charter of Fundamental Rights (especially in the case of children) For the legitimate interests of a data controller or a third party, unless these interests are overridden by interests of the data subject or her or his rights according to the Charter of Fundamental Rights (especially in the case of children). Legal rights of the data subject \u00b6 The GDPR regulation provides the data subject with an array of legal rights, related to the data and processing thereof. The data subject must be informed about: The extent of the data collection Transfer of data to any third-party and/or parties outside the EU Automated decision-making, based solely on algorithmic basis Their privacy rights under GDPR, including right to Revoke consent Access to view the data, and an overview Data portability, such as gaining access to a portable copy of the data, and in some cases to request that personal data be transferred from one data controller to another Be forgotten, i.e. to have personal data deleted and erased, if conditions mentioned in the regulation is fulfilled Correction of wrong personal information Restrict the processing of personal data, if a number of conditions is met Contest being subject to automated decision-making, based solely on automated processing, such as profiling File complaints with a Data Protection Authority (DPA) over processing of personal information, with the only exceptions to this right being that: Legal or official authority is being carried out \u2018Legitimate interest\u2019, where the organisation needs to process data in order to provide the data subject with a service they signed up for A task being carried out for public interest Children specific regulations \u00b6 If the child is under the age of 16, it is only legal to collect and process data if a parent with custody of that child, gives the permission to do so. It falls under the responsibility of the data responsible to ensure with reasonable effort that this is done. The individual countries might enact national regulation that puts this age down to 13 years. Controller and Processor \u00b6 The data controller might be a company that seeks to collect and process personal information, while a data processor might be another company that provides a cloud-service that the data controller company wishes to use. Data protection principles and measures must be designed intro the business processes for products and services. This includes measures such as pseudonymisation and high level of privacy by default (Article 25). This is the responsibility of the data controller, even if processing is carried out by a data processor. The data controller keeps internal record of the processing of personal data. This will give an overview of the treatments of the data that is initiated. Such an overview is a necessary prerequisite to fulfill a number of obligations, such as: considerations on what information to process; handling insights requests and review of breaches of personal data security. This is an internal document that data protection agency can request at any time. A report ( ENISA on privacy and data protection by design (January 12, 2015) ) specifies what needs to be done to achieve privacy and data protection by default. For instance, encryption and decryption operations must be carried out locally, and not be a remote service as the keys and data must remain in the power of the data owner. Data protection impact assessment have to be conducted when specific risks occur to the rights and freedoms of data subjects. Prior approval of the data protection authorities is required for high risks (Article 35). Pseudonymisation: Is a required process for stored data that transforms personal data in such a way that the resulting data cannot be attributed to a specific data subject, without the use of additional information. An example of such, is encryption that renders the original data unintelligible and the process cannot be reversed without access to the correct decryption key. Records of processing activities: Must be maintained, and include purposes of the processing, categories involved and envisaged time limits. Security of personal data: In the case of a breach, is the data controller is under a legal obligation to notify the supervisory authority within 72 hours of becoming aware of it. Further does the data subject(s) have to be informed if there is a high risk of an adverse impact. Data protection officer (DPO): If processing is carried out by either a public authority, or if processing operations involve regular, systematic and large scale monitoring of data subjects, or if it occurs within special categories, such as a criminal system, a data protection officer has to be appointed. The DPO is expected to have expert knowledge of data protection law and practices, and should be designated to assist the controller in monitoring their internal compliance with the regulation. A designated DPO can be either internal to the organization or external, as long as there is no conflict of interest. Organizations outside of EU, must appoint an EU-based person as representative for their GDPR obligations. The DPO is further expected to be proficient at managing IT processes, data security including dealing with cyber attacks and other business continuity processes (prevention and recovery) to deal with threats to the organization. Meeting the GDPR regulation for the GIRAF project \u00b6 The examination of the GDPR regulation, provides a basis for analysing it in the context of the GIRAF project, which is performed in this section. Organisational Structure and Definitions \u00b6 Initially a set of definitions has to be established. The purpose of the GIRAF project is to provide a tablet environment, including a set of tools, to assist autistic children with little or no verbal communication. The system developed, is based on a client-server architecture, meaning that for the system to work properly, an application, in this setup termed the client, has to be installed on a tablet, which communicates with a service installed on a server. The system is built such that the client in itself does not store information, but instead delegates data storage to the service on the server, thereby resembling a thin client, and which further establishes that whatever entity provides the service(s) on one or more servers is in control of the data. Depending on the particular circumstances under which the GIRAF system is deployed, including the specific business model and organisational structure, there might be a couple different ways to define the data controller and processor: As the GIRAF project is developed by open-source means, one choice would be for an institution wishing to provide this for its citizens, to acquire the necessary system files and deploy it on their own servers along with the applications on their tablets. This would not change the requirements of the institution to uphold the GDPR regulation to its citizens, but it would possibly be the easiest business model, as the institution would be able to internally to determine the budget allocated to uphold any IT-services, and it would further not require any professional dependencies or expenses with regards to the development of the system, thereby contrasting with the following alternatives. Another choice would be to establish an organisation with commercial intent, whether for-profit or non-profit, which would then provide the GIRAF system services, and thereby operate it as a Software-as-a-Service (SaaS). Those involved in such an organisation, could be anyone knowledgeable about the GIRAF system, such as a subset of developers. The reason for this to be required if GIRAF was to be provided as a SaaS, is two-fold; firstly, to operate a SaaS, an entity must take the responsibility as the data controller and processor, along with any professional obligations, such as dealing with expenses. Secondly, it is unlikely that Giraf is allowed to use the current AAU ITS servers for commercial purposes, and/or delegating the responsibilities of GDPR regulation to that department. Based on these observations, it is most likely that the first choice is the most appropriate one to take, as this would allow the institution interested in the GIRAF system to assume control and responsibility of the systems and data, along with allowing the current structure of the GIRAF project to continue; as nearly everyone involved as developers, are only active for a four month period once a year, after which they are replaced with the next group of students studying that semester. The definitions are therefore as follows. The data controller would be the institution seeking to make use of the GIRAF project, by running the system on their own set of servers, thereby also letting the institution assume the role as data processor. This could, based on the specific institutions wishes be delegated to an external provider, thereby making that provider the data processor. The primary data subjects is the citizens of the particular institution, while secondary data subjects would be any caretakers registered in the system. The personal information processed in the system is data such as the name of the citizen (elaborate with any other information), along with the unique calendar assigned. Essential Principles \u00b6 The GDPR regulation states several essential principles, by which every data controller should ensure their business processes operate. Based on the previous discussion of definitions, the primary responsibilities fall upon the institutions adopting the GIRAF system, but in order for any organisation to be willing to do so, the GIRAF system must be designed and implemented with the regulation in mind, which is therefore the responsibility of the development team. The principles are summarised to the following list of tasks for the GIRAF project: Determine what data is sufficient to fulfill the purpose of the purpose of the GIRAF system, and document both the details of the data, how and when it is collected and for which purpose. Determine or define how the correctness of the data is ensured, and document it. Define reasonable step to take, to ensure the integrity, confidentiality and security of the personal data, which applies both to organisational and technical levels. Determine for how long data has to be retained, for it to fulfill its purpose, along with both when and by what means, it should be deleted. Legal Processing \u00b6 For any processing of personal data to be legal, there has to be appropriate lawful purpose to do so. There are a number of lawful purposes for processing personal data, one of which is the use of consent from the data subject, by means of a consent contract for example. The GDPR regulation defines several stipulations, regarding obtaining consent from the data subject, such as requiring high privacy and data protection by default, and any request for data collection to have a specific purpose and be of opt-in nature. Further, any processing of data regarding children, requires consent of the parent or guardian of the child. These requirements should be considered in the tasks regarding the GIRAF project. Determine the lawful basis on which personal data is to be processed in the GIRAF system, and based on this, determine the most appropriate means of obtaining such lawful basis, such as seeking to establish a consent contract with the data subject. Should the data subject be a child, it should be determined how the consent should be obtained from the parents or guardian. Legal Rights of the Data Subject \u00b6 The GDPR regulation provides the data subject with several rights, which the data controller legally is required to inform the subject of. While the details of the exact rights are defined in the section describing the GDPR regulation, it is further necessary to determine when and how, the data subject should be informed. The tasks include: Determine when and how the data subject is to be informed about their legal rights. The information should include: What is the extent of the data collection? Is any data transferred to third-parties and/or parties outside EU? Is any automated decision-making occurring, at what is the impact? What is the privacy rights of the data subject? (Details listed in the section about GDPR regulation) It should further be determined how the privacy rights of the data subject is to be enforced, both within the organisation and at the technical level. Controller and Processor in relation to GDPR \u00b6 The data controller and processor is responsible for defining the means by which the GDPR regulation is adhered to, which both means definitions of the organisational processes occurring regarding personal data information, but also the technicalities of processing personal data, such as to adhere to the rather elaborate privacy measures defined in the regulation. For the GIRAF project this means: Determine, or define, the data protection principles and measures that is designed into the system, possibly along with any required organisation processes. If, for example, pseudonymisation is used, then when, where and how? Define records of processing activities, including Purpose of the processing Categories involved Envisaged time limits Define processes to be used upon the discovery of an occurred security breach Conclusion \u00b6 This chapter has examined and detailed the essential details of the GDPR regulation, along with an elaboration of how these requirements are applicable to the GIRAF project, which includes details of responsibilities to be handled by either the data controller, data processor, data subject and, in order to make it possible for the system to live up to the regulation, the indirect responsibility of the developers to make sure the system adheres to this. The tasks defined, are necessary to complete before the GIRAF system is put to use, although, depending on the chosen organisational structure and thereby on who is to run the system, the responsibility either falls upon a defined GIRAF organisation or the institution making use of the system.","title":"GDPR Regulation"},{"location":"Legal/GDPR/gdpr_regulation/#gdpr-regulation","text":"","title":"GDPR Regulation"},{"location":"Legal/GDPR/gdpr_regulation/#sources","text":"gdpr.dk datatilsynet.dk en.wikipedia.com","title":"Sources"},{"location":"Legal/GDPR/gdpr_regulation/#what-is-the-gdpr-regulation","text":"GDPR stands for General Protection Regulation and it is a collection of laws on data protection and privacy introduced in the European Union (EU), that applies to every entity that collects and processes information on any subject within the EU. An entity might be any organization, either public or private, and a subject might be a citizen of any of the member countries of the EU. The law is in place to protect people's fundamental right and freedom, namely their right to protect their personal information. The GDPR laws requires careful consideration with regards to information technology (IT) systems, which oftentimes revolves around user information. The GIRAF project is no exception to this, and it is therefore required to take into account what data is collected, stored and how it is used. The GDPR is concerned with data that is categorised as personal information.","title":"What is the GDPR regulation"},{"location":"Legal/GDPR/gdpr_regulation/#definitions-and-concepts","text":"Personal data: Is information that identifies an individual. Processing: Any one or set of operations which is performed on personal data, whether or not by automated means. Data subject: Is a person based in the EU. Data controller: Is a natural, legal person, organization, public authority, agency or other body, which determines the purposes and means of the processing of personal data, and should be able to prove it. Processor: Natural or legal person (such as an organization), public authority, agency or other body, which processes data on behalf of a data controller.","title":"Definitions and concepts"},{"location":"Legal/GDPR/gdpr_regulation/#types-of-personal-information","text":"GDPR deals with two types of personal information, ordinary and sensitive. Treatment of sensitive data is more limited in terms of the law. Ordinary personal information Sensitive personal information Name Race and ethnicity Address Political beliefs Identification number Religious or philosophical beliefs Location data Union membership Online identification Genetic data Economic Biometric data for identification of a person Taxes Health information Debt Sexual relationship and sexual orientation Social issues Sick days Family Home Car Exam Application CV Work","title":"Types of personal information"},{"location":"Legal/GDPR/gdpr_regulation/#the-essential-principles","text":"Processing of personal information should be performed in a legal, fair and transparent way. Only sufficient, relevant and limited to what is necessary in relation to the purpose to which they are addressed (\u2018data minimization\u2019). Be correct. There needs to be taken reasonable steps to make sure that the personal information processed is correct information, that is, it is required that the data controller ensures the correctness of the data with regards to the purpose it is processed in relation to and any incorrect information should be corrected or deleted. There needs to be taken reasonable steps to ensure the integrity, confidentiality and security of personal information. For example, it needs to be ensured that correctness is persisted through time, that no unauthorized access to the data should be possible, and that all personal data is processed by adequately secure means. Such steps should be ensured by the data controller, by measures at both the organizational and technical level. The personal information should not be stored longer than what is necessary to fulfill its purpose, and afterwards it should be deleted or anonymized such that it is simply information.","title":"The essential principles"},{"location":"Legal/GDPR/gdpr_regulation/#legal-processing","text":"There has to be legal basis, to process personal information. Lawful purposes for doing so, are as follows: If the data subject has given consent to the processing of his or her personal data To fulfill contractual obligations with a data subject, or for tasks at the request of a data subject who is in the process of entering into a contract To comply with a data controller's legal obligations To protect the vital interests of a data subject or another individual To perform a task in the public interest or in official authority For the legitimate interests of a data controller or a third party, unless these interests are overridden by interests of the data subject or her or his rights according to the Charter of Fundamental Rights (especially in the case of children) For the legitimate interests of a data controller or a third party, unless these interests are overridden by interests of the data subject or her or his rights according to the Charter of Fundamental Rights (especially in the case of children).","title":"Legal processing"},{"location":"Legal/GDPR/gdpr_regulation/#legal-rights-of-the-data-subject","text":"The GDPR regulation provides the data subject with an array of legal rights, related to the data and processing thereof. The data subject must be informed about: The extent of the data collection Transfer of data to any third-party and/or parties outside the EU Automated decision-making, based solely on algorithmic basis Their privacy rights under GDPR, including right to Revoke consent Access to view the data, and an overview Data portability, such as gaining access to a portable copy of the data, and in some cases to request that personal data be transferred from one data controller to another Be forgotten, i.e. to have personal data deleted and erased, if conditions mentioned in the regulation is fulfilled Correction of wrong personal information Restrict the processing of personal data, if a number of conditions is met Contest being subject to automated decision-making, based solely on automated processing, such as profiling File complaints with a Data Protection Authority (DPA) over processing of personal information, with the only exceptions to this right being that: Legal or official authority is being carried out \u2018Legitimate interest\u2019, where the organisation needs to process data in order to provide the data subject with a service they signed up for A task being carried out for public interest","title":"Legal rights of the data subject"},{"location":"Legal/GDPR/gdpr_regulation/#children-specific-regulations","text":"If the child is under the age of 16, it is only legal to collect and process data if a parent with custody of that child, gives the permission to do so. It falls under the responsibility of the data responsible to ensure with reasonable effort that this is done. The individual countries might enact national regulation that puts this age down to 13 years.","title":"Children specific regulations"},{"location":"Legal/GDPR/gdpr_regulation/#controller-and-processor","text":"The data controller might be a company that seeks to collect and process personal information, while a data processor might be another company that provides a cloud-service that the data controller company wishes to use. Data protection principles and measures must be designed intro the business processes for products and services. This includes measures such as pseudonymisation and high level of privacy by default (Article 25). This is the responsibility of the data controller, even if processing is carried out by a data processor. The data controller keeps internal record of the processing of personal data. This will give an overview of the treatments of the data that is initiated. Such an overview is a necessary prerequisite to fulfill a number of obligations, such as: considerations on what information to process; handling insights requests and review of breaches of personal data security. This is an internal document that data protection agency can request at any time. A report ( ENISA on privacy and data protection by design (January 12, 2015) ) specifies what needs to be done to achieve privacy and data protection by default. For instance, encryption and decryption operations must be carried out locally, and not be a remote service as the keys and data must remain in the power of the data owner. Data protection impact assessment have to be conducted when specific risks occur to the rights and freedoms of data subjects. Prior approval of the data protection authorities is required for high risks (Article 35). Pseudonymisation: Is a required process for stored data that transforms personal data in such a way that the resulting data cannot be attributed to a specific data subject, without the use of additional information. An example of such, is encryption that renders the original data unintelligible and the process cannot be reversed without access to the correct decryption key. Records of processing activities: Must be maintained, and include purposes of the processing, categories involved and envisaged time limits. Security of personal data: In the case of a breach, is the data controller is under a legal obligation to notify the supervisory authority within 72 hours of becoming aware of it. Further does the data subject(s) have to be informed if there is a high risk of an adverse impact. Data protection officer (DPO): If processing is carried out by either a public authority, or if processing operations involve regular, systematic and large scale monitoring of data subjects, or if it occurs within special categories, such as a criminal system, a data protection officer has to be appointed. The DPO is expected to have expert knowledge of data protection law and practices, and should be designated to assist the controller in monitoring their internal compliance with the regulation. A designated DPO can be either internal to the organization or external, as long as there is no conflict of interest. Organizations outside of EU, must appoint an EU-based person as representative for their GDPR obligations. The DPO is further expected to be proficient at managing IT processes, data security including dealing with cyber attacks and other business continuity processes (prevention and recovery) to deal with threats to the organization.","title":"Controller and Processor"},{"location":"Legal/GDPR/gdpr_regulation/#meeting-the-gdpr-regulation-for-the-giraf-project","text":"The examination of the GDPR regulation, provides a basis for analysing it in the context of the GIRAF project, which is performed in this section.","title":"Meeting the GDPR regulation for the GIRAF project"},{"location":"Legal/GDPR/gdpr_regulation/#organisational-structure-and-definitions","text":"Initially a set of definitions has to be established. The purpose of the GIRAF project is to provide a tablet environment, including a set of tools, to assist autistic children with little or no verbal communication. The system developed, is based on a client-server architecture, meaning that for the system to work properly, an application, in this setup termed the client, has to be installed on a tablet, which communicates with a service installed on a server. The system is built such that the client in itself does not store information, but instead delegates data storage to the service on the server, thereby resembling a thin client, and which further establishes that whatever entity provides the service(s) on one or more servers is in control of the data. Depending on the particular circumstances under which the GIRAF system is deployed, including the specific business model and organisational structure, there might be a couple different ways to define the data controller and processor: As the GIRAF project is developed by open-source means, one choice would be for an institution wishing to provide this for its citizens, to acquire the necessary system files and deploy it on their own servers along with the applications on their tablets. This would not change the requirements of the institution to uphold the GDPR regulation to its citizens, but it would possibly be the easiest business model, as the institution would be able to internally to determine the budget allocated to uphold any IT-services, and it would further not require any professional dependencies or expenses with regards to the development of the system, thereby contrasting with the following alternatives. Another choice would be to establish an organisation with commercial intent, whether for-profit or non-profit, which would then provide the GIRAF system services, and thereby operate it as a Software-as-a-Service (SaaS). Those involved in such an organisation, could be anyone knowledgeable about the GIRAF system, such as a subset of developers. The reason for this to be required if GIRAF was to be provided as a SaaS, is two-fold; firstly, to operate a SaaS, an entity must take the responsibility as the data controller and processor, along with any professional obligations, such as dealing with expenses. Secondly, it is unlikely that Giraf is allowed to use the current AAU ITS servers for commercial purposes, and/or delegating the responsibilities of GDPR regulation to that department. Based on these observations, it is most likely that the first choice is the most appropriate one to take, as this would allow the institution interested in the GIRAF system to assume control and responsibility of the systems and data, along with allowing the current structure of the GIRAF project to continue; as nearly everyone involved as developers, are only active for a four month period once a year, after which they are replaced with the next group of students studying that semester. The definitions are therefore as follows. The data controller would be the institution seeking to make use of the GIRAF project, by running the system on their own set of servers, thereby also letting the institution assume the role as data processor. This could, based on the specific institutions wishes be delegated to an external provider, thereby making that provider the data processor. The primary data subjects is the citizens of the particular institution, while secondary data subjects would be any caretakers registered in the system. The personal information processed in the system is data such as the name of the citizen (elaborate with any other information), along with the unique calendar assigned.","title":"Organisational Structure and Definitions"},{"location":"Legal/GDPR/gdpr_regulation/#essential-principles","text":"The GDPR regulation states several essential principles, by which every data controller should ensure their business processes operate. Based on the previous discussion of definitions, the primary responsibilities fall upon the institutions adopting the GIRAF system, but in order for any organisation to be willing to do so, the GIRAF system must be designed and implemented with the regulation in mind, which is therefore the responsibility of the development team. The principles are summarised to the following list of tasks for the GIRAF project: Determine what data is sufficient to fulfill the purpose of the purpose of the GIRAF system, and document both the details of the data, how and when it is collected and for which purpose. Determine or define how the correctness of the data is ensured, and document it. Define reasonable step to take, to ensure the integrity, confidentiality and security of the personal data, which applies both to organisational and technical levels. Determine for how long data has to be retained, for it to fulfill its purpose, along with both when and by what means, it should be deleted.","title":"Essential Principles"},{"location":"Legal/GDPR/gdpr_regulation/#legal-processing_1","text":"For any processing of personal data to be legal, there has to be appropriate lawful purpose to do so. There are a number of lawful purposes for processing personal data, one of which is the use of consent from the data subject, by means of a consent contract for example. The GDPR regulation defines several stipulations, regarding obtaining consent from the data subject, such as requiring high privacy and data protection by default, and any request for data collection to have a specific purpose and be of opt-in nature. Further, any processing of data regarding children, requires consent of the parent or guardian of the child. These requirements should be considered in the tasks regarding the GIRAF project. Determine the lawful basis on which personal data is to be processed in the GIRAF system, and based on this, determine the most appropriate means of obtaining such lawful basis, such as seeking to establish a consent contract with the data subject. Should the data subject be a child, it should be determined how the consent should be obtained from the parents or guardian.","title":"Legal Processing"},{"location":"Legal/GDPR/gdpr_regulation/#legal-rights-of-the-data-subject_1","text":"The GDPR regulation provides the data subject with several rights, which the data controller legally is required to inform the subject of. While the details of the exact rights are defined in the section describing the GDPR regulation, it is further necessary to determine when and how, the data subject should be informed. The tasks include: Determine when and how the data subject is to be informed about their legal rights. The information should include: What is the extent of the data collection? Is any data transferred to third-parties and/or parties outside EU? Is any automated decision-making occurring, at what is the impact? What is the privacy rights of the data subject? (Details listed in the section about GDPR regulation) It should further be determined how the privacy rights of the data subject is to be enforced, both within the organisation and at the technical level.","title":"Legal Rights of the Data Subject"},{"location":"Legal/GDPR/gdpr_regulation/#controller-and-processor-in-relation-to-gdpr","text":"The data controller and processor is responsible for defining the means by which the GDPR regulation is adhered to, which both means definitions of the organisational processes occurring regarding personal data information, but also the technicalities of processing personal data, such as to adhere to the rather elaborate privacy measures defined in the regulation. For the GIRAF project this means: Determine, or define, the data protection principles and measures that is designed into the system, possibly along with any required organisation processes. If, for example, pseudonymisation is used, then when, where and how? Define records of processing activities, including Purpose of the processing Categories involved Envisaged time limits Define processes to be used upon the discovery of an occurred security breach","title":"Controller and Processor in relation to GDPR"},{"location":"Legal/GDPR/gdpr_regulation/#conclusion","text":"This chapter has examined and detailed the essential details of the GDPR regulation, along with an elaboration of how these requirements are applicable to the GIRAF project, which includes details of responsibilities to be handled by either the data controller, data processor, data subject and, in order to make it possible for the system to live up to the regulation, the indirect responsibility of the developers to make sure the system adheres to this. The tasks defined, are necessary to complete before the GIRAF system is put to use, although, depending on the chosen organisational structure and thereby on who is to run the system, the responsibility either falls upon a defined GIRAF organisation or the institution making use of the system.","title":"Conclusion"},{"location":"Legal/GDPR/processing_activity_record/","text":"Processing Activity Record \u00b6 Category Subject Information Data Controller Authority /Name of company,Reg. and contact information (address, website, phone-number and email) Organization: Girafs Venner Address: Selma Lagerl\u00f8fs vej 300, 9220 Aalborg \u00d8st Phone: 40 89 21 56 E-mail: \"Ulrik Nyman\" ulrik@cs.aau.dk CVR: 40519025 The common data controller as well his contact information (address, website, phone-number and email) The data controller's representationand his contact info(address, website, phone-number and email) (Public authorities are not covered by Article 27 PCS. 2 (b) Authority / corporate data protection consultantas well as his/her contact information (address, website, phone-number and email) Not assigned. Purpose(s) Treatment or treatment purpose (a unified, logical consistent purpose of the treatment or a variety of treatments actions, indicated herein as one purpose out of all combined purpose of the data controller) The purpose of the treatment of personal information, is solely to provide a communication tool for the autistic children and the guardians of an institution. The processing of information is related to being able to personalize the interaction with the system for the individual child, by means of having a system user. The categories of registrants and the categories of personal data Category of registrants people (e.g. citizen / customers, party representatives current or former employees, other companies companies, other authorities etc. Citizens: the children with autism enrolled in an institution. Guardians: the guardians at the institution, such as caretaker or teacher. Department: is the role of a manager, administrating guardians in a department. SuperUser: an administrator ensuring maintenance of the system. Information that is being processed about the registered persons (check and describe the types of clearances covered by action activities) Information included in the specific treatment. Describe: \u2611 Identifying information \u2610 Information concerning the employment relationship to need administration, including position and service nesting place, pay ratio, information of relevance for payroll deduction, staff papers, educational illness and absenteeism. \u2610 Race or ethnic origin \u2610 Political, religious or philosophical conviction \u2610 Trade unionism affiliation \u2610 Health information including genetic data \u2610 Biometric data with for identification purposes \u2610 Sexual relationships or sexual orientation \u2610 Criminal conditions The recipients of the personal information Categories of recipientsfor which information is or will be passed to, including those in third party countries and international organizations (for example, other authorities,companies, citizens / customers, etc.) None. Third party countries and international organizations Information on the transfer of personal data for third party countries or international organizations (for example, data processor location in third countries, data processors use of cloud solutions located in third countries) None. Deletion Time of deletion of information (the expected deadlines for deleting the various categories of information) Personal information is to be deleted 1 year after the user is declared inactive, which, depending on the institution, could be either an active choice made by the institution or as a result of not using the system for the amount of time. Technical and organizational precautionary security measures General description of technical and organizational security measures (if possible, give a general description of the technical happen and organizational security emergency measures, cf. article 32, par. 1) The risk assessment can be found in the Risk Assessment Document","title":"Processing Activity Record"},{"location":"Legal/GDPR/processing_activity_record/#processing-activity-record","text":"Category Subject Information Data Controller Authority /Name of company,Reg. and contact information (address, website, phone-number and email) Organization: Girafs Venner Address: Selma Lagerl\u00f8fs vej 300, 9220 Aalborg \u00d8st Phone: 40 89 21 56 E-mail: \"Ulrik Nyman\" ulrik@cs.aau.dk CVR: 40519025 The common data controller as well his contact information (address, website, phone-number and email) The data controller's representationand his contact info(address, website, phone-number and email) (Public authorities are not covered by Article 27 PCS. 2 (b) Authority / corporate data protection consultantas well as his/her contact information (address, website, phone-number and email) Not assigned. Purpose(s) Treatment or treatment purpose (a unified, logical consistent purpose of the treatment or a variety of treatments actions, indicated herein as one purpose out of all combined purpose of the data controller) The purpose of the treatment of personal information, is solely to provide a communication tool for the autistic children and the guardians of an institution. The processing of information is related to being able to personalize the interaction with the system for the individual child, by means of having a system user. The categories of registrants and the categories of personal data Category of registrants people (e.g. citizen / customers, party representatives current or former employees, other companies companies, other authorities etc. Citizens: the children with autism enrolled in an institution. Guardians: the guardians at the institution, such as caretaker or teacher. Department: is the role of a manager, administrating guardians in a department. SuperUser: an administrator ensuring maintenance of the system. Information that is being processed about the registered persons (check and describe the types of clearances covered by action activities) Information included in the specific treatment. Describe: \u2611 Identifying information \u2610 Information concerning the employment relationship to need administration, including position and service nesting place, pay ratio, information of relevance for payroll deduction, staff papers, educational illness and absenteeism. \u2610 Race or ethnic origin \u2610 Political, religious or philosophical conviction \u2610 Trade unionism affiliation \u2610 Health information including genetic data \u2610 Biometric data with for identification purposes \u2610 Sexual relationships or sexual orientation \u2610 Criminal conditions The recipients of the personal information Categories of recipientsfor which information is or will be passed to, including those in third party countries and international organizations (for example, other authorities,companies, citizens / customers, etc.) None. Third party countries and international organizations Information on the transfer of personal data for third party countries or international organizations (for example, data processor location in third countries, data processors use of cloud solutions located in third countries) None. Deletion Time of deletion of information (the expected deadlines for deleting the various categories of information) Personal information is to be deleted 1 year after the user is declared inactive, which, depending on the institution, could be either an active choice made by the institution or as a result of not using the system for the amount of time. Technical and organizational precautionary security measures General description of technical and organizational security measures (if possible, give a general description of the technical happen and organizational security emergency measures, cf. article 32, par. 1) The risk assessment can be found in the Risk Assessment Document","title":"Processing Activity Record"},{"location":"Legal/GDPR/processing_activity_record_information/","text":"Processing Activity Record Information \u00b6 Overview \u00b6 As per the General Data Protection Regulation (GDPR) Article 30 , it is required of a data processor to maintain a record of processing activities, which should be available for the national supervisory authority, Datatilsynet in Denmark, upon request. The purpose of this document is therefore to give an overview of the contents of the record, along with the requirements of maintaining it. The actual record is linked at the bottom, and it is based on the example of a processing activity record provided by the supervisory authority Datatilsynet. Requirements and maintenance \u00b6 There are several requirements by which such a record should comply, primarily including that the record should contain the following information. Name and contact information about the data responsible, and a representative and data protection consultant if relevant. Purpose of processing A description of categories of registered and categories of personal information The categories of recipients to whom the information is or will be disclosed, including recipients in third countries and international organizations Where applicable information on transfers to third countries or international organization(s) and documentation of appropriate guarantees when transfers are made on the basis of Article 49 (2) of the Regulation 1, second paragraph If possible, the expected deadlines for deleting the various categories of information If possible, a general description of technical and organizational security measures Further, it is required that this record is continuously updated to reflect current state of both the project, such as the system, and the entities responsible for it, such as the data controller and processer. Examples of when it would be appropriate to update the record, would be if either the processing activities changed, which could be due to an update to the system to add functionality, or if the parties detailed in the record has changed, which could be due to possible organisational changes or the acquirement of 3rd party assistance that might grant access to the described personal information, e.g. cloud provider tools, logging tools or technical support tools. The processing activity record \u00b6 The record itself is provided in a separate document .","title":"Processing Activity Record Information"},{"location":"Legal/GDPR/processing_activity_record_information/#processing-activity-record-information","text":"","title":"Processing Activity Record Information"},{"location":"Legal/GDPR/processing_activity_record_information/#overview","text":"As per the General Data Protection Regulation (GDPR) Article 30 , it is required of a data processor to maintain a record of processing activities, which should be available for the national supervisory authority, Datatilsynet in Denmark, upon request. The purpose of this document is therefore to give an overview of the contents of the record, along with the requirements of maintaining it. The actual record is linked at the bottom, and it is based on the example of a processing activity record provided by the supervisory authority Datatilsynet.","title":"Overview"},{"location":"Legal/GDPR/processing_activity_record_information/#requirements-and-maintenance","text":"There are several requirements by which such a record should comply, primarily including that the record should contain the following information. Name and contact information about the data responsible, and a representative and data protection consultant if relevant. Purpose of processing A description of categories of registered and categories of personal information The categories of recipients to whom the information is or will be disclosed, including recipients in third countries and international organizations Where applicable information on transfers to third countries or international organization(s) and documentation of appropriate guarantees when transfers are made on the basis of Article 49 (2) of the Regulation 1, second paragraph If possible, the expected deadlines for deleting the various categories of information If possible, a general description of technical and organizational security measures Further, it is required that this record is continuously updated to reflect current state of both the project, such as the system, and the entities responsible for it, such as the data controller and processer. Examples of when it would be appropriate to update the record, would be if either the processing activities changed, which could be due to an update to the system to add functionality, or if the parties detailed in the record has changed, which could be due to possible organisational changes or the acquirement of 3rd party assistance that might grant access to the described personal information, e.g. cloud provider tools, logging tools or technical support tools.","title":"Requirements and maintenance"},{"location":"Legal/GDPR/processing_activity_record_information/#the-processing-activity-record","text":"The record itself is provided in a separate document .","title":"The processing activity record"}]}